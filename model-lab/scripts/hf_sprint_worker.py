#!/usr/bin/env python3
"""
Execute a generated HF sprint queue for one agent.

This script is intentionally simple:
- Reads queue JSON generated by scripts/hf_sprint_plan.py
- Runs command tasks sequentially
- Writes append-only JSONL ledger for reproducible auditing
"""

from __future__ import annotations

import argparse
import importlib.util
import json
import re
import subprocess
import sys
import time
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from harness.env import load_dotenv_if_present

ARTIFACT_RE = re.compile(r"ARTIFACT_PATH:(?P<path>.+)")


def _can_import(module: str) -> bool:
    try:
        return importlib.util.find_spec(module) is not None
    except Exception:
        return False


def _check_task_prereqs(task: dict[str, Any]) -> tuple[bool, str]:
    """
    Fast, best-effort prereq checks.

    Goal: skip tasks that we know will fail (missing auth, missing modules, missing binaries/config).
    This keeps the sprint moving and makes reports more decision-ready.
    """
    import os
    import platform
    import shutil

    model_id = str(task.get("model_id") or "")

    if model_id == "pyannote_diarization":
        if not os.environ.get("HF_TOKEN") and not os.environ.get("HUGGINGFACE_HUB_TOKEN"):
            return False, "Missing HF_TOKEN/HUGGINGFACE_HUB_TOKEN (required for pyannote)"
        if not _can_import("pyannote.audio"):
            return False, "Missing python module: pyannote.audio"

    if model_id in {"nemotron_streaming", "parakeet_multitalker"}:
        if not _can_import("nemo.collections.asr"):
            return False, "Missing python module: nemo.collections.asr (package: nemo_toolkit[asr])"

    if model_id == "yamnet":
        if not _can_import("tensorflow"):
            return False, "Missing python module: tensorflow"
        if not _can_import("tensorflow_hub"):
            return False, "Missing python module: tensorflow_hub"

    if model_id == "clap":
        if not _can_import("laion_clap"):
            return False, "Missing python module: laion_clap (package: laion-clap)"

    if model_id == "demucs":
        if not _can_import("demucs"):
            return False, "Missing python module: demucs"

    if model_id == "deepfilternet":
        if not _can_import("deepfilternet"):
            return False, "Missing python module: deepfilternet"

    if model_id == "nb_whisper_small_onnx":
        if not _can_import("onnxruntime"):
            return False, "Missing python module: onnxruntime"

    if model_id == "mlx_whisper":
        if not _can_import("mlx_whisper"):
            return False, "Missing python module: mlx_whisper (package: mlx-whisper)"

    if model_id == "whisper_cpp":
        if shutil.which("whisper-cli") is None:
            return False, "Missing whisper.cpp binary in PATH (expected: whisper-cli)"
        cfg_path = PROJECT_ROOT / "models" / "whisper_cpp" / "config.yaml"
        try:
            import yaml  # type: ignore
        except Exception:
            return False, "Missing python module: yaml (PyYAML)"
        try:
            cfg = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
        except Exception:
            cfg = {}
        whisper_cfg = cfg.get("whisper_cpp") or {}
        if not whisper_cfg.get("model_path"):
            return False, "Missing models/whisper_cpp/config.yaml whisper_cpp.model_path"

    if model_id == "seamlessm4t" and platform.system() == "Darwin":
        return False, "Known macOS crash-loop for seamlessm4t in current env (temporarily skipped)"

    return True, "ok"


def _load_queue(path: Path) -> dict[str, Any]:
    data = json.loads(path.read_text(encoding="utf-8"))
    if "tasks" not in data:
        raise ValueError("Queue file missing 'tasks'")
    if "agent_id" not in data:
        raise ValueError("Queue file missing 'agent_id'")
    return data


def _append_jsonl(path: Path, row: dict[str, Any]) -> None:
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(row) + "\n")


def _load_completed(ledger_path: Path) -> set[str]:
    completed: set[str] = set()
    if not ledger_path.exists():
        return completed

    for line in ledger_path.read_text(encoding="utf-8").splitlines():
        if not line.strip():
            continue
        try:
            row = json.loads(line)
        except json.JSONDecodeError:
            continue
        if row.get("status") == "ok":
            task_id = row.get("task", {}).get("task_id")
            if isinstance(task_id, str):
                completed.add(task_id)
    return completed


def _extract_artifact_path(stdout: str, stderr: str) -> str | None:
    joined = "\n".join([stdout, stderr])
    matches = list(ARTIFACT_RE.finditer(joined))
    if not matches:
        return None
    return matches[-1].group("path").strip()


def _safe_file_stub(task_id: str) -> str:
    return re.sub(r"[^a-zA-Z0-9_.-]+", "_", task_id)


def run_queue(
    queue: dict[str, Any],
    *,
    execution_root: Path,
    continue_on_error: bool,
    force_rerun: bool,
    max_tasks: int | None,
    dry_run: bool,
    skip_unmet_prereqs: bool,
    task_timeout_sec: int,
) -> int:
    agent_id = queue["agent_id"]
    run_dir = execution_root / agent_id
    log_dir = run_dir / "logs"
    run_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    ledger_path = run_dir / "ledger.jsonl"

    completed = set() if force_rerun else _load_completed(ledger_path)

    tasks = queue.get("tasks", [])
    if max_tasks is not None:
        tasks = tasks[:max_tasks]

    failures = 0
    executed = 0
    skipped = 0
    prereq_skipped = 0

    for task in tasks:
        task_id = task["task_id"]
        mode = task.get("mode", "ready")
        command = task.get("command")
        now = datetime.now(UTC).isoformat()

        if mode != "ready":
            row = {
                "timestamp": now,
                "agent_id": agent_id,
                "task": task,
                "status": "skipped_manual",
                "exit_code": None,
                "duration_s": 0.0,
                "artifact_path": None,
                "stdout_log": None,
                "stderr_log": None,
            }
            _append_jsonl(ledger_path, row)
            skipped += 1
            continue

        if task_id in completed:
            row = {
                "timestamp": now,
                "agent_id": agent_id,
                "task": task,
                "status": "skipped_completed",
                "exit_code": 0,
                "duration_s": 0.0,
                "artifact_path": None,
                "stdout_log": None,
                "stderr_log": None,
            }
            _append_jsonl(ledger_path, row)
            skipped += 1
            continue

        if skip_unmet_prereqs:
            ok, reason = _check_task_prereqs(task)
            if not ok:
                row = {
                    "timestamp": now,
                    "agent_id": agent_id,
                    "task": task,
                    "status": "skipped_prereq",
                    "exit_code": None,
                    "duration_s": 0.0,
                    "artifact_path": None,
                    "stdout_log": None,
                    "stderr_log": None,
                    "prereq_reason": reason,
                }
                _append_jsonl(ledger_path, row)
                prereq_skipped += 1
                continue

        if not command:
            row = {
                "timestamp": now,
                "agent_id": agent_id,
                "task": task,
                "status": "failed_no_command",
                "exit_code": None,
                "duration_s": 0.0,
                "artifact_path": None,
                "stdout_log": None,
                "stderr_log": None,
            }
            _append_jsonl(ledger_path, row)
            failures += 1
            if not continue_on_error:
                break
            continue

        if dry_run:
            row = {
                "timestamp": now,
                "agent_id": agent_id,
                "task": task,
                "status": "dry_run",
                "exit_code": None,
                "duration_s": 0.0,
                "artifact_path": None,
                "stdout_log": None,
                "stderr_log": None,
                "command": command,
            }
            _append_jsonl(ledger_path, row)
            executed += 1
            continue

        start = time.perf_counter()
        try:
            proc = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                cwd=Path.cwd(),
                timeout=task_timeout_sec,
            )
            duration_s = time.perf_counter() - start
            artifact_path = _extract_artifact_path(proc.stdout, proc.stderr)
            stdout_text = proc.stdout or ""
            stderr_text = proc.stderr or ""
            status = "ok" if proc.returncode == 0 else "failed"
            exit_code: int | None = proc.returncode
        except subprocess.TimeoutExpired as exc:
            duration_s = time.perf_counter() - start
            artifact_path = None
            stdout_text = (exc.stdout or "") if isinstance(exc.stdout, str) else ""
            stderr_text = (exc.stderr or "") if isinstance(exc.stderr, str) else ""
            stderr_text += f"\n[TIMEOUT] Exceeded {task_timeout_sec}s"
            status = "failed_timeout"
            exit_code = None

        stub = _safe_file_stub(task_id)
        stdout_log = log_dir / f"{stub}.stdout.log"
        stderr_log = log_dir / f"{stub}.stderr.log"
        stdout_log.write_text(stdout_text, encoding="utf-8")
        stderr_log.write_text(stderr_text, encoding="utf-8")

        row = {
            "timestamp": now,
            "agent_id": agent_id,
            "task": task,
            "status": status,
            "exit_code": exit_code,
            "duration_s": round(duration_s, 3),
            "artifact_path": artifact_path,
            "stdout_log": str(stdout_log),
            "stderr_log": str(stderr_log),
            "command": command,
        }
        _append_jsonl(ledger_path, row)
        executed += 1

        if status != "ok":
            failures += 1
            if not continue_on_error:
                break

    print(f"Agent: {agent_id}")
    print(f"Executed: {executed}, Skipped: {skipped}, Failures: {failures}")
    if skip_unmet_prereqs:
        print(f"Skipped (prereqs): {prereq_skipped}")
    print(f"Ledger: {ledger_path}")
    return 1 if failures else 0


def main() -> int:
    load_dotenv_if_present()
    parser = argparse.ArgumentParser(description="Execute an HF sprint queue")
    parser.add_argument("--queue", type=Path, required=True, help="Queue JSON path")
    parser.add_argument(
        "--execution-root",
        type=Path,
        default=Path("runs/hf_sprint_2026q1/execution"),
        help="Execution output root directory",
    )
    parser.add_argument(
        "--continue-on-error",
        action="store_true",
        help="Continue executing remaining tasks after failures",
    )
    parser.add_argument(
        "--force-rerun",
        action="store_true",
        help="Rerun tasks even if ledger already has successful entries",
    )
    parser.add_argument(
        "--max-tasks",
        type=int,
        default=None,
        help="Execute at most N tasks from the queue",
    )
    parser.add_argument("--dry-run", action="store_true", help="Record actions without executing")
    parser.add_argument(
        "--no-skip-unmet-prereqs",
        action="store_true",
        help="Attempt tasks even if fast prereq checks indicate they will fail",
    )
    parser.add_argument(
        "--task-timeout-sec",
        type=int,
        default=1800,
        help="Per-task timeout in seconds (default: 1800)",
    )
    args = parser.parse_args()

    queue = _load_queue(args.queue)
    return run_queue(
        queue,
        execution_root=args.execution_root,
        continue_on_error=args.continue_on_error,
        force_rerun=args.force_rerun,
        max_tasks=args.max_tasks,
        dry_run=args.dry_run,
        skip_unmet_prereqs=not args.no_skip_unmet_prereqs,
        task_timeout_sec=args.task_timeout_sec,
    )


if __name__ == "__main__":
    raise SystemExit(main())
