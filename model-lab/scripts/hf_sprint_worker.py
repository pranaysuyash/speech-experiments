#!/usr/bin/env python3
"""
Execute a generated HF sprint queue for one agent.

This script is intentionally simple:
- Reads queue JSON generated by scripts/hf_sprint_plan.py
- Runs command tasks sequentially
- Writes append-only JSONL ledger for reproducible auditing
"""

from __future__ import annotations

import argparse
import json
import re
import subprocess
import time
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

ARTIFACT_RE = re.compile(r"ARTIFACT_PATH:(?P<path>.+)")


def _load_queue(path: Path) -> dict[str, Any]:
    data = json.loads(path.read_text(encoding="utf-8"))
    if "tasks" not in data:
        raise ValueError("Queue file missing 'tasks'")
    if "agent_id" not in data:
        raise ValueError("Queue file missing 'agent_id'")
    return data


def _append_jsonl(path: Path, row: dict[str, Any]) -> None:
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(row) + "\n")


def _load_completed(ledger_path: Path) -> set[str]:
    completed: set[str] = set()
    if not ledger_path.exists():
        return completed

    for line in ledger_path.read_text(encoding="utf-8").splitlines():
        if not line.strip():
            continue
        try:
            row = json.loads(line)
        except json.JSONDecodeError:
            continue
        if row.get("status") == "ok":
            task_id = row.get("task", {}).get("task_id")
            if isinstance(task_id, str):
                completed.add(task_id)
    return completed


def _extract_artifact_path(stdout: str, stderr: str) -> str | None:
    joined = "\n".join([stdout, stderr])
    matches = list(ARTIFACT_RE.finditer(joined))
    if not matches:
        return None
    return matches[-1].group("path").strip()


def _safe_file_stub(task_id: str) -> str:
    return re.sub(r"[^a-zA-Z0-9_.-]+", "_", task_id)


def run_queue(
    queue: dict[str, Any],
    *,
    execution_root: Path,
    continue_on_error: bool,
    force_rerun: bool,
    max_tasks: int | None,
    dry_run: bool,
) -> int:
    agent_id = queue["agent_id"]
    run_dir = execution_root / agent_id
    log_dir = run_dir / "logs"
    run_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    ledger_path = run_dir / "ledger.jsonl"

    completed = set() if force_rerun else _load_completed(ledger_path)

    tasks = queue.get("tasks", [])
    if max_tasks is not None:
        tasks = tasks[:max_tasks]

    failures = 0
    executed = 0
    skipped = 0

    for task in tasks:
        task_id = task["task_id"]
        mode = task.get("mode", "ready")
        command = task.get("command")
        now = datetime.now(UTC).isoformat()

        if mode != "ready":
            row = {
                "timestamp": now,
                "agent_id": agent_id,
                "task": task,
                "status": "skipped_manual",
                "exit_code": None,
                "duration_s": 0.0,
                "artifact_path": None,
                "stdout_log": None,
                "stderr_log": None,
            }
            _append_jsonl(ledger_path, row)
            skipped += 1
            continue

        if task_id in completed:
            row = {
                "timestamp": now,
                "agent_id": agent_id,
                "task": task,
                "status": "skipped_completed",
                "exit_code": 0,
                "duration_s": 0.0,
                "artifact_path": None,
                "stdout_log": None,
                "stderr_log": None,
            }
            _append_jsonl(ledger_path, row)
            skipped += 1
            continue

        if not command:
            row = {
                "timestamp": now,
                "agent_id": agent_id,
                "task": task,
                "status": "failed_no_command",
                "exit_code": None,
                "duration_s": 0.0,
                "artifact_path": None,
                "stdout_log": None,
                "stderr_log": None,
            }
            _append_jsonl(ledger_path, row)
            failures += 1
            if not continue_on_error:
                break
            continue

        if dry_run:
            row = {
                "timestamp": now,
                "agent_id": agent_id,
                "task": task,
                "status": "dry_run",
                "exit_code": None,
                "duration_s": 0.0,
                "artifact_path": None,
                "stdout_log": None,
                "stderr_log": None,
                "command": command,
            }
            _append_jsonl(ledger_path, row)
            executed += 1
            continue

        start = time.perf_counter()
        proc = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            cwd=Path.cwd(),
        )
        duration_s = time.perf_counter() - start
        artifact_path = _extract_artifact_path(proc.stdout, proc.stderr)
        stub = _safe_file_stub(task_id)
        stdout_log = log_dir / f"{stub}.stdout.log"
        stderr_log = log_dir / f"{stub}.stderr.log"
        stdout_log.write_text(proc.stdout or "", encoding="utf-8")
        stderr_log.write_text(proc.stderr or "", encoding="utf-8")

        status = "ok" if proc.returncode == 0 else "failed"
        row = {
            "timestamp": now,
            "agent_id": agent_id,
            "task": task,
            "status": status,
            "exit_code": proc.returncode,
            "duration_s": round(duration_s, 3),
            "artifact_path": artifact_path,
            "stdout_log": str(stdout_log),
            "stderr_log": str(stderr_log),
            "command": command,
        }
        _append_jsonl(ledger_path, row)
        executed += 1

        if proc.returncode != 0:
            failures += 1
            if not continue_on_error:
                break

    print(f"Agent: {agent_id}")
    print(f"Executed: {executed}, Skipped: {skipped}, Failures: {failures}")
    print(f"Ledger: {ledger_path}")
    return 1 if failures else 0


def main() -> int:
    parser = argparse.ArgumentParser(description="Execute an HF sprint queue")
    parser.add_argument("--queue", type=Path, required=True, help="Queue JSON path")
    parser.add_argument(
        "--execution-root",
        type=Path,
        default=Path("runs/hf_sprint_2026q1/execution"),
        help="Execution output root directory",
    )
    parser.add_argument(
        "--continue-on-error",
        action="store_true",
        help="Continue executing remaining tasks after failures",
    )
    parser.add_argument(
        "--force-rerun",
        action="store_true",
        help="Rerun tasks even if ledger already has successful entries",
    )
    parser.add_argument(
        "--max-tasks",
        type=int,
        default=None,
        help="Execute at most N tasks from the queue",
    )
    parser.add_argument("--dry-run", action="store_true", help="Record actions without executing")
    args = parser.parse_args()

    queue = _load_queue(args.queue)
    return run_queue(
        queue,
        execution_root=args.execution_root,
        continue_on_error=args.continue_on_error,
        force_rerun=args.force_rerun,
        max_tasks=args.max_tasks,
        dry_run=args.dry_run,
    )


if __name__ == "__main__":
    raise SystemExit(main())
