# Model Lab Migration Guide

## ðŸŽ¯ **What Changed?**

We've restructured the lab to follow ChatGPT's scalable architecture recommendations. The key changes:

### **Before** (Scattered Structure):
```
model-lab/
â”œâ”€â”€ notebooks/audio/     # Mixed notebooks from different models
â”œâ”€â”€ harness/             # Model-specific harness
â””â”€â”€ data/                # Disorganized data
```

### **After** (Scalable Structure):
```
model-lab/
â”œâ”€â”€ models/              # Isolated per model
â”‚   â”œâ”€â”€ lfm2_5_audio/
â”‚   â””â”€â”€ whisper/
â”œâ”€â”€ harness/             # Shared infrastructure
â”œâ”€â”€ runs/                # JSON results for comparison
â”œâ”€â”€ compare/             # Automated comparison dashboards
â””â”€â”€ data/                # Organized test data
```

## ðŸ“‹ **File Migration Status**

### **âœ… Automatically Migrated**:
- All LFM notebooks â†’ `models/lfm2_5_audio/notebooks/`
- Data files â†’ `data/` with proper organization
- Original harness â†’ `models/lfm2_5_audio/harness/` (temporary)

### **ðŸ”„ Need Your Action**:

#### **1. Update Your Notebook References**
Old notebooks might reference old paths. Update these:

**Old paths**:
```python
# Old audio path
audio_path = Path('data/audio/PRIMARY/llm_recording_pranay.m4a')

# Old harness import
import sys
sys.path.append('harness')
```

**New paths**:
```python
# New audio path (relative to model directory)
audio_path = Path.cwd().parent.parent.parent / 'data' / 'audio' / 'PRIMARY' / 'llm_recording_pranay.m4a'

# New harness import (from anywhere)
harness_path = Path.cwd().parent.parent / 'harness'
sys.path.insert(0, str(harness_path))
from harness import AudioLoader, ModelRegistry
```

#### **2. Test Data Organization**
Your test files are now organized:
```
data/
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ PRIMARY/              # Your original recordings
â”‚   â”‚   â”œâ”€â”€ llm_recording_pranay.m4a
â”‚   â”‚   â””â”€â”€ UX_Psychology_*.m4a
â”‚   â”œâ”€â”€ GROUND_TRUTH/         # Reference audio
â”‚   â””â”€â”€ SYNTHETIC/            # Generated test audio
â””â”€â”€ text/
    â”œâ”€â”€ PRIMARY/              # Your texts
    â”‚   â””â”€â”€ llm.txt
    â””â”€â”€ GROUND_TRUTH/         # Reference texts
```

#### **3. Legacy Notebooks**
Your original notebooks are preserved in `models/lfm2_5_audio/notebooks/`:
- `lfm_complete_working.ipynb` - Original LFM implementation
- `test_environment.ipynb` - Environment validation
- `asr_evaluation.ipynb` - ASR with your recordings
- `tts_evaluation.ipynb` - TTS evaluation
- `conversation_analysis.ipynb` - NotebookLM analysis

**These still work** but use the old structure. We recommend migrating to the new systematic notebooks:
- `00_smoke.ipynb` - Quick validation
- `10_asr.ipynb` - ASR evaluation

## ðŸš€ **Quick Start with New Structure**

### **Step 1: Test LFM2.5-Audio**
```bash
cd models/lfm2_5_audio
jupyter notebook notebooks/00_smoke.ipynb
```

### **Step 2: Test Whisper Baseline**
```bash
cd ../whisper
jupyter notebook notebooks/00_smoke.ipynb
jupyter notebook notebooks/10_asr.ipynb
```

### **Step 3: Compare Results**
```bash
cd ../../compare
jupyter notebook 00_scorecard.ipynb
```

## ðŸŽ¯ **Benefits of New Structure**

### **1. Scalability**
- Add new models without touching existing code
- Each model is self-contained

### **2. Fair Comparison**
- Shared harness ensures identical metrics
- Same test data, same evaluation

### **3. Automation**
- Results â†’ JSON â†’ Scorecard â†’ Decision
- No manual comparison needed

### **4. Production Ready**
- Config-driven model loading
- Systematic testing workflow
- Clear production recommendations

## ðŸ”§ **Technical Changes**

### **Harness Modules**
New shared infrastructure in `harness/`:
- **audio_io.py**: Consistent audio loading
- **metrics_asr.py**: WER/CER calculation
- **metrics_tts.py**: Audio similarity
- **timers.py**: Performance monitoring
- **registry.py**: Model loading
- **normalize.py**: Text normalization

### **Model Configuration**
Each model has `config.yaml`:
```yaml
model_name: LiquidAI/LFM2.5-Audio-1.5B
model_type: lfm2_5_audio
device: mps
modes: [asr, tts, chat]
```

### **Results Format**
Standardized JSON output:
```json
{
  "model": "lfm2_5_audio",
  "test_type": "asr",
  "timestamp": "2026-01-08T12:34:56",
  "wer": 0.05,
  "cer": 0.03,
  "latency_ms": 450,
  "rtf": 0.045
}
```

## ðŸ“Š **Comparison Dashboard**

The `compare/00_scorecard.ipynb` automatically:
1. Loads all JSON results from `runs/`
2. Builds comparison table
3. Calculates production scores
4. Generates visualization plots
5. Provides recommendation

**No manual work needed** - just run the notebooks and this dashboard.

## ðŸ†˜ **Troubleshooting**

### **Issue**: Import errors for harness
**Solution**: Update import paths as shown above

### **Issue**: Can't find test data
**Solution**: Update paths to use new `data/` structure

### **Issue**: Old notebooks not working
**Solution**: Try new systematic notebooks (00_smoke, 10_asr)

### **Issue**: Whisper not installed
**Solution**: `uv add openai-whisper`

## ðŸŽ‰ **Migration Complete**

You now have a production-ready model testing lab that:
- Scales to unlimited models
- Ensures fair comparisons
- Automates decision-making
- Follows best practices

**The migration is 100% backward compatible** - your old notebooks still work, but we recommend using the new systematic approach.

---

**Questions?** Check the model-specific README files:
- `models/lfm2_5_audio/README.md`
- `models/whisper/README.md`