# Tweet Drafts - January 8, 2026

**Context**: Posting findings from 1-day experiment with speech models on Apple Silicon  
**Tone**: Casual, informative, sharing learnings

---

## ğŸ¯ Tweet 1: CUDA Issue & MPS Solution

### Version A (Concise):

```
Spent today testing LiquidAI/LFM2.5-Audio-1.5B on Apple Silicon ğŸ

Hit a CUDA issue right away - model tried to load CUDA even on M3 chip.

Quick fix:
â€¢ Load processor on CPU first
â€¢ Move to MPS after
â€¢ Handle audio tensor shapes properly

Now running smooth on Apple Silicon!

#MachineLearning #AppleSilicon
```

### Version B (Problem-Solution):

```
Day 1 with LiquidAI/LFM2.5-Audio-1.5B on M3 MacBook ğŸ’»

âŒ Problem: Model hardcoded CUDA, failed on Apple Silicon
âœ… Solution: Load processor CPU-first, then migrate to MPS

Small fix, big difference. Model now runs native on MPS.

Sometimes the best debugging is just understanding the defaults ğŸ”§

#MLOps #AppleSilicon
```

### Version C (Story Format):

```
Tried running LiquidAI/LFM2.5-Audio-1.5B on my M3 Mac today.

Immediate crash: CUDA not available ğŸ¤”

Turns out the processor was hardcoded to CUDA by default. Solution was simple - load on CPU, then move to MPS device.

One day, one model, one fix. That's the fun of experimentation! ğŸš€

#ML #MachineLearning
```

---

## ğŸ¯ Tweet 2: LFM-2.5-Audio Findings

### Version A (Research Findings):

```
Finished testing LiquidAI/LFM2.5-Audio-1.5B for speech recognition ğŸ¤

Key finding: It's NOT optimized for ASR (automatic speech recognition)

On 163s audio:
â€¢ 137.8% WER (vs 24% for specialized models)
â€¢ Incomplete long-form transcription (10% output on 15min audio)

LFM is multi-modal & conversational - different use case entirely ğŸ¯
```

### Version B (Comparison Focus):

```
Compared 4 speech models today: Whisper, Faster-Whisper, LiquidAI/LFM2.5-Audio-1.5B, and SeamlessM4T

LFM2.5-Audio findings:
â€¢ Built for conversation, not pure transcription
â€¢ 137.8% WER on technical audio
â€¢ Only completes ~10% of long-form content

Great multi-modal model, but use Whisper for ASR ğŸ™ï¸

Different tools for different jobs!
```

### Version C (Balanced Perspective):

```
Tested LiquidAI/LFM2.5-Audio-1.5B alongside Whisper models today ğŸ”Š

Reality check: LFM isn't for ASR transcription.
â€¢ High error rates (138% WER)
â€¢ Struggles with long audio
â€¢ But that's not what it's built for!

LFM2.5 does conversation & multi-modal - totally different mission.

Right tool for right job ğŸ› ï¸

@maximelabonne
```

### Version D (Community Value):

```
PSA for anyone testing LiquidAI/LFM2.5-Audio-1.5B for speech-to-text ğŸ¤

My findings after 1 day:
âŒ Not suitable for pure ASR (137.8% WER vs 24% for Whisper)
âŒ Incomplete on long audio (10% output)
âœ… Built for conversational AI instead

Save yourself debugging time - use Whisper/Faster-Whisper for transcription!
```

---

## ğŸ¯ Combined Thread Option

### Tweet 1:

```
24-hour experiment with LiquidAI/LFM2.5-Audio-1.5B on Apple Silicon M3 ğŸ§ª

First challenge: CUDA hardcoding broke on MPS. Fixed by loading processor on CPU first, then migrating to device.

Then benchmarked for speech recognition... ğŸ§µ
```

### Tweet 2:

```
Key finding: LFM2.5-Audio isn't optimized for pure ASR transcription.

Results on real audio:
â€¢ 137.8% WER (Whisper: 24%)
â€¢ Incomplete long-form (10% output on 15min)

But that's OK! LFM is built for conversational AI, not transcription.
```

### Tweet 3:

```
Bottom line:
âœ… Got LFM2.5 running on Apple Silicon (MPS fix needed)
âœ… Benchmarked vs Whisper, Faster-Whisper, SeamlessM4T
âœ… Found sweet spot for each model

LFM = Conversation
Whisper = Transcription

Full results in model-lab repo ğŸ“Š

@maximelabonne
```

---

## ğŸ¯ Recommendations

**For Tweet 1 (CUDA/MPS)**: Use **Version B** - clear problem/solution format that's helpful for others

**For Tweet 2 (LFM findings)**: Use **Version C** - balanced, tags Maxime, acknowledges different use cases

**Alternative**: Use the **Combined Thread** if you want to tell the full story in one narrative

---

## ğŸ“ Notes

- All tweets acknowledge LFM2.5 is NOT an ASR-first model
- Findings are constructive, not critical
- Actual model name used: `LiquidAI/LFM2.5-Audio-1.5B`
- MPS solution is practical and helpful for community
- WER numbers provided for context but not overemphasized
- Tone stays positive and appreciative of all models tested
