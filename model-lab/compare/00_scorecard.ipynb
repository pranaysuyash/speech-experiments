{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Scorecard\n",
    "\n",
    "**Purpose**: Transform experimental results into production decisions\n",
    "\n",
    "**Models Compared**:\n",
    "- LFM2.5-Audio-1.5B (LiquidAI)\n",
    "- Whisper-Large-V3 (OpenAI)\n",
    "\n",
    "**Metrics**: Accuracy, Speed, Memory, Production Readiness\n",
    "\n",
    "---\n",
    "\n",
    "This dashboard automatically loads results from all model tests and produces:\n",
    "- Comparative scorecards\n",
    "- Production recommendations\n",
    "- Cost-performance analysis\n",
    "- Visualization plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPARISON SETUP ===\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== Model Comparison Dashboard ===\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(\"‚úÖ Comparison setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD RESULTS ===\n",
    "\n",
    "\n",
    "def load_results_from_directory(model_dir: Path) -> dict:\n",
    "    \"\"\"Load all JSON results from a model directory.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    if not model_dir.exists():\n",
    "        return results\n",
    "\n",
    "    # Walk through subdirectories (asr, tts, chat)\n",
    "    for task_dir in model_dir.iterdir():\n",
    "        if task_dir.is_dir():\n",
    "            task_name = task_dir.name\n",
    "            results[task_name] = []\n",
    "\n",
    "            # Load all JSON files in task directory\n",
    "            for json_file in sorted(task_dir.glob(\"*.json\")):\n",
    "                try:\n",
    "                    with open(json_file, \"r\") as f:\n",
    "                        data = json.load(f)\n",
    "                        results[task_name].append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {json_file}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Load results from all models\n",
    "runs_dir = Path.cwd().parent / \"runs\"\n",
    "\n",
    "all_results = {}\n",
    "models = [\"lfm2_5_audio\", \"whisper\"]\n",
    "\n",
    "for model in models:\n",
    "    model_dir = runs_dir / model\n",
    "    print(f\"Loading results for {model}...\")\n",
    "    results = load_results_from_directory(model_dir)\n",
    "\n",
    "    total_files = sum(len(files) for files in results.values())\n",
    "    print(f\"  ‚úì Loaded {total_files} result files\")\n",
    "\n",
    "    if results:\n",
    "        all_results[model] = results\n",
    "\n",
    "print(f\"\\n‚úì Loaded results from {len(all_results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BUILD COMPARISON TABLE ===\n",
    "\n",
    "\n",
    "def extract_key_metrics(results: dict, model_name: str) -> list:\n",
    "    \"\"\"Extract key metrics for comparison table.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for task_name, test_runs in results.items():\n",
    "        if not test_runs:\n",
    "            continue\n",
    "\n",
    "        # Use most recent test run\n",
    "        latest_run = test_runs[-1]\n",
    "\n",
    "        row = {\n",
    "            \"Model\": model_name.replace(\"_\", \"-\").title(),\n",
    "            \"Test\": task_name.upper(),\n",
    "            \"WER (%)\": latest_run.get(\"wer\", 0) * 100,\n",
    "            \"CER (%)\": latest_run.get(\"cer\", 0) * 100,\n",
    "            \"Latency (ms)\": latest_run.get(\"latency_ms\", 0),\n",
    "            \"RTF\": latest_run.get(\"rtf\", 0),\n",
    "            \"Timestamp\": latest_run.get(\"timestamp\", \"Unknown\"),\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# Build comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, model_results in all_results.items():\n",
    "    comparison_data.extend(extract_key_metrics(model_results, model_name))\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "if not df_comparison.empty:\n",
    "    print(\"=== Model Comparison Scorecard ===\")\n",
    "    print(df_comparison.to_string(index=False))\n",
    "else:\n",
    "    print(\"No comparison data available yet.\")\n",
    "    print(\"Run model test notebooks first to generate results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRODUCTION READINESS SCORE ===\n",
    "\n",
    "\n",
    "def calculate_production_score(row: pd.Series) -> float:\n",
    "    \"\"\"Calculate production readiness score (0-100).\"\"\"\n",
    "    scores = []\n",
    "\n",
    "    # Accuracy score (WER: lower is better)\n",
    "    if not pd.isna(row.get(\"WER (%)\", 0)):\n",
    "        wer = row[\"WER (%)\"]\n",
    "        wer_score = max(0, 100 - wer)  # 0% WER = 100 score\n",
    "        scores.append(wer_score)\n",
    "\n",
    "    # Speed score (RTF: lower is better, <1.0 = realtime)\n",
    "    if not pd.isna(row.get(\"RTF\", 0)):\n",
    "        rtf = row[\"RTF\"]\n",
    "        rtf_score = max(0, 100 - rtf * 50)  # RTF=0 = 100, RTF=2.0 = 0\n",
    "        scores.append(rtf_score)\n",
    "\n",
    "    return np.mean(scores) if scores else 0\n",
    "\n",
    "\n",
    "if not df_comparison.empty:\n",
    "    # Calculate production scores\n",
    "    df_comparison[\"Production Score\"] = df_comparison.apply(calculate_production_score, axis=1)\n",
    "    df_comparison[\"Grade\"] = df_comparison[\"Production Score\"].apply(\n",
    "        lambda x: \"A\" if x >= 80 else \"B\" if x >= 60 else \"C\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Production Readiness Scorecard ===\")\n",
    "    print(df_comparison[[\"Model\", \"Test\", \"Production Score\", \"Grade\"]].to_string(index=False))\n",
    "\n",
    "    # Overall recommendation\n",
    "    best_scores = df_comparison.groupby(\"Model\")[\"Production Score\"].mean()\n",
    "    best_model = best_scores.idxmax()\n",
    "    best_score = best_scores.max()\n",
    "\n",
    "    print(f\"\\n=== PRODUCTION RECOMMENDATION ===\")\n",
    "    print(f\"üèÜ Recommended: {best_model}\")\n",
    "    print(f\"   Overall Score: {best_score:.1f}/100\")\n",
    "\n",
    "    if best_score >= 80:\n",
    "        print(\"   ‚úÖ Ready for production deployment\")\n",
    "    elif best_score >= 60:\n",
    "        print(\"   ‚ö†Ô∏è  Ready with monitoring required\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Not recommended for production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VISUALIZATION ===\n",
    "\n",
    "if not df_comparison.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\"Model Comparison: Production Decision Dashboard\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Plot 1: WER Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    for model in df_comparison[\"Model\"].unique():\n",
    "        model_data = df_comparison[df_comparison[\"Model\"] == model]\n",
    "        ax1.bar(model_data[\"Test\"], model_data[\"WER (%)\"], label=model, alpha=0.7)\n",
    "    ax1.set_ylabel(\"WER (%)\")\n",
    "    ax1.set_title(\"Word Error Rate (lower is better)\")\n",
    "    ax1.legend()\n",
    "    ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Plot 2: Speed Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    for model in df_comparison[\"Model\"].unique():\n",
    "        model_data = df_comparison[df_comparison[\"Model\"] == model]\n",
    "        ax2.bar(model_data[\"Test\"], model_data[\"RTF\"], label=model, alpha=0.7)\n",
    "    ax2.axhline(y=1.0, color=\"r\", linestyle=\"--\", label=\"Realtime threshold\")\n",
    "    ax2.set_ylabel(\"Real-Time Factor\")\n",
    "    ax2.set_title(\"Processing Speed (lower is better)\")\n",
    "    ax2.legend()\n",
    "    ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Plot 3: Production Scores\n",
    "    ax3 = axes[1, 0]\n",
    "    for model in df_comparison[\"Model\"].unique():\n",
    "        model_data = df_comparison[df_comparison[\"Model\"] == model]\n",
    "        ax3.bar(model_data[\"Test\"], model_data[\"Production Score\"], label=model, alpha=0.7)\n",
    "    ax3.axhline(y=80, color=\"g\", linestyle=\"--\", label=\"Production ready\")\n",
    "    ax3.axhline(y=60, color=\"orange\", linestyle=\"--\", label=\"Monitor\")\n",
    "    ax3.set_ylabel(\"Score (0-100)\")\n",
    "    ax3.set_title(\"Production Readiness Score\")\n",
    "    ax3.legend()\n",
    "    ax3.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Plot 4: Latency Comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    for model in df_comparison[\"Model\"].unique():\n",
    "        model_data = df_comparison[df_comparison[\"Model\"] == model]\n",
    "        ax4.bar(model_data[\"Test\"], model_data[\"Latency (ms)\"], label=model, alpha=0.7)\n",
    "    ax4.axhline(y=500, color=\"r\", linestyle=\"--\", label=\"500ms target\")\n",
    "    ax4.set_ylabel(\"Latency (ms)\")\n",
    "    ax4.set_title(\"Processing Latency (lower is better)\")\n",
    "    ax4.legend()\n",
    "    ax4.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    plot_path = Path.cwd().parent / \"runs\" / \"comparison_plots.png\"\n",
    "    plot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"‚úì Comparison plots saved to {plot_path}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVE COMPARISON RESULTS ===\n",
    "\n",
    "if not df_comparison.empty:\n",
    "    # Save comparison table\n",
    "    comparison_path = Path.cwd().parent / \"runs\" / \"model_comparison.json\"\n",
    "\n",
    "    comparison_results = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"num_models\": len(all_results),\n",
    "        \"models_compared\": list(all_results.keys()),\n",
    "        \"comparison_table\": df_comparison.to_dict(\"records\"),\n",
    "        \"recommendation\": {\n",
    "            \"best_model\": best_model if \"best_model\" in locals() else None,\n",
    "            \"best_score\": float(best_score) if \"best_score\" in locals() else None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with open(comparison_path, \"w\") as f:\n",
    "        json.dump(comparison_results, f, indent=2)\n",
    "\n",
    "    print(f\"‚úì Comparison results saved to {comparison_path}\")\n",
    "    print(f\"\\nüéâ Model comparison complete!\")\n",
    "    print(f\"‚úÖ Production decision ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ **Decision Framework**\n",
    "\n",
    "### **Key Metrics**:\n",
    "- **WER**: Word Error Rate (lower = better accuracy)\n",
    "- **RTF**: Real-Time Factor (lower = faster, <1.0 = realtime)\n",
    "- **Production Score**: Combined metric (0-100, higher = better)\n",
    "\n",
    "### **Decision Matrix**:\n",
    "- **Score ‚â• 80**: ‚úÖ Deploy with confidence\n",
    "- **Score 60-80**: ‚ö†Ô∏è Deploy with monitoring\n",
    "- **Score < 60**: ‚ùå Not production-ready\n",
    "\n",
    "### **Next Steps**:\n",
    "1. Run all model test notebooks (00_smoke.ipynb, 10_asr.ipynb, etc.)\n",
    "2. Re-run this notebook to see updated comparisons\n",
    "3. Use production recommendation for deployment decisions\n",
    "\n",
    "### **Adding New Models**:\n",
    "1. Create folder under `models/<new_model>/`\n",
    "2. Add `config.yaml` and notebooks\n",
    "3. Run tests to generate results in `runs/<new_model>/`\n",
    "4. This notebook will automatically include them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (model-lab)",
   "language": "python",
   "name": "model-lab"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}