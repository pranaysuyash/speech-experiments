{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726871ba",
   "metadata": {},
   "source": [
    "# üß™ Model-Lab Multi-Platform Compatibility Test\n",
    "\n",
    "**Date:** January 8, 2026  \n",
    "**Purpose:** Test model-lab across CPU, MPS, GPU (CUDA), and TPU platforms  \n",
    "**VS Code Support:** Works with VS Code Jupyter + Google Colab extension\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Two Ways to Test\n",
    "\n",
    "### Option 1: Local Testing with UV Venv (Recommended for MPS)\n",
    "\n",
    "**For testing on your Mac with Apple Silicon:**\n",
    "\n",
    "1. **Select the UV Kernel in VS Code**:\n",
    "\n",
    "   - Click \"Select Kernel\" (top-right of notebook)\n",
    "   - Choose `Model Lab (UV Python 3.12)` from the dropdown\n",
    "   - This uses your UV venv with all dependencies installed\n",
    "\n",
    "2. **Run Cell 2** (Install Dependencies) - Skip this, already installed in UV venv\n",
    "\n",
    "3. **Run Cell 4** (MPS Test) - Tests all 4 models on Apple Silicon GPU\n",
    "\n",
    "4. **Results** - Get real MPS performance metrics\n",
    "\n",
    "**MPS bypass notes (what Cell 4 does automatically):**\n",
    "\n",
    "- LFM processor is loaded on CPU first, then moved to MPS to avoid the vendor CUDA default crash. (See `docs/LFM_MPS_FIX_SUMMARY.md` for details.)\n",
    "- SeamlessM4T loads with fallbacks: direct MPS ‚Üí low_cpu_mem_usage on CPU then move ‚Üí CPU-only if needed. The device used is printed in the log.\n",
    "\n",
    "---\n",
    "\n",
    "### Option 2: Cloud Testing with Google Colab (For GPU/TPU)\n",
    "\n",
    "**For testing on Google's cloud infrastructure:**\n",
    "\n",
    "1. **Click \"Select Kernel\"** (top-right corner of notebook)\n",
    "2. **Choose \"Colab\"** from dropdown\n",
    "3. **Select Runtime Type**:\n",
    "   - **CPU** - Free (always available)\n",
    "   - **GPU (T4)** - Free with limits (~12hr sessions)\n",
    "   - **TPU (v2-8/v5e)** - Free with limits\n",
    "4. **Run Cell 2** (Install Dependencies) - One time only for cloud environment\n",
    "5. **Run the matching test cell** (3, 5, or 6)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What Gets Tested\n",
    "\n",
    "### Models (All 4):\n",
    "\n",
    "- ‚úÖ **Whisper** (tiny) - OpenAI's ASR model\n",
    "- ‚úÖ **Faster-Whisper** (tiny) - Optimized CTranslate2 version\n",
    "- ‚úÖ **LFM-2.5-Audio** (1.5B) - Liquid AI's foundation model\n",
    "- ‚úÖ **SeamlessM4T** (v2-large) - Meta's multilingual ASR + translation\n",
    "\n",
    "**üìù Model Size Notes:**\n",
    "\n",
    "- **Test Files** (this notebook, colab_test.py): Hardcoded to \"tiny\" for fast testing\n",
    "- **Production Code** (harness/registry.py): Configurable, defaults to \"large-v3\" for best accuracy\n",
    "- Change model sizes in production via config: `{\"model_name\": \"base|small|medium|large-v3\"}`\n",
    "\n",
    "### Test Cells:\n",
    "\n",
    "| Cell  | Name                 | Best For                     | Kernel            |\n",
    "| ----- | -------------------- | ---------------------------- | ----------------- |\n",
    "| **2** | Install Dependencies | First time cloud testing     | Colab             |\n",
    "| **3** | CPU Test             | Baseline on any system       | Any (Colab or UV) |\n",
    "| **4** | MPS Test             | Apple Silicon GPU (M1/M2/M3) | **UV Kernel** ‚≠ê  |\n",
    "| **5** | GPU Test             | NVIDIA CUDA (T4 on Colab)    | Colab GPU         |\n",
    "| **6** | TPU Test             | Google TPU (Colab TPU)       | Colab TPU         |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Start Steps\n",
    "\n",
    "**For MPS Testing (Local Mac):**\n",
    "\n",
    "1. **Select Kernel** ‚Üí `Model Lab (UV Python 3.12)`\n",
    "2. **Run Cell 4** ‚Üí MPS test begins\n",
    "3. **Wait ~3-5 min** ‚Üí Results show\n",
    "\n",
    "**For GPU Testing (Colab Cloud):**\n",
    "\n",
    "1. **Select Kernel** ‚Üí `Colab`\n",
    "2. **Select Runtime** ‚Üí GPU (T4)\n",
    "3. **Run Cell 2** ‚Üí Install dependencies (one time)\n",
    "4. **Run Cell 5** ‚Üí GPU test begins\n",
    "5. **Wait ~2-3 min** ‚Üí Results show\n",
    "\n",
    "**For TPU Testing (Colab Cloud):**\n",
    "\n",
    "1. **Select Kernel** ‚Üí `Colab`\n",
    "2. **Select Runtime** ‚Üí TPU (v5e)\n",
    "3. **Run Cell 2** ‚Üí Install dependencies\n",
    "4. **Run Cell 6** ‚Üí TPU test begins\n",
    "5. **Wait ~2-3 min** ‚Üí Results show\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Performance Expectations\n",
    "\n",
    "| Platform | Device | Whisper | Faster-Whisper | RTF   |\n",
    "| -------- | ------ | ------- | -------------- | ----- |\n",
    "| **MPS**  | M3 GPU | ~3.2s   | ~2.8s          | 0.64x |\n",
    "| **GPU**  | T4     | ~1.8s   | ~1.2s          | 0.24x |\n",
    "| **CPU**  | Any    | ~14s    | ~10s           | 2.8x  |\n",
    "\n",
    "**RTF = Real-Time Factor (lower = faster than real-time)**\n",
    "\n",
    "---\n",
    "\n",
    "**üëâ Start with Cell 4 (MPS) using UV Kernel**  \n",
    "**üëâ Then try Cell 5 (GPU) using Colab kernel if you want cloud results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üì¶ INSTALLING DEPENDENCIES\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ Installing PyTorch with CUDA support...\n",
      "2Ô∏è‚É£ Installing ASR models...\n",
      "3Ô∏è‚É£ Installing LFM-2.5-Audio...\n",
      "4Ô∏è‚É£ Installing SeamlessM4T...\n",
      "   ‚ÑπÔ∏è SeamlessM4T is included in transformers (already installed above)\n",
      "5Ô∏è‚É£ Handling TensorFlow conflict with torch-xla...\n",
      "   ‚úÖ Replaced tensorflow with tensorflow-cpu\n",
      "6Ô∏è‚É£ Installing TPU support (for TPU runtime)...\n",
      "   ‚úÖ TPU support installed\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ALL DEPENDENCIES INSTALLED\n",
      "======================================================================\n",
      "\n",
      "üìã Next: Run ONE test cell below matching your runtime:\n",
      "   ‚Ä¢ Cell 3: CPU (any runtime)\n",
      "   ‚Ä¢ Cell 4: MPS (local Mac)\n",
      "   ‚Ä¢ Cell 5: GPU (Colab GPU)\n",
      "   ‚Ä¢ Cell 6: TPU (Colab TPU)\n"
     ]
    }
   ],
   "source": [
    "# üîß Cell 2: Install Dependencies (Run First in Colab!)\n",
    "# This cell is now runtime-aware to avoid breaking TPU runtimes by installing CUDA wheels.\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üì¶ INSTALLING DEPENDENCIES (runtime-aware)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def _is_colab_tpu_runtime() -> bool:\n",
    "    # Heuristics that are cheap and don't require importing torch/tensorflow.\n",
    "    return bool(\n",
    "        os.environ.get(\"COLAB_TPU_ADDR\")\n",
    "        or os.environ.get(\"TPU_NAME\")\n",
    "        or os.environ.get(\"XRT_TPU_CONFIG\")\n",
    "        or os.environ.get(\"TPU_ACCELERATOR_TYPE\")\n",
    "    )\n",
    "\n",
    "\n",
    "def _has_nvidia_smi() -> bool:\n",
    "    return Path(\"/usr/bin/nvidia-smi\").exists() or Path(\"/bin/nvidia-smi\").exists()\n",
    "\n",
    "\n",
    "is_tpu = _is_colab_tpu_runtime()\n",
    "is_gpu = _has_nvidia_smi()\n",
    "print(f\"\\nüß≠ Runtime detection:\")\n",
    "print(f\"  TPU runtime detected: {is_tpu}\")\n",
    "print(f\"  GPU runtime detected: {is_gpu}\")\n",
    "print(f\"  Python: {sys.version.split()[0]}\")\n",
    "\n",
    "# Core ML packages\n",
    "if is_tpu:\n",
    "    print(\"\\n1Ô∏è‚É£ TPU runtime: installing PyTorch/XLA stack (avoid CUDA wheels)\")\n",
    "    # IMPORTANT: torch_xla wheels are tightly coupled to torch version.\n",
    "    # Installing torch_xla[tpu] from the libtpu index will pull a compatible torch build.\n",
    "    !pip install -q 'torch_xla[tpu]==2.9.0' -f https://storage.googleapis.com/libtpu-releases/index.html\n",
    "    print(\"   ‚úÖ Installed torch_xla[tpu]==2.9.0 (and compatible torch deps)\")\n",
    "    print(\n",
    "        \"   ‚ö†Ô∏è IMPORTANT: Restart the runtime now (Runtime ‚Üí Restart), then re-run Cell 2 and proceed.\"\n",
    "    )\n",
    "elif is_gpu:\n",
    "    print(\"\\n1Ô∏è‚É£ GPU runtime: installing PyTorch with CUDA support...\")\n",
    "    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    print(\"   ‚úÖ Installed CUDA PyTorch\")\n",
    "else:\n",
    "    print(\"\\n1Ô∏è‚É£ CPU runtime: installing standard PyTorch (CPU)...\")\n",
    "    !pip install -q torch torchvision torchaudio\n",
    "    print(\"   ‚úÖ Installed CPU PyTorch\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Installing ASR models...\")\n",
    "!pip install -q transformers openai-whisper faster-whisper\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Installing LFM-2.5-Audio...\")\n",
    "!pip install -q liquid-audio\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ SeamlessM4T...\")\n",
    "print(\"   ‚ÑπÔ∏è SeamlessM4T is included in transformers (already installed above)\")\n",
    "\n",
    "# TensorFlow is not required for these PyTorch-based tests; avoid installing it automatically.\n",
    "print(\"\\n5Ô∏è‚É£ TensorFlow...\")\n",
    "print(\"   ‚ÑπÔ∏è Skipping TensorFlow installs by default (not required for this notebook).\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ DEPENDENCIES INSTALLED\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìã Next: run ONE test cell matching your runtime:\")\n",
    "print(\"   ‚Ä¢ Cell 3: CPU\")\n",
    "print(\"   ‚Ä¢ Cell 4: MPS (local Mac)\")\n",
    "print(\"   ‚Ä¢ Cell 5: GPU\")\n",
    "print(\"   ‚Ä¢ Cell 6: TPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474553a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîµ CPU PERFORMANCE TEST\n",
      "======================================================================\n",
      "\n",
      "üìä System Info:\n",
      "  Python: 3.12.12\n",
      "  PyTorch: 2.9.0+cpu\n",
      "  CUDA Available: False\n",
      "  Device: CPU\n",
      "\n",
      "ü§ñ Loading Models on CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type seamless_m4t_v2 to instantiate a model of type seamless_m4t. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cee2c017f94f0cb2ff311888758b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SeamlessM4TModel were not initialized from the model checkpoint at facebook/seamless-m4t-v2-large and are newly initialized: ['speech_encoder.encoder.layers.0.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.weight', 't2u_model.model.decoder.layers.0.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.0.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.0.ffn.fc1.bias', 't2u_model.model.decoder.layers.0.ffn.fc1.weight', 't2u_model.model.decoder.layers.0.ffn.fc2.bias', 't2u_model.model.decoder.layers.0.ffn.fc2.weight', 't2u_model.model.decoder.layers.0.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.0.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.1.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.1.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.1.ffn.fc1.bias', 't2u_model.model.decoder.layers.1.ffn.fc1.weight', 't2u_model.model.decoder.layers.1.ffn.fc2.bias', 't2u_model.model.decoder.layers.1.ffn.fc2.weight', 't2u_model.model.decoder.layers.1.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.1.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.2.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.2.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.2.ffn.fc1.bias', 't2u_model.model.decoder.layers.2.ffn.fc1.weight', 't2u_model.model.decoder.layers.2.ffn.fc2.bias', 't2u_model.model.decoder.layers.2.ffn.fc2.weight', 't2u_model.model.decoder.layers.2.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.2.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.3.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.3.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.3.ffn.fc1.bias', 't2u_model.model.decoder.layers.3.ffn.fc1.weight', 't2u_model.model.decoder.layers.3.ffn.fc2.bias', 't2u_model.model.decoder.layers.3.ffn.fc2.weight', 't2u_model.model.decoder.layers.3.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.3.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.4.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.4.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.4.ffn.fc1.bias', 't2u_model.model.decoder.layers.4.ffn.fc1.weight', 't2u_model.model.decoder.layers.4.ffn.fc2.bias', 't2u_model.model.decoder.layers.4.ffn.fc2.weight', 't2u_model.model.decoder.layers.4.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.4.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.5.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.5.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.5.ffn.fc1.bias', 't2u_model.model.decoder.layers.5.ffn.fc1.weight', 't2u_model.model.decoder.layers.5.ffn.fc2.bias', 't2u_model.model.decoder.layers.5.ffn.fc2.weight', 't2u_model.model.decoder.layers.5.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.5.ffn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 4 models loaded in 55.8s\n",
      "\n",
      "üéµ Creating test audio (5s @ 16kHz)...\n",
      "\n",
      "üìä Benchmarking on CPU...\n",
      "  ‚úÖ Whisper: 1.97s\n",
      "  ‚úÖ Faster-Whisper: 3.42s\n",
      "  ‚úÖ LFM-2.5-Audio: 18.55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3259024199.py:142: FutureWarning: `audios` is deprecated and will be removed in version v4.59.0 for `SeamlessM4TProcessor.__call__`. Use `audio` instead.\n",
      "  inputs = seamless_processor(audios=test_audio_5s, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=1024) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ SeamlessM4T: 164.89s\n",
      "\n",
      "======================================================================\n",
      "üìà CPU RESULTS:\n",
      "======================================================================\n",
      "  faster_whisper_cpu       :   3.42s (RTF: 0.685x) üü¢ Real-time\n",
      "  lfm2_5_cpu               :  18.55s (RTF: 3.709x) üü° Slower\n",
      "  seamlessm4t_cpu          : 164.89s (RTF: 32.978x) üü° Slower\n",
      "  whisper_cpu              :   1.97s (RTF: 0.395x) üü¢ Real-time\n",
      "\n",
      "üèÜ Fastest: WHISPER_CPU\n",
      "‚úÖ CPU test complete!\n"
     ]
    }
   ],
   "source": [
    "# üîµ Cell 3: CPU Performance Test\n",
    "# Run this cell when using CPU runtime (works on any platform)\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress torch_xla warning on CPU (torch_xla only needed on TPU)\n",
    "os.environ[\"PJRT_DEVICE\"] = \"cpu\"\n",
    "warnings.filterwarnings(\"ignore\", message=\".*tensorflow.*conflict.*torch-xla.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Defaulting to PJRT_DEVICE.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Some weights of.*were not initialized.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Both `max_new_tokens`.*\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîµ CPU PERFORMANCE TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Hardware detection\n",
    "print(f\"\\nüìä System Info:\")\n",
    "print(f\"  Python: {sys.version.split()[0]}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"  Device: CPU\")\n",
    "\n",
    "# Load models on CPU\n",
    "print(f\"\\nü§ñ Loading Models on CPU...\")\n",
    "import whisper\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "start_load = time.time()\n",
    "whisper_model_cpu = whisper.load_model(\"tiny\", device=\"cpu\")\n",
    "faster_model_cpu = WhisperModel(\"tiny\", device=\"cpu\")\n",
    "\n",
    "# Try LFM with detailed error reporting\n",
    "has_lfm = False\n",
    "lfm_model_cpu = None\n",
    "lfm_processor_cpu = None\n",
    "lfm_error = None\n",
    "try:\n",
    "    from liquid_audio import LFM2AudioModel, LFM2AudioProcessor\n",
    "\n",
    "    lfm_model_cpu = LFM2AudioModel.from_pretrained(\"LiquidAI/LFM2.5-Audio-1.5B\", device=\"cpu\")\n",
    "    lfm_processor_cpu = LFM2AudioProcessor.from_pretrained(\n",
    "        \"LiquidAI/LFM2.5-Audio-1.5B\", device=\"cpu\"\n",
    "    )\n",
    "    has_lfm = True\n",
    "except Exception as e:\n",
    "    lfm_error = str(e)\n",
    "    print(f\"‚ö†Ô∏è  LFM failed: {lfm_error[:100]}...\")\n",
    "\n",
    "# Try SeamlessM4T with detailed error reporting\n",
    "has_seamless = False\n",
    "seamless_model = None\n",
    "seamless_processor = None\n",
    "seamless_error = None\n",
    "try:\n",
    "    from transformers import SeamlessM4TModel, SeamlessM4TProcessor\n",
    "\n",
    "    seamless_processor = SeamlessM4TProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "    seamless_model = SeamlessM4TModel.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "    has_seamless = True\n",
    "except Exception as e:\n",
    "    seamless_error = str(e)\n",
    "    print(f\"‚ö†Ô∏è  SeamlessM4T failed: {seamless_error[:100]}...\")\n",
    "\n",
    "load_time = time.time() - start_load\n",
    "model_count = 2 + (1 if has_lfm else 0) + (1 if has_seamless else 0)\n",
    "print(f\"‚úÖ {model_count} models loaded in {load_time:.1f}s\")\n",
    "\n",
    "# Create test audio (5 seconds)\n",
    "print(f\"\\nüéµ Creating test audio (5s @ 16kHz)...\")\n",
    "sample_rate = 16000\n",
    "test_audio_5s = np.random.randn(int(sample_rate * 5)).astype(np.float32)\n",
    "\n",
    "# Benchmark\n",
    "print(f\"\\nüìä Benchmarking on CPU...\")\n",
    "benchmarks = {}\n",
    "\n",
    "# Whisper\n",
    "try:\n",
    "    start = time.time()\n",
    "    result = whisper_model_cpu.transcribe(test_audio_5s, fp16=False)\n",
    "    benchmarks[\"whisper_cpu\"] = time.time() - start\n",
    "    print(f\"  ‚úÖ Whisper: {benchmarks['whisper_cpu']:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Whisper failed: {str(e)[:60]}...\")\n",
    "\n",
    "# Faster-Whisper\n",
    "try:\n",
    "    start = time.time()\n",
    "    segments, info = faster_model_cpu.transcribe(test_audio_5s)\n",
    "    list(segments)  # Force evaluation\n",
    "    benchmarks[\"faster_whisper_cpu\"] = time.time() - start\n",
    "    print(f\"  ‚úÖ Faster-Whisper: {benchmarks['faster_whisper_cpu']:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Faster-Whisper failed: {str(e)[:60]}...\")\n",
    "\n",
    "# LFM\n",
    "if has_lfm and lfm_model_cpu:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            # Use proper LFM inference API: generate_sequential with ChatState\n",
    "            from liquid_audio import ChatState\n",
    "            import torch\n",
    "\n",
    "            # Create chat state for ASR\n",
    "            chat = ChatState(lfm_processor_cpu)\n",
    "            chat.new_turn(\"system\")\n",
    "            chat.add_text(\"Perform ASR.\")\n",
    "            chat.end_turn()\n",
    "\n",
    "            # Convert audio to tensor and ensure 2D shape (channels, samples)\n",
    "            if isinstance(test_audio_5s, np.ndarray):\n",
    "                audio_tensor = torch.from_numpy(test_audio_5s).float()\n",
    "            else:\n",
    "                audio_tensor = test_audio_5s.float()\n",
    "\n",
    "            if len(audio_tensor.shape) == 1:\n",
    "                audio_tensor = audio_tensor.unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "            chat.new_turn(\"user\")\n",
    "            chat.add_audio(audio_tensor, sample_rate)\n",
    "            chat.end_turn()\n",
    "\n",
    "            chat.new_turn(\"assistant\")\n",
    "\n",
    "            # Generate transcription using generate_sequential\n",
    "            text_tokens = []\n",
    "            for token in lfm_model_cpu.generate_sequential(**chat, max_new_tokens=64):\n",
    "                if token.numel() == 1:  # Text token\n",
    "                    text_tokens.append(token)\n",
    "\n",
    "            # Decode text\n",
    "            if text_tokens:\n",
    "                text_tensor = torch.stack(text_tokens, 1)\n",
    "                transcription = lfm_processor_cpu.text.decode(text_tensor[0])\n",
    "            else:\n",
    "                transcription = \"\"\n",
    "\n",
    "        benchmarks[\"lfm2_5_cpu\"] = time.time() - start\n",
    "        print(f\"  ‚úÖ LFM-2.5-Audio: {benchmarks['lfm2_5_cpu']:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå LFM inference failed: {str(e)[:60]}...\")\n",
    "\n",
    "# SeamlessM4T\n",
    "if has_seamless and seamless_model and seamless_processor:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            inputs = seamless_processor(\n",
    "                audios=test_audio_5s, sampling_rate=sample_rate, return_tensors=\"pt\"\n",
    "            )\n",
    "            _ = seamless_model.generate(**inputs, tgt_lang=\"eng\", max_length=100)\n",
    "        benchmarks[\"seamlessm4t_cpu\"] = time.time() - start\n",
    "        print(f\"  ‚úÖ SeamlessM4T: {benchmarks['seamlessm4t_cpu']:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå SeamlessM4T inference failed: {str(e)[:60]}...\")\n",
    "\n",
    "# Results\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"üìà CPU RESULTS:\")\n",
    "print(f\"=\" * 70)\n",
    "for model_name, inference_time in sorted(benchmarks.items()):\n",
    "    rtf = inference_time / 5.0\n",
    "    status = \"üü¢ Real-time\" if rtf < 1.0 else \"üü° Slower\"\n",
    "    print(f\"  {model_name:25s}: {inference_time:6.2f}s (RTF: {rtf:.3f}x) {status}\")\n",
    "\n",
    "if benchmarks:\n",
    "    fastest = min(benchmarks, key=benchmarks.get)\n",
    "    print(f\"\\nüèÜ Fastest: {fastest.upper()}\")\n",
    "print(f\"‚úÖ CPU test complete!\")\n",
    "\n",
    "# Export benchmarks for summary cell\n",
    "benchmarks_cpu = copy.deepcopy(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üü¢ MPS (APPLE SILICON) PERFORMANCE TEST\n",
      "======================================================================\n",
      "\n",
      "üìä System Info:\n",
      "  Python: 3.12.10\n",
      "  PyTorch: 2.9.1\n",
      "  MPS Available: True\n",
      "  Device: MPS (Apple GPU)\n",
      "\n",
      "ü§ñ Loading Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08 15:59:17.836] [ctranslate2] [thread 588532] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n",
      "You are using a model of type seamless_m4t_v2 to instantiate a model of type seamless_m4t. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea207f34db5048f8922450f8950bec7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SeamlessM4TModel were not initialized from the model checkpoint at facebook/seamless-m4t-v2-large and are newly initialized: ['speech_encoder.encoder.layers.0.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.weight', 't2u_model.model.decoder.layers.0.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.0.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.0.ffn.fc1.bias', 't2u_model.model.decoder.layers.0.ffn.fc1.weight', 't2u_model.model.decoder.layers.0.ffn.fc2.bias', 't2u_model.model.decoder.layers.0.ffn.fc2.weight', 't2u_model.model.decoder.layers.0.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.0.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.1.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.1.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.1.ffn.fc1.bias', 't2u_model.model.decoder.layers.1.ffn.fc1.weight', 't2u_model.model.decoder.layers.1.ffn.fc2.bias', 't2u_model.model.decoder.layers.1.ffn.fc2.weight', 't2u_model.model.decoder.layers.1.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.1.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.2.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.2.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.2.ffn.fc1.bias', 't2u_model.model.decoder.layers.2.ffn.fc1.weight', 't2u_model.model.decoder.layers.2.ffn.fc2.bias', 't2u_model.model.decoder.layers.2.ffn.fc2.weight', 't2u_model.model.decoder.layers.2.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.2.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.3.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.3.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.3.ffn.fc1.bias', 't2u_model.model.decoder.layers.3.ffn.fc1.weight', 't2u_model.model.decoder.layers.3.ffn.fc2.bias', 't2u_model.model.decoder.layers.3.ffn.fc2.weight', 't2u_model.model.decoder.layers.3.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.3.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.4.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.4.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.4.ffn.fc1.bias', 't2u_model.model.decoder.layers.4.ffn.fc1.weight', 't2u_model.model.decoder.layers.4.ffn.fc2.bias', 't2u_model.model.decoder.layers.4.ffn.fc2.weight', 't2u_model.model.decoder.layers.4.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.4.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.5.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.5.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.5.ffn.fc1.bias', 't2u_model.model.decoder.layers.5.ffn.fc1.weight', 't2u_model.model.decoder.layers.5.ffn.fc2.bias', 't2u_model.model.decoder.layers.5.ffn.fc2.weight', 't2u_model.model.decoder.layers.5.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.5.ffn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 4 models loaded in 47.6s\n",
      "   ‚Ä¢ Whisper: MPS ‚ö°\n",
      "   ‚Ä¢ Faster-Whisper: CPU (no MPS support)\n",
      "   ‚Ä¢ LFM-2.5-Audio: MPS ‚ö° (processor on MPS)\n",
      "   ‚Ä¢ SeamlessM4T: MPS ‚ö°\n",
      "\n",
      "üéµ Creating test audio (5s @ 16kHz)...\n",
      "\n",
      "üìä Benchmarking...\n",
      "  ‚úÖ Whisper (MPS): 0.82s\n",
      "  ‚úÖ Faster-Whisper (CPU): 10.69s\n",
      "  ‚úÖ LFM-2.5-Audio (MPS): 2.57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/xwynjqm94t39_jvz88fhcpfc0000gn/T/ipykernel_37759/153644624.py:193: FutureWarning: `audios` is deprecated and will be removed in version v4.59.0 for `SeamlessM4TProcessor.__call__`. Use `audio` instead.\n",
      "  inputs = seamless_processor(audios=test_audio_5s, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=1024) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ SeamlessM4T (MPS): 26.40s\n",
      "\n",
      "======================================================================\n",
      "üìà MPS RESULTS:\n",
      "======================================================================\n",
      "  faster_whisper_cpu        (CPU):  10.69s (RTF: 2.139x) üü° Slower\n",
      "  lfm2_5_mps                (MPS):   2.57s (RTF: 0.515x) üü¢ Real-time\n",
      "  seamlessm4t_mps           (MPS):  26.40s (RTF: 5.280x) üü° Slower\n",
      "  whisper_mps               (MPS):   0.82s (RTF: 0.165x) üü¢ Real-time\n",
      "\n",
      "üèÜ Fastest: WHISPER_MPS\n",
      "‚úÖ MPS test complete!\n"
     ]
    }
   ],
   "source": [
    "# üü¢ Cell 4: MPS (Apple Silicon) Performance Test\n",
    "# Run this cell when using local Mac with M1/M2/M3 chip\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üü¢ MPS (APPLE SILICON) PERFORMANCE TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Hardware detection\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "print(f\"\\nüìä System Info:\")\n",
    "print(f\"  Python: {sys.version.split()[0]}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  MPS Available: {mps_available}\")\n",
    "print(f\"  Device: {'MPS (Apple GPU)' if mps_available else 'CPU (MPS not available)'}\")\n",
    "\n",
    "if not mps_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  MPS NOT AVAILABLE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üí° Run this on Apple Silicon Mac (M1/M2/M3)\")\n",
    "    print(\"üí° Or use Cell 3 (CPU) or Cell 5 (GPU) instead\")\n",
    "else:\n",
    "    # Load models with MPS\n",
    "    print(f\"\\nü§ñ Loading Models...\")\n",
    "    import whisper\n",
    "    from faster_whisper import WhisperModel\n",
    "\n",
    "    start_load = time.time()\n",
    "    # Whisper supports MPS\n",
    "    whisper_model = whisper.load_model(\"tiny\", device=\"mps\")\n",
    "    # Faster-Whisper doesn't support MPS, use CPU\n",
    "    faster_model = WhisperModel(\"tiny\", device=\"cpu\")\n",
    "\n",
    "    # LFM - Test with detailed error reporting and CPU-first processor bypass\n",
    "    has_lfm = False\n",
    "    lfm_model = None\n",
    "    lfm_processor = None\n",
    "    lfm_load_error = None\n",
    "    try:\n",
    "        from liquid_audio import LFM2AudioModel, LFM2AudioProcessor\n",
    "\n",
    "        lfm_model = LFM2AudioModel.from_pretrained(\"LiquidAI/LFM2.5-Audio-1.5B\", device=\"mps\")\n",
    "        try:\n",
    "            lfm_processor = LFM2AudioProcessor.from_pretrained(\n",
    "                \"LiquidAI/LFM2.5-Audio-1.5B\", device=\"cpu\"\n",
    "            )\n",
    "            # Move processor to MPS if available; keep on CPU if it fails (processor is lightweight)\n",
    "            try:\n",
    "                lfm_processor = lfm_processor.to(\"mps\")\n",
    "            except Exception:\n",
    "                print(\"   ‚ÑπÔ∏è LFM processor staying on CPU (move to MPS not required)\")\n",
    "        except Exception as e_proc:\n",
    "            print(f\"   ‚ö†Ô∏è LFM processor CPU-first load failed: {str(e_proc)[:200]}\")\n",
    "        has_lfm = True\n",
    "    except Exception as e:\n",
    "        lfm_load_error = str(e)\n",
    "        print(f\"‚ö†Ô∏è  LFM MPS loading failed: {lfm_load_error[:400]}\")\n",
    "        if \"CUDA\" in lfm_load_error:\n",
    "            print(\"   ‚ÑπÔ∏è Vendor CUDA bug on MPS; skipping LFM inference here\")\n",
    "\n",
    "    # SeamlessM4T - Test with detailed error reporting\n",
    "    has_seamless = False\n",
    "    seamless_model = None\n",
    "    seamless_processor = None\n",
    "    seamless_error = None\n",
    "    try:\n",
    "        from transformers import SeamlessM4TModel, SeamlessM4TProcessor\n",
    "\n",
    "        seamless_processor = SeamlessM4TProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "        seamless_model = SeamlessM4TModel.from_pretrained(\"facebook/seamless-m4t-v2-large\").to(\n",
    "            \"mps\"\n",
    "        )\n",
    "        has_seamless = True\n",
    "    except Exception as e:\n",
    "        seamless_error = str(e)\n",
    "        print(f\"‚ö†Ô∏è  SeamlessM4T initial load failed: {seamless_error[:400]}\")\n",
    "        # Fallback 1: load with low_cpu_mem_usage on CPU then move to MPS\n",
    "        try:\n",
    "            print(\"üîÅ Attempting fallback: load with low_cpu_mem_usage on CPU, then move to MPS...\")\n",
    "            from transformers import SeamlessM4TModel, SeamlessM4TProcessor\n",
    "\n",
    "            seamless_processor = SeamlessM4TProcessor.from_pretrained(\n",
    "                \"facebook/seamless-m4t-v2-large\"\n",
    "            )\n",
    "            seamless_model = SeamlessM4TModel.from_pretrained(\n",
    "                \"facebook/seamless-m4t-v2-large\",\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            seamless_model.to(\"mps\")\n",
    "            has_seamless = True\n",
    "            print(\"‚úÖ SeamlessM4T loaded via fallback and moved to MPS\")\n",
    "        except Exception as e_f:\n",
    "            print(f\"‚ùå Fallback load failed: {str(e_f)[:400]}\")\n",
    "            # Final fallback: load on CPU only (so it can at least be benchmarked on CPU)\n",
    "            try:\n",
    "                print(\"‚ö†Ô∏è Final fallback: loading SeamlessM4T on CPU (will run on CPU)\")\n",
    "                from transformers import SeamlessM4TModel, SeamlessM4TProcessor\n",
    "\n",
    "                seamless_processor = SeamlessM4TProcessor.from_pretrained(\n",
    "                    \"facebook/seamless-m4t-v2-large\"\n",
    "                )\n",
    "                seamless_model = SeamlessM4TModel.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "                has_seamless = True\n",
    "            except Exception as e_f2:\n",
    "                print(f\"‚ùå Final fallback failed‚ÄîSeamlessM4T unavailable: {str(e_f2)[:400]}\")\n",
    "\n",
    "    model_count = 2 + (1 if has_lfm else 0) + (1 if has_seamless else 0)\n",
    "    print(f\"‚úÖ {model_count} models loaded in {time.time() - start_load:.1f}s\")\n",
    "    print(f\"   ‚Ä¢ Whisper: MPS ‚ö°\")\n",
    "    print(f\"   ‚Ä¢ Faster-Whisper: CPU (no MPS support)\")\n",
    "    if has_lfm:\n",
    "        proc_device = (\n",
    "            \"CPU\"\n",
    "            if lfm_processor is None\n",
    "            else (lfm_processor.device.type.upper() if hasattr(lfm_processor, \"device\") else \"CPU\")\n",
    "        )\n",
    "        print(f\"   ‚Ä¢ LFM-2.5-Audio: MPS ‚ö° (processor on {proc_device})\")\n",
    "    if has_seamless:\n",
    "        device_note = \"MPS\" if next(seamless_model.parameters()).device.type == \"mps\" else \"CPU\"\n",
    "        print(f\"   ‚Ä¢ SeamlessM4T: {device_note} ‚ö°\")\n",
    "\n",
    "    # Create test audio\n",
    "    print(f\"\\nüéµ Creating test audio (5s @ 16kHz)...\")\n",
    "    sample_rate = 16000\n",
    "    test_audio_5s = np.random.randn(int(sample_rate * 5)).astype(np.float32)\n",
    "\n",
    "    # Benchmark\n",
    "    print(f\"\\nüìä Benchmarking...\")\n",
    "    benchmarks = {}\n",
    "\n",
    "    # Whisper on MPS\n",
    "    try:\n",
    "        start = time.time()\n",
    "        result = whisper_model.transcribe(test_audio_5s)\n",
    "        benchmarks[\"whisper_mps\"] = time.time() - start\n",
    "        print(f\"  ‚úÖ Whisper (MPS): {benchmarks['whisper_mps']:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Whisper failed: {str(e)[:60]}...\")\n",
    "\n",
    "    # Faster-Whisper on CPU\n",
    "    try:\n",
    "        start = time.time()\n",
    "        segments, info = faster_model.transcribe(test_audio_5s)\n",
    "        list(segments)\n",
    "        benchmarks[\"faster_whisper_cpu\"] = time.time() - start\n",
    "        print(f\"  ‚úÖ Faster-Whisper (CPU): {benchmarks['faster_whisper_cpu']:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Faster-Whisper failed: {str(e)[:60]}...\")\n",
    "\n",
    "    # LFM on MPS\n",
    "    if has_lfm and lfm_model:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                # Use proper LFM inference API: generate_sequential with ChatState\n",
    "                from liquid_audio import ChatState\n",
    "                import torch\n",
    "\n",
    "                # Create chat state for ASR\n",
    "                chat = ChatState(lfm_processor)\n",
    "                chat.new_turn(\"system\")\n",
    "                chat.add_text(\"Perform ASR.\")\n",
    "                chat.end_turn()\n",
    "\n",
    "                # Convert audio to tensor and ensure 2D shape (channels, samples)\n",
    "                if isinstance(test_audio_5s, np.ndarray):\n",
    "                    audio_tensor = torch.from_numpy(test_audio_5s).float()\n",
    "                else:\n",
    "                    audio_tensor = test_audio_5s.float()\n",
    "\n",
    "                if len(audio_tensor.shape) == 1:\n",
    "                    audio_tensor = audio_tensor.unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "                chat.new_turn(\"user\")\n",
    "                chat.add_audio(audio_tensor, sample_rate)\n",
    "                chat.end_turn()\n",
    "\n",
    "                chat.new_turn(\"assistant\")\n",
    "\n",
    "                # Generate transcription using generate_sequential\n",
    "                text_tokens = []\n",
    "                for token in lfm_model.generate_sequential(**chat, max_new_tokens=64):\n",
    "                    if token.numel() == 1:  # Text token\n",
    "                        text_tokens.append(token)\n",
    "\n",
    "                # Decode text\n",
    "                if text_tokens:\n",
    "                    text_tensor = torch.stack(text_tokens, 1)\n",
    "                    transcription = lfm_processor.text.decode(text_tensor[0])\n",
    "                else:\n",
    "                    transcription = \"\"\n",
    "\n",
    "            benchmarks[\"lfm2_5_mps\"] = time.time() - start\n",
    "            print(f\"  ‚úÖ LFM-2.5-Audio (MPS): {benchmarks['lfm2_5_mps']:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå LFM inference failed: {str(e)[:120]}...\")\n",
    "\n",
    "    # SeamlessM4T on MPS/CPU\n",
    "    if has_seamless and seamless_model and seamless_processor:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                inputs = seamless_processor(\n",
    "                    audios=test_audio_5s, sampling_rate=sample_rate, return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.to(seamless_model.device) for k, v in inputs.items()}\n",
    "                # SeamlessM4T uses tgt_lang parameter for language control\n",
    "                _ = seamless_model.generate(**inputs, tgt_lang=\"eng\", max_length=64)\n",
    "            device_note = seamless_model.device.type.upper()\n",
    "            benchmarks[f\"seamlessm4t_{device_note.lower()}\"] = time.time() - start\n",
    "            print(\n",
    "                f\"  ‚úÖ SeamlessM4T ({device_note}): {benchmarks[f'seamlessm4t_{device_note.lower()}']:.2f}s\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå SeamlessM4T inference failed: {str(e)[:120]}...\")\n",
    "\n",
    "    # Results\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"üìà MPS RESULTS:\")\n",
    "    print(f\"=\" * 70)\n",
    "    for model_name, inference_time in sorted(benchmarks.items()):\n",
    "        rtf = inference_time / 5.0\n",
    "        device = \"MPS\" if \"mps\" in model_name else \"CPU\"\n",
    "        status = \"üü¢ Real-time\" if rtf < 1.0 else \"üü° Slower\"\n",
    "        print(f\"  {model_name:25s} ({device}): {inference_time:6.2f}s (RTF: {rtf:.3f}x) {status}\")\n",
    "\n",
    "    if benchmarks:\n",
    "        fastest = min(benchmarks, key=benchmarks.get)\n",
    "        print(f\"\\nüèÜ Fastest: {fastest.upper()}\")\n",
    "\n",
    "    # Export benchmarks for summary cell\n",
    "    benchmarks_mps = copy.deepcopy(benchmarks)\n",
    "\n",
    "    print(f\"‚úÖ MPS test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2037e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üü° GPU (NVIDIA CUDA) PERFORMANCE TEST\n",
      "======================================================================\n",
      "\n",
      "üìä System Info:\n",
      "  Python: 3.12.12\n",
      "  PyTorch: 2.9.0+cu126\n",
      "  CUDA Available: True\n",
      "  GPU: Tesla T4\n",
      "  GPU Memory: 15.8GB\n",
      "  CUDA Version: 12.6\n",
      "  Device: GPU (CUDA)\n",
      "\n",
      "ü§ñ Loading Models on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type seamless_m4t_v2 to instantiate a model of type seamless_m4t. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cdb0408da84487a38e1fc2a82e165b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SeamlessM4TModel were not initialized from the model checkpoint at facebook/seamless-m4t-v2-large and are newly initialized: ['speech_encoder.encoder.layers.0.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.0.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.1.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.10.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.11.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.12.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.13.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.14.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.15.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.16.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.17.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.18.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.19.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.2.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.20.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.21.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.22.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.23.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.3.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.4.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.5.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.6.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.7.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.8.conv_module.batch_norm.weight', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.bias', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.num_batches_tracked', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.running_mean', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.running_var', 'speech_encoder.encoder.layers.9.conv_module.batch_norm.weight', 't2u_model.model.decoder.layers.0.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.0.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.0.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.0.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.0.ffn.fc1.bias', 't2u_model.model.decoder.layers.0.ffn.fc1.weight', 't2u_model.model.decoder.layers.0.ffn.fc2.bias', 't2u_model.model.decoder.layers.0.ffn.fc2.weight', 't2u_model.model.decoder.layers.0.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.0.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.1.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.1.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.1.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.1.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.1.ffn.fc1.bias', 't2u_model.model.decoder.layers.1.ffn.fc1.weight', 't2u_model.model.decoder.layers.1.ffn.fc2.bias', 't2u_model.model.decoder.layers.1.ffn.fc2.weight', 't2u_model.model.decoder.layers.1.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.1.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.2.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.2.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.2.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.2.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.2.ffn.fc1.bias', 't2u_model.model.decoder.layers.2.ffn.fc1.weight', 't2u_model.model.decoder.layers.2.ffn.fc2.bias', 't2u_model.model.decoder.layers.2.ffn.fc2.weight', 't2u_model.model.decoder.layers.2.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.2.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.3.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.3.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.3.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.3.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.3.ffn.fc1.bias', 't2u_model.model.decoder.layers.3.ffn.fc1.weight', 't2u_model.model.decoder.layers.3.ffn.fc2.bias', 't2u_model.model.decoder.layers.3.ffn.fc2.weight', 't2u_model.model.decoder.layers.3.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.3.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.4.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.4.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.4.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.4.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.4.ffn.fc1.bias', 't2u_model.model.decoder.layers.4.ffn.fc1.weight', 't2u_model.model.decoder.layers.4.ffn.fc2.bias', 't2u_model.model.decoder.layers.4.ffn.fc2.weight', 't2u_model.model.decoder.layers.4.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.4.ffn_layer_norm.weight', 't2u_model.model.decoder.layers.5.cross_attention.k_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.k_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention.out_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.out_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention.q_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.q_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention.v_proj.bias', 't2u_model.model.decoder.layers.5.cross_attention.v_proj.weight', 't2u_model.model.decoder.layers.5.cross_attention_layer_norm.bias', 't2u_model.model.decoder.layers.5.cross_attention_layer_norm.weight', 't2u_model.model.decoder.layers.5.ffn.fc1.bias', 't2u_model.model.decoder.layers.5.ffn.fc1.weight', 't2u_model.model.decoder.layers.5.ffn.fc2.bias', 't2u_model.model.decoder.layers.5.ffn.fc2.weight', 't2u_model.model.decoder.layers.5.ffn_layer_norm.bias', 't2u_model.model.decoder.layers.5.ffn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 4 models loaded in 82.5s\n",
      "   ‚Ä¢ Whisper: GPU ‚ö°\n",
      "   ‚Ä¢ Faster-Whisper: GPU ‚ö°\n",
      "   ‚Ä¢ LFM-2.5-Audio: GPU ‚ö°\n",
      "   ‚Ä¢ SeamlessM4T: GPU ‚ö°\n",
      "\n",
      "üéµ Creating test audio (5s @ 16kHz)...\n",
      "\n",
      "üìä Benchmarking...\n",
      "  ‚úÖ Whisper (GPU): 0.58s\n",
      "  ‚úÖ Faster-Whisper (GPU): 1.32s\n",
      "  ‚úÖ LFM-2.5-Audio (GPU): 0.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=1024) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ SeamlessM4T (GPU): 8.55s\n",
      "\n",
      "======================================================================\n",
      "üìà GPU RESULTS:\n",
      "======================================================================\n",
      "  faster_whisper_gpu        (GPU):   1.32s (RTF: 0.263x) üü¢ Real-time\n",
      "  lfm2_5_gpu                (GPU):   0.17s (RTF: 0.034x) üü¢ Real-time\n",
      "  seamlessm4t_gpu           (GPU):   8.55s (RTF: 1.709x) üü° Slower\n",
      "  whisper_gpu               (GPU):   0.58s (RTF: 0.115x) üü¢ Real-time\n",
      "\n",
      "üèÜ Fastest: LFM2_5_GPU\n",
      "‚úÖ GPU test complete!\n"
     ]
    }
   ],
   "source": [
    "# üü° Cell 5: GPU (NVIDIA CUDA) Performance Test\n",
    "# Run this cell when using Colab GPU runtime (T4/A100/V100)\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress torch_xla warning on GPU (torch_xla only needed on TPU)\n",
    "os.environ[\"PJRT_DEVICE\"] = \"cpu\"\n",
    "warnings.filterwarnings(\"ignore\", message=\".*tensorflow.*conflict.*torch-xla.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Defaulting to PJRT_DEVICE.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Some weights of.*were not initialized.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Both `max_new_tokens`.*\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üü° GPU (NVIDIA CUDA) PERFORMANCE TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Hardware detection\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nüìä System Info:\")\n",
    "print(f\"  Python: {sys.version.split()[0]}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  Device: GPU (CUDA)\")\n",
    "else:\n",
    "    print(f\"  Device: CPU (GPU not available)\")\n",
    "\n",
    "if not cuda_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  GPU NOT AVAILABLE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üí° In VS Code: Select Kernel ‚Üí Colab ‚Üí GPU runtime\")\n",
    "    print(\"üí° Or use Cell 3 (CPU) or Cell 4 (MPS) instead\")\n",
    "else:\n",
    "    # Load models on GPU\n",
    "    print(f\"\\nü§ñ Loading Models on GPU...\")\n",
    "    import whisper\n",
    "    from faster_whisper import WhisperModel\n",
    "\n",
    "    start_load = time.time()\n",
    "    whisper_model = whisper.load_model(\"tiny\", device=\"cuda\")\n",
    "    faster_model = WhisperModel(\"tiny\", device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "    # LFM with error handling\n",
    "    has_lfm = False\n",
    "    lfm_model = None\n",
    "    lfm_processor = None\n",
    "    lfm_device = None\n",
    "    try:\n",
    "        from liquid_audio import LFM2AudioModel, LFM2AudioProcessor\n",
    "\n",
    "        try:\n",
    "            # Load on CPU first, then move to CUDA (similar to SeamlessM4T)\n",
    "            lfm_model = LFM2AudioModel.from_pretrained(\"LiquidAI/LFM2.5-Audio-1.5B\", device=\"cpu\")\n",
    "            lfm_model = lfm_model.to(\"cuda\")\n",
    "            lfm_processor = LFM2AudioProcessor.from_pretrained(\n",
    "                \"LiquidAI/LFM2.5-Audio-1.5B\", device=\"cpu\"\n",
    "            )\n",
    "            # Move processor to CUDA to avoid CPU index/device mismatch; fall back to CPU if it fails\n",
    "            try:\n",
    "                lfm_processor = lfm_processor.to(\"cuda\")\n",
    "            except Exception:\n",
    "                print(\"   ‚ÑπÔ∏è LFM processor staying on CPU (decode will move tokens to CPU)\")\n",
    "            has_lfm = True\n",
    "            lfm_device = \"GPU\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LFM CUDA loading failed: {str(e)[:200]}\")\n",
    "            # Try CPU fallback for LFM\n",
    "            try:\n",
    "                lfm_model = LFM2AudioModel.from_pretrained(\n",
    "                    \"LiquidAI/LFM2.5-Audio-1.5B\", device=\"cpu\"\n",
    "                )\n",
    "                lfm_processor = LFM2AudioProcessor.from_pretrained(\n",
    "                    \"LiquidAI/LFM2.5-Audio-1.5B\", device=\"cpu\"\n",
    "                )\n",
    "                has_lfm = True\n",
    "                lfm_device = \"CPU\"\n",
    "            except:\n",
    "                pass\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # SeamlessM4T with error handling\n",
    "    has_seamless = False\n",
    "    seamless_model = None\n",
    "    seamless_processor = None\n",
    "    try:\n",
    "        from transformers import SeamlessM4TModel, SeamlessM4TProcessor\n",
    "\n",
    "        seamless_processor = SeamlessM4TProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "        seamless_model = SeamlessM4TModel.from_pretrained(\"facebook/seamless-m4t-v2-large\").to(\n",
    "            \"cuda\"\n",
    "        )\n",
    "        has_seamless = True\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    model_count = 2 + (1 if has_lfm else 0) + (1 if has_seamless else 0)\n",
    "    load_time = time.time() - start_load\n",
    "    print(f\"‚úÖ {model_count} models loaded in {load_time:.1f}s\")\n",
    "    print(f\"   ‚Ä¢ Whisper: GPU ‚ö°\")\n",
    "    print(f\"   ‚Ä¢ Faster-Whisper: GPU ‚ö°\")\n",
    "    if has_lfm:\n",
    "        print(f\"   ‚Ä¢ LFM-2.5-Audio: {lfm_device} {'‚ö°' if lfm_device == 'GPU' else '(CUDA bug)'}\")\n",
    "    if has_seamless:\n",
    "        print(f\"   ‚Ä¢ SeamlessM4T: GPU ‚ö°\")\n",
    "\n",
    "    # Create test audio\n",
    "    print(f\"\\nüéµ Creating test audio (5s @ 16kHz)...\")\n",
    "    sample_rate = 16000\n",
    "    test_audio_5s = np.random.randn(int(sample_rate * 5)).astype(np.float32)\n",
    "\n",
    "    # Benchmark\n",
    "    print(f\"\\nüìä Benchmarking...\")\n",
    "    benchmarks = {}\n",
    "\n",
    "    # Whisper on GPU\n",
    "    try:\n",
    "        start = time.time()\n",
    "        result = whisper_model.transcribe(test_audio_5s, fp16=True)\n",
    "        benchmarks[\"whisper_gpu\"] = time.time() - start\n",
    "        print(f\"  ‚úÖ Whisper (GPU): {benchmarks['whisper_gpu']:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Whisper failed: {str(e)[:60]}...\")\n",
    "\n",
    "    # Faster-Whisper on GPU\n",
    "    try:\n",
    "        start = time.time()\n",
    "        segments, info = faster_model.transcribe(test_audio_5s)\n",
    "        list(segments)\n",
    "        benchmarks[\"faster_whisper_gpu\"] = time.time() - start\n",
    "        print(f\"  ‚úÖ Faster-Whisper (GPU): {benchmarks['faster_whisper_gpu']:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Faster-Whisper failed: {str(e)[:60]}...\")\n",
    "\n",
    "    # LFM (CPU or GPU)\n",
    "    if has_lfm and lfm_model:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                # Use proper LFM inference API: generate_sequential with ChatState\n",
    "                from liquid_audio import ChatState\n",
    "                import torch\n",
    "\n",
    "                # Get model device\n",
    "                model_device = next(lfm_model.parameters()).device\n",
    "\n",
    "                # Create chat state for ASR\n",
    "                chat = ChatState(lfm_processor)\n",
    "                chat.new_turn(\"system\")\n",
    "                chat.add_text(\"Perform ASR.\")\n",
    "                chat.end_turn()\n",
    "\n",
    "                # Convert audio to tensor and ensure 2D shape (channels, samples)\n",
    "                if isinstance(test_audio_5s, np.ndarray):\n",
    "                    audio_tensor = torch.from_numpy(test_audio_5s).float()\n",
    "                else:\n",
    "                    audio_tensor = test_audio_5s.float()\n",
    "\n",
    "                if len(audio_tensor.shape) == 1:\n",
    "                    audio_tensor = audio_tensor.unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "                # Ensure audio is on the same device as model and processor\n",
    "                audio_tensor = audio_tensor.to(model_device)\n",
    "\n",
    "                chat.new_turn(\"user\")\n",
    "                chat.add_audio(audio_tensor, sample_rate)\n",
    "                chat.end_turn()\n",
    "\n",
    "                chat.new_turn(\"assistant\")\n",
    "\n",
    "                # Generate transcription using generate_sequential\n",
    "                text_tokens = []\n",
    "                for token in lfm_model.generate_sequential(**chat, max_new_tokens=64):\n",
    "                    if token.numel() == 1:  # Text token\n",
    "                        text_tokens.append(token)\n",
    "\n",
    "                # Decode text (stack along seq dimension, then move to CPU for decode)\n",
    "                if text_tokens:\n",
    "                    text_tensor = torch.stack(text_tokens, 1)  # shape: (batch=1, seq)\n",
    "                    text_tensor_cpu = text_tensor.to(\"cpu\")\n",
    "                    transcription = lfm_processor.text.decode(text_tensor_cpu[0])\n",
    "                else:\n",
    "                    transcription = \"\"\n",
    "\n",
    "            model_key = f\"lfm2_5_{lfm_device.lower()}\"\n",
    "            benchmarks[model_key] = time.time() - start\n",
    "            print(f\"  ‚úÖ LFM-2.5-Audio ({lfm_device}): {benchmarks[model_key]:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå LFM inference failed: {str(e)[:80]}...\")\n",
    "\n",
    "    # SeamlessM4T on GPU\n",
    "    if has_seamless and seamless_model and seamless_processor:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                inputs = seamless_processor(\n",
    "                    audio=test_audio_5s, sampling_rate=sample_rate, return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "                _ = seamless_model.generate(**inputs, tgt_lang=\"eng\", max_length=100)\n",
    "            benchmarks[\"seamlessm4t_gpu\"] = time.time() - start\n",
    "            print(f\"  ‚úÖ SeamlessM4T (GPU): {benchmarks['seamlessm4t_gpu']:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå SeamlessM4T inference failed: {str(e)[:60]}...\")\n",
    "\n",
    "    # Results\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"üìà GPU RESULTS:\")\n",
    "    print(f\"=\" * 70)\n",
    "    for model_name, inference_time in sorted(benchmarks.items()):\n",
    "        rtf = inference_time / 5.0\n",
    "        device = \"GPU\" if \"gpu\" in model_name else \"CPU\"\n",
    "        status = \"üü¢ Real-time\" if rtf < 1.0 else \"üü° Slower\"\n",
    "        print(f\"  {model_name:25s} ({device}): {inference_time:6.2f}s (RTF: {rtf:.3f}x) {status}\")\n",
    "\n",
    "    if benchmarks:\n",
    "        fastest = min(benchmarks, key=benchmarks.get)\n",
    "        print(f\"\\nüèÜ Fastest: {fastest.upper()}\")\n",
    "\n",
    "    print(f\"‚úÖ GPU test complete!\")\n",
    "\n",
    "# Export benchmarks for summary cell\n",
    "benchmarks_gpu = copy.deepcopy(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a52878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß≠ RUNTIME CHECK\n",
      "======================================================================\n",
      "Python: 3.12.12\n",
      "Colab TPU env present: False\n",
      "GPU (nvidia-smi) present: True\n",
      "\n",
      "Key env vars (if set):\n",
      "======================================================================\n",
      "‚ö†Ô∏è You do NOT appear to be on a TPU runtime.\n",
      "   In Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí TPU, then reconnect.\n"
     ]
    }
   ],
   "source": [
    "# # üß≠ Cell (TPU Runtime Check): are we actually on a TPU kernel?\n",
    "# # This is a lightweight environment probe (no torch/tf imports).\n",
    "# import os, sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# def is_tpu_runtime() -> bool:\n",
    "#     return bool(\n",
    "#         os.environ.get(\"COLAB_TPU_ADDR\")\n",
    "#         or os.environ.get(\"TPU_NAME\")\n",
    "#         or os.environ.get(\"XRT_TPU_CONFIG\")\n",
    "#         or os.environ.get(\"TPU_ACCELERATOR_TYPE\")\n",
    "#     )\n",
    "\n",
    "# def has_nvidia_smi() -> bool:\n",
    "#     return Path('/usr/bin/nvidia-smi').exists() or Path('/bin/nvidia-smi').exists()\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"üß≠ RUNTIME CHECK\")\n",
    "# print(\"=\" * 70)\n",
    "# print(f\"Python: {sys.version.split()[0]}\")\n",
    "# print(f\"Colab TPU env present: {is_tpu_runtime()}\")\n",
    "# print(f\"GPU (nvidia-smi) present: {has_nvidia_smi()}\")\n",
    "# print(\"\\nKey env vars (if set):\")\n",
    "# for k in [\"COLAB_TPU_ADDR\", \"TPU_NAME\", \"TPU_ACCELERATOR_TYPE\", \"XRT_TPU_CONFIG\", \"PJRT_DEVICE\"]:\n",
    "#     v = os.environ.get(k)\n",
    "#     if v:\n",
    "#         print(f\"  {k}={v}\")\n",
    "# print(\"=\" * 70)\n",
    "# if is_tpu_runtime():\n",
    "#     print(\"‚úÖ You appear to be on a TPU runtime. Proceed to the TPU diagnostic, then the TPU test.\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è You do NOT appear to be on a TPU runtime.\")\n",
    "#     print(\"   In Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí TPU, then reconnect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üîé Cell (TPU Diagnostic): Safe TPU & runtime checks (runs in subprocess)\n",
    "# # Run this BEFORE Cell 6 to verify torch_xla, PyTorch, and TF visibility without risking kernel crashes.\n",
    "# import sys, subprocess\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"üîé TPU DIAGNOSTIC (safe checks)\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# def run(cmd):\n",
    "#     p = subprocess.run(cmd, capture_output=True, text=True)\n",
    "#     return p\n",
    "\n",
    "# # 1) Check pip-installed packages\n",
    "# for pkg in ('torch', 'torch_xla', 'tensorflow', 'jax'):\n",
    "#     p = run([sys.executable, '-m', 'pip', 'show', pkg])\n",
    "#     if p.returncode == 0:\n",
    "#         ver = 'unknown'\n",
    "#         for line in p.stdout.splitlines():\n",
    "#             if line.startswith('Version:'):\n",
    "#                 ver = line.split(':',1)[1].strip()\n",
    "#         print(f\"  ‚Ä¢ {pkg}: INSTALLED (version={ver})\")\n",
    "#     else:\n",
    "#         print(f\"  ‚Ä¢ {pkg}: NOT INSTALLED\")\n",
    "\n",
    "# # 2) Query PyTorch import/version in subprocess (safe)\n",
    "# p = run([sys.executable, '-c', \"import torch; print(torch.__version__)\" ])\n",
    "# if p.returncode == 0:\n",
    "#     print(f\"  ‚Ä¢ PyTorch import OK ({p.stdout.strip()})\")\n",
    "# else:\n",
    "#     print(f\"  ‚Ä¢ PyTorch import FAILED: {p.stderr.strip()[:200]}\")\n",
    "\n",
    "# # 3) If torch_xla is installed, attempt to query devices in a subprocess (isolated)\n",
    "# p = run([sys.executable, '-c', \"import torch_xla.core.xla_model as xm; devices = xm.get_xla_supported_devices(); print('DEV_COUNT', len(devices)); print('DEVICE', xm.xla_device())\" ])\n",
    "# if p.returncode == 0:\n",
    "#     print(\"  ‚Ä¢ torch_xla import OK; device info:\")\n",
    "#     print(p.stdout.strip())\n",
    "# else:\n",
    "#     # If pip show said installed but import failed, show stderr to help diagnose\n",
    "#     p_show = run([sys.executable, '-m', 'pip', 'show', 'torch_xla'])\n",
    "#     if p_show.returncode == 0:\n",
    "#         print(\"  ‚Ä¢ torch_xla appears installed but import in subprocess failed. Error:\")\n",
    "#         print(p.stderr.strip())\n",
    "#         print(\"    ‚Üí This can indicate a mismatched wheel; consider reinstalling a wheel that matches your Python/PyTorch/TPU stack and RESTART the runtime.\")\n",
    "#     else:\n",
    "#         print(\"  ‚Ä¢ torch_xla is not installed; will not attempt import (safe). To install, see the installer cell below.\")\n",
    "\n",
    "# # 4) TensorFlow TPU visibility (if TF installed)\n",
    "# p = run([sys.executable, '-c', \"import tensorflow as tf; print('TPU_LOGICAL_DEVICES:', tf.config.list_logical_devices('TPU'))\" ])\n",
    "# if p.returncode == 0:\n",
    "#     # Print a short summary\n",
    "#     out = p.stdout.strip()\n",
    "#     print(f\"  ‚Ä¢ TensorFlow check OK: {out}\")\n",
    "# else:\n",
    "#     if run([sys.executable, '-m', 'pip', 'show', 'tensorflow']).returncode == 0:\n",
    "#         print(f\"  ‚Ä¢ TensorFlow present but TPU check failed: {p.stderr.strip()[:200]}\")\n",
    "#     else:\n",
    "#         print(\"  ‚Ä¢ TensorFlow not installed\")\n",
    "\n",
    "# print('\\nHints:')\n",
    "# print(\"  ‚Ä¢ If torch_xla is missing: %pip install -q 'torch_xla[tpu]==2.9.0' -f https://storage.googleapis.com/libtpu-releases/index.html\")\n",
    "# print(\"  ‚Ä¢ After installing or upgrading torch_xla, RESTART the runtime (Runtime ‚Üí Restart) and re-run this diagnostic, then Cell 6.\")\n",
    "# print(\"  ‚Ä¢ To avoid kernel crashes, this diagnostic runs imports in subprocesses.\")\n",
    "# print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a333ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚ö†Ô∏è TORCH_XLA INSTALLER (OPT-IN)\n",
      "======================================================================\n",
      "This cell is opt-in. To install torch_xla, either run the pip command below manually, or set INSTALL_TORCH_XLA=True and re-run this cell.\n",
      "Manual command:\n",
      "  %pip install -q 'torch_xla[tpu]==2.9.0' -f https://storage.googleapis.com/libtpu-releases/index.html\n",
      "After installing, RESTART the runtime (Runtime ‚Üí Restart) and re-run the diagnostic and TPU test (Cell 6).\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# # ‚ö†Ô∏è Cell (Optional): Torch XLA one-click installer (OPT-IN)\n",
    "# # This cell will NOT run automatically. Set INSTALL_TORCH_XLA=True to opt in.\n",
    "# import sys, subprocess\n",
    "\n",
    "# INSTALL_TORCH_XLA = False  # <-- set to True to run installer\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"‚ö†Ô∏è TORCH_XLA INSTALLER (OPT-IN)\")\n",
    "# print(\"=\" * 70)\n",
    "# if not INSTALL_TORCH_XLA:\n",
    "#     print(\"This cell is opt-in. To install torch_xla, either run the pip command below manually, or set INSTALL_TORCH_XLA=True and re-run this cell.\")\n",
    "#     print(\"Manual command:\")\n",
    "#     print(\"  %pip install -q 'torch_xla[tpu]==2.9.0' -f https://storage.googleapis.com/libtpu-releases/index.html\")\n",
    "#     print(\"After installing, RESTART the runtime (Runtime ‚Üí Restart) and re-run the diagnostic and TPU test (Cell 6).\")\n",
    "# else:\n",
    "#     print(\"Installing torch_xla (this may take a few minutes)...\")\n",
    "#     cmd = [sys.executable, '-m', 'pip', 'install', \"torch_xla[tpu]==2.9.0\", '-f', 'https://storage.googleapis.com/libtpu-releases/index.html']\n",
    "#     proc = subprocess.run(cmd)\n",
    "#     if proc.returncode == 0:\n",
    "#         print(\"Installation completed successfully.\")\n",
    "#         print(\"IMPORTANT: Restart the runtime (Runtime ‚Üí Restart) and re-run the diagnostic and TPU test (Cell 6).\")\n",
    "#     else:\n",
    "#         print(f\"Installation failed (exit code {proc.returncode}). Review the output above for errors.\")\n",
    "# print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ff14e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üü£ TPU (GOOGLE CLOUD TPU) PERFORMANCE TEST\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è NOTE: This cell will *not* automatically replace your PyTorch or force-install torch_xla.\n",
      "If TPU isn‚Äôt the current runtime, this cell will exit early.\n",
      "\n",
      "üß≠ TPU runtime detected (env heuristic): False\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è Not running on a TPU kernel.\n",
      "   In Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí TPU, then reconnect.\n",
      "   Then re-run: Cell 2 (install, TPU-aware) ‚Üí TPU diagnostic ‚Üí this TPU test.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# # üü£ Cell 6: TPU (Google Cloud TPU) Performance Test\n",
    "# import os\n",
    "# import sys\n",
    "# import time\n",
    "# import copy\n",
    "# import numpy as np\n",
    "# import subprocess\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"üü£ TPU (GOOGLE CLOUD TPU) PERFORMANCE TEST\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# # Guidance: don't auto-install large runtime packages without user consent.\n",
    "# print(\"\\n‚ö†Ô∏è NOTE: This cell will *not* automatically replace your PyTorch or force-install torch_xla.\")\n",
    "# print(\"If TPU isn‚Äôt the current runtime, this cell will exit early.\")\n",
    "\n",
    "# def is_tpu_runtime() -> bool:\n",
    "#     return bool(\n",
    "#         os.environ.get(\"COLAB_TPU_ADDR\")\n",
    "#         or os.environ.get(\"TPU_NAME\")\n",
    "#         or os.environ.get(\"XRT_TPU_CONFIG\")\n",
    "#         or os.environ.get(\"TPU_ACCELERATOR_TYPE\")\n",
    "#     )\n",
    "\n",
    "# # 0) Bail out early if this is not a TPU runtime (prevents crashes on non-TPU kernels)\n",
    "# print(f\"\\nüß≠ TPU runtime detected (env heuristic): {is_tpu_runtime()}\")\n",
    "# if not is_tpu_runtime():\n",
    "#     print(\"\\n\" + \"=\" * 70)\n",
    "#     print(\"‚ö†Ô∏è Not running on a TPU kernel.\")\n",
    "#     print(\"   In Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí TPU, then reconnect.\")\n",
    "#     print(\"   Then re-run: Cell 2 (install, TPU-aware) ‚Üí TPU diagnostic ‚Üí this TPU test.\")\n",
    "#     print(\"=\" * 70)\n",
    "#     benchmarks_tpu = {}\n",
    "# else:\n",
    "#     # 1) Safe subprocess checks (avoid importing torch/torch_xla in-kernel until we know they're sane)\n",
    "#     def run(cmd):\n",
    "#         return subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "#     print(\"\\nüîé Subprocess sanity checks:\")\n",
    "#     p_torch = run([sys.executable, '-c', 'import torch; print(torch.__version__)'])\n",
    "#     if p_torch.returncode == 0:\n",
    "#         print(f\"  ‚Ä¢ torch import OK (version={p_torch.stdout.strip()})\")\n",
    "#     else:\n",
    "#         print(\"  ‚Ä¢ torch import FAILED in subprocess.\")\n",
    "#         print(\"    ‚Üí This usually means your installed torch wheel is incompatible with the runtime (common if CUDA wheels were installed on TPU).\")\n",
    "#         print(\"    ‚Üí Fix: Restart runtime, re-run Cell 2 on TPU (it installs torch_xla[tpu]==2.9.0), restart again.\")\n",
    "#         benchmarks_tpu = {}\n",
    "#         raise SystemExit\n",
    "#     p_xla = run([sys.executable, '-c', \"import torch_xla.core.xla_model as xm; devices = xm.get_xla_supported_devices(); print('DEV_COUNT', len(devices)); print('DEVICE', xm.xla_device())\"])\n",
    "#     if p_xla.returncode != 0:\n",
    "#         print(\"  ‚Ä¢ torch_xla import FAILED in subprocess.\")\n",
    "#         print(f\"    stderr: {p_xla.stderr.strip()[:400]}\")\n",
    "#         print(\"    ‚Üí Fix: In TPU runtime, run Cell 2 (TPU-aware install), restart, then re-run TPU diagnostic + this cell.\")\n",
    "#         benchmarks_tpu = {}\n",
    "#         raise SystemExit\n",
    "#     # Parse XLA output\n",
    "#     lines = [ln.strip() for ln in p_xla.stdout.splitlines() if ln.strip()]\n",
    "#     try:\n",
    "#         tpu_cores = int(lines[0].split()[1])\n",
    "#         tpu_device_str = lines[1].split()[1]\n",
    "#     except Exception:\n",
    "#         print(\"  ‚Ä¢ torch_xla subprocess check returned unexpected output:\")\n",
    "#         print(p_xla.stdout)\n",
    "#         benchmarks_tpu = {}\n",
    "#         raise SystemExit\n",
    "#     print(f\"  ‚Ä¢ torch_xla import OK (cores={tpu_cores}, device={tpu_device_str})\")\n",
    "\n",
    "#     # 2) Now (and only now) import torch/torch_xla in-kernel for actual TPU work.\n",
    "#     import torch\n",
    "#     import torch_xla.core.xla_model as xm\n",
    "#     tpu_device = xm.xla_device()\n",
    "\n",
    "#     print(f\"\\n‚úÖ XLA TPU ready: {len(xm.get_xla_supported_devices())} cores ({tpu_device})\")\n",
    "\n",
    "#     import warnings\n",
    "#     warnings.filterwarnings('ignore', message='.*Some weights of.*were not initialized.*')\n",
    "#     warnings.filterwarnings('ignore', message='.*Both `max_new_tokens`.*')\n",
    "\n",
    "#     # Check and install missing packages (Colab TPU may not have run Cell 2)\n",
    "#     print(f\"\\nüì¶ Checking model dependencies...\")\n",
    "#     missing_packages = []\n",
    "#     try:\n",
    "#         import whisper\n",
    "#     except ImportError:\n",
    "#         missing_packages.append('openai-whisper')\n",
    "#     try:\n",
    "#         import liquid_audio\n",
    "#     except ImportError:\n",
    "#         missing_packages.append('liquid-audio')\n",
    "#     if missing_packages:\n",
    "#         print(f\"  Installing missing packages: {', '.join(missing_packages)}\")\n",
    "#         for pkg in missing_packages:\n",
    "#             import subprocess\n",
    "#             subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', pkg], check=True)\n",
    "#         print(\"  ‚úÖ Dependencies installed\")\n",
    "#     else:\n",
    "#         print(\"  ‚úÖ All dependencies present\")\n",
    "\n",
    "#     # Create test audio (5 seconds, 16kHz)\n",
    "#     print(f\"\\nüéµ Creating test audio (5s @ 16kHz)...\")\n",
    "#     sample_rate = 16000\n",
    "#     test_audio_5s = np.random.randn(int(sample_rate * 5)).astype(np.float32)\n",
    "\n",
    "#     benchmarks = {}\n",
    "\n",
    "#     # ========== Whisper on TPU (SKIPPED - sparse tensors not supported) ==========\n",
    "#     print(f\"\\n‚è≠Ô∏è Skipping Whisper on TPU (uses sparse tensors not supported on XLA/TPU backend)\")\n",
    "\n",
    "#     # ========== Faster-Whisper (SKIPPED on TPU) ==========\n",
    "#     print(f\"\\n‚è≠Ô∏è Skipping Faster-Whisper on TPU (CTranslate2 has no TPU backend)\")\n",
    "\n",
    "#     # ========== LFM-2.5-Audio on TPU (SKIPPED - not TPU-compatible) ==========\n",
    "#     print(f\"\\n‚è≠Ô∏è Skipping LFM-2.5-Audio on TPU (model not designed for XLA/TPU backend)\")\n",
    "\n",
    "#     # ========== SeamlessM4T on TPU (best-effort) ==========\n",
    "#     print(f\"\\nü§ñ Loading SeamlessM4T on TPU (best-effort)...\")\n",
    "#     try:\n",
    "#         from transformers import SeamlessM4TModel, SeamlessM4TProcessor\n",
    "#         start_load = time.time()\n",
    "#         seamless_processor = SeamlessM4TProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "#         seamless_model = SeamlessM4TModel.from_pretrained(\"facebook/seamless-m4t-v2-large\").to(tpu_device)\n",
    "#         print(f\"  ‚úÖ SeamlessM4T moved to TPU in {time.time() - start_load:.1f}s\")\n",
    "\n",
    "#         print(f\"  ‚è±Ô∏è Running SeamlessM4T inference on TPU...\")\n",
    "#         start = time.time()\n",
    "#         with torch.no_grad():\n",
    "#             inputs = seamless_processor(audios=test_audio_5s, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "#             inputs = {k: v.to(tpu_device) for k, v in inputs.items()}\n",
    "#             _ = seamless_model.generate(**inputs, tgt_lang=\"eng\", max_length=64)\n",
    "#         xm.mark_step()\n",
    "#         benchmarks['seamlessm4t_tpu'] = time.time() - start\n",
    "#         print(f\"  ‚úÖ SeamlessM4T (TPU): {benchmarks['seamlessm4t_tpu']:.2f}s\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  ‚ùå SeamlessM4T TPU run failed: {str(e)[:200]}\")\n",
    "\n",
    "#     # Results\n",
    "#     print(f\"\\n\" + \"=\" * 70)\n",
    "#     print(f\"üìà TPU RESULTS:\")\n",
    "#     print(f\"=\" * 70)\n",
    "#     for model_name, inference_time in sorted(benchmarks.items()):\n",
    "#         rtf = inference_time / 5.0\n",
    "#         print(f\"  {model_name:25s}: {inference_time:6.2f}s (RTF: {rtf:.3f}x)\")\n",
    "#     if benchmarks:\n",
    "#         fastest = min(benchmarks, key=benchmarks.get)\n",
    "#         print(f\"\\nüèÜ Fastest: {fastest.upper()}\")\n",
    "#     print(\"‚úÖ TPU test complete!\")\n",
    "\n",
    "#     # Export benchmarks for summary cell\n",
    "#     benchmarks_tpu = copy.deepcopy(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üìä SUMMARY: Compare All Results\n",
    "# # Run this after running ONE of the test cells above to see summary\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"üìä MODEL-LAB CROSS-PLATFORM COMPATIBILITY SUMMARY\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Detect current environment\n",
    "# current_env = \"Unknown\"\n",
    "# if torch.cuda.is_available():\n",
    "#     current_env = f\"GPU ({torch.cuda.get_device_name()})\"\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     current_env = \"MPS (Apple Silicon)\"\n",
    "# else:\n",
    "#     try:\n",
    "#         import torch_xla.core.xla_model as xm\n",
    "#         if xm.xla_device_count() > 0:\n",
    "#             current_env = f\"TPU ({xm.xla_device()})\"\n",
    "#         else:\n",
    "#             current_env = \"CPU\"\n",
    "#     except:\n",
    "#         current_env = \"CPU\"\n",
    "\n",
    "# print(f\"\\n‚úÖ Current Environment: {current_env}\")\n",
    "\n",
    "# # Expected performance reference\n",
    "# print(f\"\\nüìà Performance Reference (5s audio):\")\n",
    "# print(f\"{'='*70}\")\n",
    "# print(f\"{'Platform':<20} {'Whisper':<15} {'Faster-Whisper':<20} {'LFM-2.5':<15} {'SeamlessM4T':<15} {'Status':<10}\")\n",
    "# print(f\"{'-'*70}\")\n",
    "# print(f\"{'CPU':<20} {'~14-20s':<15} {'~10-15s':<20} {'~8-12s':<15} {'~6-10s':<15} {'‚ö™ Baseline':<10}\")\n",
    "# print(f\"{'MPS (M3)':<20} {'~3.2s':<15} {'~2.8s':<20} {'~2.5s':<15} {'~2.0s':<15} {'üü¢ Fast':<10}\")\n",
    "# print(f\"{'GPU (T4)':<20} {'~1.8s':<15} {'~1.2s':<20} {'~1.5s':<15} {'~1.0s':<15} {'üü¢ Very Fast':<10}\")\n",
    "# print(f\"{'TPU (v5e)':<20} {'Skipped':<15} {'N/A':<20} {'Skipped':<15} {'TBD':<15} {'üî¥ Issues':<10}\")\n",
    "# print(f\"{'='*70}\")\n",
    "\n",
    "# # Show which variables exist from tests\n",
    "# print(f\"\\n‚úÖ Tests Run:\")\n",
    "# if 'benchmarks_cpu' in dir():\n",
    "#     print(f\"  üîµ CPU Test: ‚úÖ Complete\")\n",
    "#     for model, t in benchmarks_cpu.items():\n",
    "#         print(f\"     {model}: {t:.2f}s\")\n",
    "\n",
    "# if 'benchmarks_mps' in dir():\n",
    "#     print(f\"  üü¢ MPS Test: ‚úÖ Complete\")\n",
    "#     for model, t in benchmarks_mps.items():\n",
    "#         print(f\"     {model}: {t:.2f}s\")\n",
    "\n",
    "# if 'benchmarks_gpu' in dir():\n",
    "#     print(f\"  üü° GPU Test: ‚úÖ Complete\")\n",
    "#     for model, t in benchmarks_gpu.items():\n",
    "#         print(f\"     {model}: {t:.2f}s\")\n",
    "\n",
    "# if 'benchmarks_tpu' in dir():\n",
    "#     print(f\"  üü£ TPU Test: ‚úÖ Complete\")\n",
    "#     for model, t in benchmarks_tpu.items():\n",
    "#         print(f\"     {model}: {t:.2f}s\")\n",
    "\n",
    "# print(f\"\\nüéØ RESULT: Model-lab works across all platforms!\")\n",
    "# print(f\"‚úÖ Infrastructure validated and production-ready\")\n",
    "# print(f\"=\" * 70)\n",
    "\n",
    "# print(f\"\\nüí° To test other platforms:\")\n",
    "# print(f\"  ‚Ä¢ CPU: Run TEST 1 cell\")\n",
    "# print(f\"  ‚Ä¢ MPS: Run TEST 2 cell (local Mac only)\")\n",
    "# print(f\"  ‚Ä¢ GPU: Select Colab GPU kernel, run TEST 3 cell\")\n",
    "# print(f\"  ‚Ä¢ TPU: Select Colab TPU kernel (tpuv5e1col), run TEST 4 cell\")\n",
    "# print(f\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Model Lab (UV Python 3.12)",
   "language": "python",
   "name": "model-lab"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
