{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFM-2.5-Audio Working Test\n",
    "\n",
    "Simple working test of LFM-2.5-Audio model for transcription and audio processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and imports\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent / \"harness\"))\n",
    "\n",
    "from liquid_audio import ChatState, LFM2AudioModel, LFM2AudioProcessor\n",
    "from liquid_audio.processor import PreprocessorConfig\n",
    "\n",
    "# Setup device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device.upper()}\")\n",
    "\n",
    "# Model info\n",
    "HF_REPO = \"LiquidAI/LFM2.5-Audio-1.5B\"\n",
    "print(f\"Model: {HF_REPO}\")\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load model components\n",
    "print(\"Loading LFM model components...\")\n",
    "\n",
    "# Create processor\n",
    "processor = LFM2AudioProcessor(\n",
    "    text_tokenizer_path=HF_REPO,\n",
    "    audio_processor_config=PreprocessorConfig(\n",
    "        sample_rate=24000,\n",
    "        features=128,\n",
    "        normalize=\"per_feature\",\n",
    "        window_size=0.02,\n",
    "        window_stride=0.01,\n",
    "        window=\"hann\",\n",
    "        n_fft=512,\n",
    "        log=True,\n",
    "        frame_splicing=1,\n",
    "        dither=1e-5,\n",
    "        pad_to=16,\n",
    "        pad_value=0,\n",
    "    ),\n",
    ")\n",
    "print(\"‚úÖ Audio processor ready\")\n",
    "\n",
    "# Load model\n",
    "model = LFM2AudioModel.from_pretrained(HF_REPO, device=device)\n",
    "model.eval()\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "\n",
    "# Create chat state\n",
    "chat = ChatState(processor)\n",
    "print(\"‚úÖ Chat state initialized\")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocabulary size: {len(processor.text_tokenizer):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load test audio\n",
    "audio_path = Path(\"data/audio/clean_speech_10s.wav\")\n",
    "\n",
    "if audio_path.exists():\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(str(audio_path))\n",
    "    print(f\"‚úÖ Audio loaded: {waveform.shape}, sample rate: {sr}\")\n",
    "    print(f\"   Duration: {waveform.shape[1] / sr:.1f}s\")\n",
    "\n",
    "    # Convert to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        print(\"‚úÖ Converted to mono\")\n",
    "\n",
    "    # Resample to 24kHz if needed (LFM expects 24kHz)\n",
    "    if sr != 24000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 24000)\n",
    "        waveform = resampler(waveform)\n",
    "        print(\"‚úÖ Resampled to 24kHz\")\n",
    "\n",
    "    print(f\"Final audio shape: {waveform.shape}\")\n",
    "else:\n",
    "    print(f\"‚ùå Audio file not found: {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prepare audio for LFM\n",
    "# LFM expects audio in a specific format with mel spectrogram preprocessing\n",
    "\n",
    "\n",
    "def prepare_audio_for_lfm(waveform, processor):\n",
    "    \"\"\"Prepare audio waveform for LFM processing\"\"\"\n",
    "    # The processor expects audio in a specific format\n",
    "    # This involves converting to mel spectrograms internally\n",
    "\n",
    "    # Normalize audio\n",
    "    waveform = waveform / waveform.abs().max()\n",
    "\n",
    "    return waveform\n",
    "\n",
    "\n",
    "processed_audio = prepare_audio_for_lfm(waveform, processor)\n",
    "print(\"‚úÖ Audio prepared for LFM processing\")\n",
    "print(f\"Processed audio shape: {processed_audio.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Simple transcription test\n",
    "print(\"Testing LFM transcription...\")\n",
    "print(\"Note: This is a basic test - full implementation requires understanding LFM API\")\n",
    "\n",
    "# For now, let's test the chat interface\n",
    "chat.new_turn(\"user\")\n",
    "\n",
    "# We need to understand how LFM expects audio input\n",
    "# The liquid-audio library has specific input formats\n",
    "\n",
    "print(\"üîß LFM system loaded successfully\")\n",
    "print(\"üìù Ready for transcription testing\")\n",
    "print(\"‚ö†Ô∏è  Note: Full transcription requires understanding specific LFM API format\")\n",
    "\n",
    "# Test that we can access model components\n",
    "print(f\"Model has {len(model._modules)} main components\")\n",
    "for name, module in model._modules.items():\n",
    "    print(f\"  - {name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Results and timing\n",
    "\n",
    "# Test inference speed\n",
    "start_time = time.time()\n",
    "\n",
    "# Simple test - run a dummy forward pass\n",
    "with torch.no_grad():\n",
    "    # This is just to test the model works\n",
    "    # Actual transcription requires proper input format\n",
    "    dummy_input = torch.randint(0, len(processor.text_tokenizer), (1, 10)).to(device)\n",
    "    # We can't run this without knowing the exact input format\n",
    "    pass\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"‚è±Ô∏è  Setup time: {(end_time - start_time) * 1000:.1f}ms\")\n",
    "print(\"‚úÖ LFM system is ready for systematic testing\")\n",
    "print()\n",
    "print(\"üéØ Next steps:\")\n",
    "print(\"   1. Understand exact LFM input/output format\")\n",
    "print(\"   2. Implement proper transcription pipeline\")\n",
    "print(\"   3. Test with canonical audio files\")\n",
    "print(\"   4. Compare with other models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}