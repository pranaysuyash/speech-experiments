model_id,family,version,local_or_api,category,tasks_supported,languages,streaming_support,realtime_latency_target_ms,base_weights,runtime,quantization,hardware_reqs,context_or_max_audio,benchmarks,license,pricing,docs_url,community_signal,integration_complexity,known_failure_modes,ideal_use_cases,avoid_when,notes,implementation_priority,maturity_level,latency_class,cost_class,packaging_targets,eval_suites,evidence_links,last_reviewed_utc,consent_risk,maintenance_risk
whisper-tiny-openai-whisper-pytorch-fp32,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-openai-whisper-pytorch-fp16,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-transformers-hf-fp32,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-transformers-hf-fp16,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-faster-whisper-ctranslate2-fp16,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-faster-whisper-ctranslate2-int8,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-faster-whisper-ctranslate2-int4,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-whisper-cpp-fp16,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-whisper-cpp-int8,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-whisper-cpp-int4,Whisper,tiny,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-tiny,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-openai-whisper-pytorch-fp32,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-openai-whisper-pytorch-fp16,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-transformers-hf-fp32,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-transformers-hf-fp16,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-faster-whisper-ctranslate2-fp16,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-faster-whisper-ctranslate2-int8,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-faster-whisper-ctranslate2-int4,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-fp16,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-int8,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-int4,Whisper,base,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-base,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-openai-whisper-pytorch-fp32,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-openai-whisper-pytorch-fp16,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-transformers-hf-fp32,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-transformers-hf-fp16,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-faster-whisper-ctranslate2-fp16,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-faster-whisper-ctranslate2-int8,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-faster-whisper-ctranslate2-int4,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-fp16,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-int8,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-int4,Whisper,small,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-small,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-openai-whisper-pytorch-fp32,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-openai-whisper-pytorch-fp16,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-transformers-hf-fp32,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-transformers-hf-fp16,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-faster-whisper-ctranslate2-fp16,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-faster-whisper-ctranslate2-int8,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-faster-whisper-ctranslate2-int4,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-fp16,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-int8,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-int4,Whisper,medium,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-medium,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-openai-whisper-pytorch-fp32,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-openai-whisper-pytorch-fp16,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-transformers-hf-fp32,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-transformers-hf-fp16,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-faster-whisper-ctranslate2-fp16,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-faster-whisper-ctranslate2-int8,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-faster-whisper-ctranslate2-int4,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-fp16,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-int8,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-int4,Whisper,large-v2,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v2,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-openai-whisper-pytorch-fp32,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-openai-whisper-pytorch-fp16,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-transformers-hf-fp32,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-transformers-hf-fp16,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-faster-whisper-ctranslate2-fp16,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-faster-whisper-ctranslate2-int8,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-faster-whisper-ctranslate2-int4,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-fp16,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-int8,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-int4,Whisper,large-v3,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-openai-whisper-pytorch-fp32,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-openai-whisper-pytorch-fp16,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-transformers-hf-fp32,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-transformers-hf-fp16,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-faster-whisper-ctranslate2-fp16,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-faster-whisper-ctranslate2-int8,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-faster-whisper-ctranslate2-int4,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-fp16,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-int8,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-int4,Whisper,large-v3-turbo,Local,ASR,"ASR, language ID, transcription, (optional) translation via task=translate",multilingual (except *.en variants where applicable),Chunked streaming; near real-time possible with small chunks,,whisper-large-v3-turbo,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; performance varies by runtime and quantization,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",General-purpose transcription; offline workflows; robust baseline across many languages,Hard real-time low-latency requirements without careful chunking; extremely noisy multi-speaker rooms,Treat each runtime and quantization combo as a separate deployable variant.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-openai-whisper-pytorch-fp32,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-openai-whisper-pytorch-fp16,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-transformers-hf-fp32,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-transformers-hf-fp16,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-faster-whisper-ctranslate2-fp16,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-faster-whisper-ctranslate2-int8,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-faster-whisper-ctranslate2-int4,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-fp16,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-int8,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-int4,Whisper,tiny.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-tiny.en,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-openai-whisper-pytorch-fp32,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-openai-whisper-pytorch-fp16,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-transformers-hf-fp32,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-transformers-hf-fp16,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-faster-whisper-ctranslate2-fp16,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-faster-whisper-ctranslate2-int8,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-faster-whisper-ctranslate2-int4,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-fp16,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-int8,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-int4,Whisper,base.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-base.en,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-openai-whisper-pytorch-fp32,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-openai-whisper-pytorch-fp16,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-transformers-hf-fp32,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-transformers-hf-fp16,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-faster-whisper-ctranslate2-fp16,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-faster-whisper-ctranslate2-int8,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-faster-whisper-ctranslate2-int4,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-fp16,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-int8,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-int4,Whisper,small.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-small.en,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-openai-whisper-pytorch-fp32,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,openai-whisper (PyTorch),fp32,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-openai-whisper-pytorch-fp16,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,openai-whisper (PyTorch),fp16,CPU or GPU; Apple Silicon supported via PyTorch/Metal in some setups,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-transformers-hf-fp32,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,transformers (HF),fp32,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-transformers-hf-fp16,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,transformers (HF),fp16,CPU or GPU; best with GPU for batch,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-faster-whisper-ctranslate2-fp16,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-faster-whisper-ctranslate2-int8,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-faster-whisper-ctranslate2-int4,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for high throughput; int8 works well on CPU,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-fp16,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,whisper.cpp,fp16,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-int8,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-int4,Whisper,medium.en,Local,ASR,"ASR (English-only), transcription",English,Chunked streaming; near real-time possible with small chunks,,whisper-medium.en,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available; very small footprint with int4/int8,Chunked processing; 30s windows typical; can run longer with segmentation.,See runtime docs; English-only variants can be faster at similar size,See model and runtime repos (OpenAI released Whisper weights; code license varies by implementation),,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",English transcription with lower compute than multilingual models,Any non-English content,English-only checkpoints.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-tiny,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-tiny,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-base,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-base,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-small,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-small,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-medium,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-medium,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-large-v2,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-large-v2,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-large-v3,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-large-v3,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-large-v3-turbo,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-large-v3-turbo,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-tiny.en,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-tiny.en,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-base.en,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-base.en,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-small.en,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-small.en,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisperx-on-whisper-medium.en,WhisperX,latest,Local,ASR+Alignment+Diarization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-medium.en,WhisperX pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/m-bain/whisperX,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Adds word-level timestamps and diarization glue; depends on diarization backend,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
stable-ts-on-whisper-tiny,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-tiny,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-base,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-base,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-small,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-small,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-medium,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-medium,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-large-v2,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-large-v2,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-large-v3,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-large-v3,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-large-v3-turbo,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-large-v3-turbo,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-tiny.en,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-tiny.en,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-base.en,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-base.en,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-small.en,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-small.en,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
stable-ts-on-whisper-medium.en,stable-ts,latest,Local,ASR+Timestamp stabilization,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-medium.en,stable-ts pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/jianfch/stable-ts,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Post-processing for better timestamps and subtitles; not a new base ASR model,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-tiny,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-tiny,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-base,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-base,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-small,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-small,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-medium,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-medium,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-large-v2,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-large-v2,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-large-v3,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-large-v3,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-large-v3-turbo,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-large-v3-turbo,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-tiny.en,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-tiny.en,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-base.en,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-base.en,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-small.en,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-small.en,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
whisper-timestamped-on-whisper-medium.en,whisper-timestamped,latest,Local,ASR+Word timestamps,Transcription + timestamps (and diarization where applicable),Depends on underlying Whisper checkpoint,Usually batch or chunked; diarization may be batch-oriented,,whisper-medium.en,whisper-timestamped pipeline,depends on underlying runtime,Depends on underlying Whisper runtime plus extra alignment/diarization components,Often better on longer audio; alignment adds compute,Varies; alignment and diarization can dominate runtime,See fork repo; underlying components have their own licenses,,https://github.com/linto-ai/whisper-timestamped,high,high,Diarization errors on overlapping speech; word timestamps can drift; GPU memory spikes on long files,"Subtitle generation, word-level search, speaker-attributed transcripts",Ultra low latency live captions without heavy optimization,Word timestamps from Whisper output; quality depends on audio and settings,,,,,,,,2026-02-05T05:38:30Z,,
distil-whisper-large-v3-transformers-hf-fp16,Distil-Whisper,distil-whisper-large-v3,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,large-v3,transformers (HF),fp16,CPU or GPU; fp16 on GPU recommended,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-large-v3-transformers-hf-int8,Distil-Whisper,distil-whisper-large-v3,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,large-v3,transformers (HF),int8,CPU or GPU; fp16 on GPU recommended,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-large-v3-faster-whisper-ctranslate2-fp16,Distil-Whisper,distil-whisper-large-v3,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,large-v3,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for throughput,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-large-v3-faster-whisper-ctranslate2-int8,Distil-Whisper,distil-whisper-large-v3,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,large-v3,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for throughput,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-large-v3-faster-whisper-ctranslate2-int4,Distil-Whisper,distil-whisper-large-v3,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,large-v3,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for throughput,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-large-v3-whisper-cpp-int8,Distil-Whisper,distil-whisper-large-v3,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,large-v3,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-large-v3-whisper-cpp-int4,Distil-Whisper,distil-whisper-large-v3,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,large-v3,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-medium-en-transformers-hf-fp16,Distil-Whisper,distil-whisper-medium.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,medium.en,transformers (HF),fp16,CPU or GPU; fp16 on GPU recommended,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-medium-en-transformers-hf-int8,Distil-Whisper,distil-whisper-medium.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,medium.en,transformers (HF),int8,CPU or GPU; fp16 on GPU recommended,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-medium-en-faster-whisper-ctranslate2-fp16,Distil-Whisper,distil-whisper-medium.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,medium.en,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for throughput,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-medium-en-faster-whisper-ctranslate2-int8,Distil-Whisper,distil-whisper-medium.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,medium.en,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for throughput,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-medium-en-faster-whisper-ctranslate2-int4,Distil-Whisper,distil-whisper-medium.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,medium.en,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for throughput,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-medium-en-whisper-cpp-int8,Distil-Whisper,distil-whisper-medium.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,medium.en,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-medium-en-whisper-cpp-int4,Distil-Whisper,distil-whisper-medium.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,medium.en,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-small-en-transformers-hf-fp16,Distil-Whisper,distil-whisper-small.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,small.en,transformers (HF),fp16,CPU or GPU; fp16 on GPU recommended,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-small-en-transformers-hf-int8,Distil-Whisper,distil-whisper-small.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,small.en,transformers (HF),int8,CPU or GPU; fp16 on GPU recommended,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-small-en-faster-whisper-ctranslate2-fp16,Distil-Whisper,distil-whisper-small.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,small.en,faster-whisper (CTranslate2),fp16,CPU ok; GPU recommended for throughput,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-small-en-faster-whisper-ctranslate2-int8,Distil-Whisper,distil-whisper-small.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,small.en,faster-whisper (CTranslate2),int8,CPU ok; GPU recommended for throughput,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-small-en-faster-whisper-ctranslate2-int4,Distil-Whisper,distil-whisper-small.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,small.en,faster-whisper (CTranslate2),int4,CPU ok; GPU recommended for throughput,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-small-en-whisper-cpp-int8,Distil-Whisper,distil-whisper-small.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,small.en,whisper.cpp,int8,CPU; Apple Silicon Metal acceleration available,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
distil-whisper-small-en-whisper-cpp-int4,Distil-Whisper,distil-whisper-small.en,Local,ASR,"ASR, transcription",English for *.en; otherwise depends on checkpoint,Chunked streaming possible,,small.en,whisper.cpp,int4,CPU; Apple Silicon Metal acceleration available,Chunked processing; 30s windows typical,See distil-whisper + runtime docs; designed for faster inference,See model card and repo,,https://huggingface.co/distil-whisper,high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Lower latency transcription on local hardware,Need maximum accuracy on hard audio,Distilled checkpoints; verify language coverage per model card.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
voxtral-realtime,Voxtral,realtime,Local and API,ASR,Streaming ASR,multilingual (see provider docs),Native streaming,sub-200 configurable (provider claim),Voxtral Realtime,Provider runtime / local weights (where available),,CPU or GPU; depends on deployment,Streaming,See provider docs,Apache 2.0 for open weights (verify per release),See provider pricing for API,https://mistral.ai/news/voxtral-transcribe-2,growing,medium,Streaming boundaries can hurt punctuation; diarization not guaranteed,Low latency mic transcription; live captions,Need fully offline with a mature ecosystem and tooling,Track both local and API surfaces if you plan hybrid deployments.,P0,production,realtime,payg,,"asr-api,tts-api",https://mistral.ai/news/voxtral-transcribe-2,2026-02-05T05:38:30Z,,medium
wav2vec2-family,wav2vec2,various,Local,ASR,ASR (varies by checkpoint),varies by checkpoint,varies; some support streaming decoding,,various checkpoints,PyTorch/ONNX/TensorRT/other depending on checkpoint,varies,CPU or GPU; varies,varies,See model cards or benchmarks per checkpoint,varies,,https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&search=wav2vec2,high,high,Checkpoint quality varies; often needs domain-specific fine-tuning,"Domain-tuned ASR, specialized languages, research baselines",Need a single turnkey general-purpose ASR with minimal tuning,Treat as families; expand into concrete checkpoints you actually plan to run.,,,,,,,,2026-02-05T05:38:30Z,,
hubert-family,HuBERT,various,Local,ASR,ASR (varies by checkpoint),varies by checkpoint,varies; some support streaming decoding,,various checkpoints,PyTorch/ONNX/TensorRT/other depending on checkpoint,varies,CPU or GPU; varies,varies,See model cards or benchmarks per checkpoint,varies,,https://huggingface.co/models?search=hubert,high,high,Checkpoint quality varies; often needs domain-specific fine-tuning,"Domain-tuned ASR, specialized languages, research baselines",Need a single turnkey general-purpose ASR with minimal tuning,Treat as families; expand into concrete checkpoints you actually plan to run.,,,,,,,,2026-02-05T05:38:30Z,,
conformer-various-family,Conformer (various),various,Local,ASR,ASR (varies by checkpoint),varies by checkpoint,varies; some support streaming decoding,,various checkpoints,PyTorch/ONNX/TensorRT/other depending on checkpoint,varies,CPU or GPU; varies,varies,See model cards or benchmarks per checkpoint,varies,,https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&search=conformer,medium,high,Checkpoint quality varies; often needs domain-specific fine-tuning,"Domain-tuned ASR, specialized languages, research baselines",Need a single turnkey general-purpose ASR with minimal tuning,Treat as families; expand into concrete checkpoints you actually plan to run.,,,,,,,,2026-02-05T05:38:30Z,,
nemo-asr-various-family,NeMo ASR (various),various,Local,ASR,ASR (varies by checkpoint),varies by checkpoint,varies; some support streaming decoding,,various checkpoints,PyTorch/ONNX/TensorRT/other depending on checkpoint,varies,CPU or GPU; varies,varies,See model cards or benchmarks per checkpoint,varies,,https://github.com/NVIDIA/NeMo,high,high,Checkpoint quality varies; often needs domain-specific fine-tuning,"Domain-tuned ASR, specialized languages, research baselines",Need a single turnkey general-purpose ASR with minimal tuning,Treat as families; expand into concrete checkpoints you actually plan to run.,,,,,,,,2026-02-05T05:38:30Z,,
espnet-recipes-models-family,ESPnet (recipes/models),various,Local,ASR,ASR (varies by checkpoint),varies by checkpoint,varies; some support streaming decoding,,various checkpoints,PyTorch/ONNX/TensorRT/other depending on checkpoint,varies,CPU or GPU; varies,varies,See model cards or benchmarks per checkpoint,varies,,https://github.com/espnet/espnet,high,high,Checkpoint quality varies; often needs domain-specific fine-tuning,"Domain-tuned ASR, specialized languages, research baselines",Need a single turnkey general-purpose ASR with minimal tuning,Treat as families; expand into concrete checkpoints you actually plan to run.,,,,,,,,2026-02-05T05:38:30Z,,
vosk-kaldi-various-family,Vosk/Kaldi (various),various,Local,ASR,ASR (varies by checkpoint),varies by checkpoint,varies; some support streaming decoding,,various checkpoints,PyTorch/ONNX/TensorRT/other depending on checkpoint,varies,CPU or GPU; varies,varies,See model cards or benchmarks per checkpoint,varies,,https://alphacephei.com/vosk/,medium,high,Checkpoint quality varies; often needs domain-specific fine-tuning,"Domain-tuned ASR, specialized languages, research baselines",Need a single turnkey general-purpose ASR with minimal tuning,Treat as families; expand into concrete checkpoints you actually plan to run.,,,,,,,,2026-02-05T05:38:30Z,,
deepspeech-legacy-family,DeepSpeech (legacy),various,Local,ASR,ASR (varies by checkpoint),varies by checkpoint,varies; some support streaming decoding,,various checkpoints,PyTorch/ONNX/TensorRT/other depending on checkpoint,varies,CPU or GPU; varies,varies,See model cards or benchmarks per checkpoint,varies,,https://github.com/mozilla/DeepSpeech,low,high,Checkpoint quality varies; often needs domain-specific fine-tuning,"Domain-tuned ASR, specialized languages, research baselines",Need a single turnkey general-purpose ASR with minimal tuning,Treat as families; expand into concrete checkpoints you actually plan to run.,,,,,,,,2026-02-05T05:38:30Z,,
openai-gpt-4o-mini-transcribe,OpenAI,gpt-4o-mini-transcribe,API,ASR,"Transcription (and sometimes diarization, language ID, keyword spotting)",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; benchmark yourself on your audio,commercial terms,see provider pricing,https://platform.openai.com/docs/models/gpt-4o-mini-transcribe,high,low,"Vendor limits, rate limits, privacy constraints, and domain drift","Fast time-to-ship, elastic scaling, multilingual coverage","Strict offline, privacy, or cost constraints",Treat as service entries; store pricing and limits separately because they change often.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
google-speech-to-text-v2,Google Cloud,Speech-to-Text v2,API,ASR,"Transcription (and sometimes diarization, language ID, keyword spotting)",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; benchmark yourself on your audio,commercial terms,see provider pricing,https://cloud.google.com/speech-to-text,high,low,"Vendor limits, rate limits, privacy constraints, and domain drift","Fast time-to-ship, elastic scaling, multilingual coverage","Strict offline, privacy, or cost constraints",Treat as service entries; store pricing and limits separately because they change often.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
aws-transcribe,AWS,Transcribe,API,ASR,"Transcription (and sometimes diarization, language ID, keyword spotting)",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; benchmark yourself on your audio,commercial terms,see provider pricing,https://aws.amazon.com/transcribe/,high,low,"Vendor limits, rate limits, privacy constraints, and domain drift","Fast time-to-ship, elastic scaling, multilingual coverage","Strict offline, privacy, or cost constraints",Treat as service entries; store pricing and limits separately because they change often.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
azure-speech-to-text,Microsoft Azure,Speech to Text,API,ASR,"Transcription (and sometimes diarization, language ID, keyword spotting)",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; benchmark yourself on your audio,commercial terms,see provider pricing,https://learn.microsoft.com/azure/ai-services/speech-service/speech-to-text,high,low,"Vendor limits, rate limits, privacy constraints, and domain drift","Fast time-to-ship, elastic scaling, multilingual coverage","Strict offline, privacy, or cost constraints",Treat as service entries; store pricing and limits separately because they change often.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
deepgram-asr,Deepgram,ASR,API,ASR,"Transcription (and sometimes diarization, language ID, keyword spotting)",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; benchmark yourself on your audio,commercial terms,see provider pricing,https://developers.deepgram.com/,high,low,"Vendor limits, rate limits, privacy constraints, and domain drift","Fast time-to-ship, elastic scaling, multilingual coverage","Strict offline, privacy, or cost constraints",Treat as service entries; store pricing and limits separately because they change often.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
piper,Piper,various voices,Local,TTS,Speech synthesis; some support voice cloning and multilingual outputs,varies by voice/model,possible with chunked generation; not always native,,various,PyTorch / ONNX / other,varies,CPU ok for small models; GPU recommended for high quality and low latency,unlimited (generation),benchmark on your target device; MOS is often subjective,see repo/model card,,https://github.com/rhasspy/piper,medium,medium,"Pronunciation errors, prosody issues, voice drift, artifacts on long text","Offline narration, UI voices, kids apps, accessibility",Need studio-quality voice with strict brand consistency and SLAs,Store voices as separate assets; treat each voice pack as an entry if needed.,,,,,,,,2026-02-05T05:38:30Z,,
coqui-xtts-v2,XTTS,v2,Local,TTS + Voice cloning,Speech synthesis; some support voice cloning and multilingual outputs,varies by voice/model,possible with chunked generation; not always native,,various,PyTorch / ONNX / other,varies,CPU ok for small models; GPU recommended for high quality and low latency,unlimited (generation),benchmark on your target device; MOS is often subjective,see repo/model card,,https://huggingface.co/coqui/XTTS-v2,high,medium,"Pronunciation errors, prosody issues, voice drift, artifacts on long text","Offline narration, UI voices, kids apps, accessibility",Need studio-quality voice with strict brand consistency and SLAs,Store voices as separate assets; treat each voice pack as an entry if needed.,P2,,,,,,,2026-02-05T05:38:30Z,high,high
bark,Bark,latest,Local,TTS / text-to-audio,Speech synthesis; some support voice cloning and multilingual outputs,varies by voice/model,possible with chunked generation; not always native,,various,PyTorch / ONNX / other,varies,CPU ok for small models; GPU recommended for high quality and low latency,unlimited (generation),benchmark on your target device; MOS is often subjective,see repo/model card,,https://github.com/suno-ai/bark,high,medium,"Pronunciation errors, prosody issues, voice drift, artifacts on long text","Offline narration, UI voices, kids apps, accessibility",Need studio-quality voice with strict brand consistency and SLAs,Store voices as separate assets; treat each voice pack as an entry if needed.,,,,,,,,2026-02-05T05:38:30Z,,
styletts2,StyleTTS2,latest,Local,TTS,Speech synthesis; some support voice cloning and multilingual outputs,varies by voice/model,possible with chunked generation; not always native,,various,PyTorch / ONNX / other,varies,CPU ok for small models; GPU recommended for high quality and low latency,unlimited (generation),benchmark on your target device; MOS is often subjective,see repo/model card,,https://github.com/yl4579/StyleTTS2,medium,medium,"Pronunciation errors, prosody issues, voice drift, artifacts on long text","Offline narration, UI voices, kids apps, accessibility",Need studio-quality voice with strict brand consistency and SLAs,Store voices as separate assets; treat each voice pack as an entry if needed.,,,,,,,,2026-02-05T05:38:30Z,,
vits-family,VITS,various,Local,TTS,Speech synthesis; some support voice cloning and multilingual outputs,varies by voice/model,possible with chunked generation; not always native,,various,PyTorch / ONNX / other,varies,CPU ok for small models; GPU recommended for high quality and low latency,unlimited (generation),benchmark on your target device; MOS is often subjective,see repo/model card,,https://github.com/jaywalnut310/vits,high,medium,"Pronunciation errors, prosody issues, voice drift, artifacts on long text","Offline narration, UI voices, kids apps, accessibility",Need studio-quality voice with strict brand consistency and SLAs,Store voices as separate assets; treat each voice pack as an entry if needed.,,,,,,,,2026-02-05T05:38:30Z,,
openai-tts,OpenAI,TTS API,API,TTS,"Speech synthesis; sometimes SSML, voices, and cloning features",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; measure MOS and latency yourself,commercial terms,see provider pricing,https://platform.openai.com/docs/guides/text-to-speech,high,low,Voice availability varies; cost spikes at scale; occasional glitches,Production voices with reliability and variety,Offline-only or highly cost-sensitive use cases,Track voice IDs as separate resources.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
elevenlabs-tts,ElevenLabs,TTS,API,TTS + Voice cloning,"Speech synthesis; sometimes SSML, voices, and cloning features",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; measure MOS and latency yourself,commercial terms,see provider pricing,https://elevenlabs.io/docs,high,low,Voice availability varies; cost spikes at scale; occasional glitches,Production voices with reliability and variety,Offline-only or highly cost-sensitive use cases,Track voice IDs as separate resources.,P2,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,high,medium
azure-tts,Microsoft Azure,Neural TTS,API,TTS,"Speech synthesis; sometimes SSML, voices, and cloning features",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; measure MOS and latency yourself,commercial terms,see provider pricing,https://learn.microsoft.com/azure/ai-services/speech-service/text-to-speech,high,low,Voice availability varies; cost spikes at scale; occasional glitches,Production voices with reliability and variety,Offline-only or highly cost-sensitive use cases,Track voice IDs as separate resources.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
google-tts,Google Cloud,Text-to-Speech,API,TTS,"Speech synthesis; sometimes SSML, voices, and cloning features",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; measure MOS and latency yourself,commercial terms,see provider pricing,https://cloud.google.com/text-to-speech,high,low,Voice availability varies; cost spikes at scale; occasional glitches,Production voices with reliability and variety,Offline-only or highly cost-sensitive use cases,Track voice IDs as separate resources.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
aws-polly,AWS,Polly,API,TTS,"Speech synthesis; sometimes SSML, voices, and cloning features",varies,yes (provider dependent),,,Hosted API,,,provider limits,provider claims; measure MOS and latency yourself,commercial terms,see provider pricing,https://aws.amazon.com/polly/,high,low,Voice availability varies; cost spikes at scale; occasional glitches,Production voices with reliability and variety,Offline-only or highly cost-sensitive use cases,Track voice IDs as separate resources.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
rvc-webui,RVC,latest,Local,Voice Conversion,Voice conversion / cloning (varies),varies,usually batch; real-time possible with heavy optimization,,varies,PyTorch / ONNX / custom,varies,GPU recommended; CPU often slow,short clips recommended for best quality,community benchmarks vary; test with your voices,see repo/model cards,,https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI,high,high,"Artifacts, robotic tone, identity leakage, poor generalization across accents","Character voices, dubbing experiments, voice style transfer",High-stakes identity use cases; legal and consent constraints,Strong policy and consent requirements in real products.,P2,,,,,,,2026-02-05T05:38:30Z,high,high
so-vits-svc,so-vits-svc,latest,Local,Voice Conversion / Singing VC,Voice conversion / cloning (varies),varies,usually batch; real-time possible with heavy optimization,,varies,PyTorch / ONNX / custom,varies,GPU recommended; CPU often slow,short clips recommended for best quality,community benchmarks vary; test with your voices,see repo/model cards,,https://github.com/svc-develop-team/so-vits-svc,high,high,"Artifacts, robotic tone, identity leakage, poor generalization across accents","Character voices, dubbing experiments, voice style transfer",High-stakes identity use cases; legal and consent constraints,Strong policy and consent requirements in real products.,P2,,,,,,,2026-02-05T05:38:30Z,high,high
openvoice,OpenVoice,latest,Local,Voice cloning,Voice conversion / cloning (varies),varies,usually batch; real-time possible with heavy optimization,,varies,PyTorch / ONNX / custom,varies,GPU recommended; CPU often slow,short clips recommended for best quality,community benchmarks vary; test with your voices,see repo/model cards,,https://github.com/myshell-ai/OpenVoice,high,high,"Artifacts, robotic tone, identity leakage, poor generalization across accents","Character voices, dubbing experiments, voice style transfer",High-stakes identity use cases; legal and consent constraints,Strong policy and consent requirements in real products.,P2,,,,,,,2026-02-05T05:38:30Z,high,high
deepfilternet,DeepFilterNet,latest,Local,Speech enhancement / noise suppression,"Noise suppression, dereverb, source separation (varies)",language-agnostic,some support streaming; often chunked,,various,C/C++ or PyTorch,varies,CPU ok for RNNoise; GPU helps for deep models,chunked,see repo; measure on your audio,see repo/model card,,https://github.com/Rikorose/DeepFilterNet,high,medium,"Musical noise, speech distortion, separation bleed","Improve ASR, clean mic input, isolate vocals/instruments",If artifacts are unacceptable or audio is already clean,Put before ASR; keep original audio for auditability.,P1,production,near-realtime,free/open,"mac,linux,windows","enhancement-speech,sep-music",,2026-02-05T05:38:30Z,,medium
rnnoise,RNNoise,latest,Local,Noise suppression,"Noise suppression, dereverb, source separation (varies)",language-agnostic,some support streaming; often chunked,,various,C/C++ or PyTorch,varies,CPU ok for RNNoise; GPU helps for deep models,chunked,see repo; measure on your audio,see repo/model card,,https://github.com/xiph/rnnoise,high,medium,"Musical noise, speech distortion, separation bleed","Improve ASR, clean mic input, isolate vocals/instruments",If artifacts are unacceptable or audio is already clean,Put before ASR; keep original audio for auditability.,P1,production,near-realtime,free/open,"mac,linux,windows","enhancement-speech,sep-music",,2026-02-05T05:38:30Z,,medium
demucs,Demucs,latest,Local,Source separation,"Noise suppression, dereverb, source separation (varies)",language-agnostic,some support streaming; often chunked,,various,C/C++ or PyTorch,varies,CPU ok for RNNoise; GPU helps for deep models,chunked,see repo; measure on your audio,see repo/model card,,https://github.com/facebookresearch/demucs,high,medium,"Musical noise, speech distortion, separation bleed","Improve ASR, clean mic input, isolate vocals/instruments",If artifacts are unacceptable or audio is already clean,Put before ASR; keep original audio for auditability.,P1,production,near-realtime,free/open,"mac,linux,windows","enhancement-speech,sep-music",,2026-02-05T05:38:30Z,,medium
speechbrain-sepformer,SepFormer (SpeechBrain),various,Local,Enhancement / separation,"Noise suppression, dereverb, source separation (varies)",language-agnostic,some support streaming; often chunked,,various,C/C++ or PyTorch,varies,CPU ok for RNNoise; GPU helps for deep models,chunked,see repo; measure on your audio,see repo/model card,,https://huggingface.co/speechbrain/sepformer-whamr-enhancement,high,medium,"Musical noise, speech distortion, separation bleed","Improve ASR, clean mic input, isolate vocals/instruments",If artifacts are unacceptable or audio is already clean,Put before ASR; keep original audio for auditability.,P1,production,near-realtime,free/open,"mac,linux,windows","enhancement-speech,sep-music",,2026-02-05T05:38:30Z,,medium
yamnet,YAMNet,1,Local,Audio event classification,"Embedding extraction, tagging, retrieval, zero-shot matching (varies)",language-agnostic for non-speech tasks; CLAP supports text prompts,possible with sliding windows,,various,TF/PyTorch,varies,CPU ok; GPU helps for throughput,"windowed (e.g., 0.96s to a few seconds)",AudioSet and task-specific evaluations; see model docs,see repo/model cards,,https://www.tensorflow.org/hub/tutorials/yamnet,high,low,Domain mismatch; false positives on similar sounds,"Audio search, tagging, anomaly detection, dataset triage",Need precise event boundaries without a dedicated detection model,Store embeddings with versioning for reproducibility.,P1,production,batch,free/open,"mac,linux,windows","tagging-audioset,retrieval-clap",,2026-02-05T05:38:30Z,,medium
panns-cnn14,PANNs CNN14,various,Local,Audio tagging / embeddings,"Embedding extraction, tagging, retrieval, zero-shot matching (varies)",language-agnostic for non-speech tasks; CLAP supports text prompts,possible with sliding windows,,various,TF/PyTorch,varies,CPU ok; GPU helps for throughput,"windowed (e.g., 0.96s to a few seconds)",AudioSet and task-specific evaluations; see model docs,see repo/model cards,,https://github.com/qiuqiangkong/audioset_tagging_cnn,high,low,Domain mismatch; false positives on similar sounds,"Audio search, tagging, anomaly detection, dataset triage",Need precise event boundaries without a dedicated detection model,Store embeddings with versioning for reproducibility.,P1,production,batch,free/open,"mac,linux,windows","tagging-audioset,retrieval-clap",,2026-02-05T05:38:30Z,,medium
beats,BEATs,various,Local,Audio representation / embeddings,"Embedding extraction, tagging, retrieval, zero-shot matching (varies)",language-agnostic for non-speech tasks; CLAP supports text prompts,possible with sliding windows,,various,TF/PyTorch,varies,CPU ok; GPU helps for throughput,"windowed (e.g., 0.96s to a few seconds)",AudioSet and task-specific evaluations; see model docs,see repo/model cards,,https://github.com/microsoft/unilm/blob/master/beats/README.md,high,low,Domain mismatch; false positives on similar sounds,"Audio search, tagging, anomaly detection, dataset triage",Need precise event boundaries without a dedicated detection model,Store embeddings with versioning for reproducibility.,P1,production,batch,free/open,"mac,linux,windows","tagging-audioset,retrieval-clap",,2026-02-05T05:38:30Z,,medium
clap,CLAP,various,Local,Text-audio retrieval embeddings,"Embedding extraction, tagging, retrieval, zero-shot matching (varies)",language-agnostic for non-speech tasks; CLAP supports text prompts,possible with sliding windows,,various,TF/PyTorch,varies,CPU ok; GPU helps for throughput,"windowed (e.g., 0.96s to a few seconds)",AudioSet and task-specific evaluations; see model docs,see repo/model cards,,https://github.com/LAION-AI/CLAP,high,low,Domain mismatch; false positives on similar sounds,"Audio search, tagging, anomaly detection, dataset triage",Need precise event boundaries without a dedicated detection model,Store embeddings with versioning for reproducibility.,P1,production,batch,free/open,"mac,linux,windows","tagging-audioset,retrieval-clap",,2026-02-05T05:38:30Z,,medium
vggish,VGGish,various,Local,Audio embeddings,"Embedding extraction, tagging, retrieval, zero-shot matching (varies)",language-agnostic for non-speech tasks; CLAP supports text prompts,possible with sliding windows,,various,TF/PyTorch,varies,CPU ok; GPU helps for throughput,"windowed (e.g., 0.96s to a few seconds)",AudioSet and task-specific evaluations; see model docs,see repo/model cards,,https://github.com/tensorflow/models/tree/master/research/audioset,high,low,Domain mismatch; false positives on similar sounds,"Audio search, tagging, anomaly detection, dataset triage",Need precise event boundaries without a dedicated detection model,Store embeddings with versioning for reproducibility.,P1,production,batch,free/open,"mac,linux,windows","tagging-audioset,retrieval-clap",,2026-02-05T05:38:30Z,,medium
openl3,OpenL3,various,Local,Audio embeddings,"Embedding extraction, tagging, retrieval, zero-shot matching (varies)",language-agnostic for non-speech tasks; CLAP supports text prompts,possible with sliding windows,,various,TF/PyTorch,varies,CPU ok; GPU helps for throughput,"windowed (e.g., 0.96s to a few seconds)",AudioSet and task-specific evaluations; see model docs,see repo/model cards,,https://github.com/marl/openl3,high,low,Domain mismatch; false positives on similar sounds,"Audio search, tagging, anomaly detection, dataset triage",Need precise event boundaries without a dedicated detection model,Store embeddings with versioning for reproducibility.,P1,production,batch,free/open,"mac,linux,windows","tagging-audioset,retrieval-clap",,2026-02-05T05:38:30Z,,medium
audiocraft-musicgen,MusicGen (AudioCraft),various,Local,Music generation,Text-to-music or text-to-audio generation (varies),prompts in many languages; training bias varies,no (typically),,various,PyTorch,varies,GPU strongly recommended; memory depends on model size and duration,generation length limits depend on model and compute,subjective; rely on eval suites where available,see repo/model cards,,https://github.com/facebookresearch/audiocraft,high,high,"Artifacts, repetition, poor structure, prompt sensitivity","Game music, sound effects, prototyping",Need predictable commercial-grade compositions without artifacts,Include content and copyright policy gates in products.,P3,research,batch,free/open,"linux,windows,mac",gen-audio-subjective,,2026-02-05T05:38:30Z,,high
audiocraft-audiogen,AudioGen (AudioCraft),various,Local,Audio generation,Text-to-music or text-to-audio generation (varies),prompts in many languages; training bias varies,no (typically),,various,PyTorch,varies,GPU strongly recommended; memory depends on model size and duration,generation length limits depend on model and compute,subjective; rely on eval suites where available,see repo/model cards,,https://github.com/facebookresearch/audiocraft,high,high,"Artifacts, repetition, poor structure, prompt sensitivity","Game music, sound effects, prototyping",Need predictable commercial-grade compositions without artifacts,Include content and copyright policy gates in products.,P3,research,batch,free/open,"linux,windows,mac",gen-audio-subjective,,2026-02-05T05:38:30Z,,high
audiocraft-magnet,MAGNeT (AudioCraft),various,Local,Music generation,Text-to-music or text-to-audio generation (varies),prompts in many languages; training bias varies,no (typically),,various,PyTorch,varies,GPU strongly recommended; memory depends on model size and duration,generation length limits depend on model and compute,subjective; rely on eval suites where available,see repo/model cards,,https://github.com/facebookresearch/audiocraft,high,high,"Artifacts, repetition, poor structure, prompt sensitivity","Game music, sound effects, prototyping",Need predictable commercial-grade compositions without artifacts,Include content and copyright policy gates in products.,P3,research,batch,free/open,"linux,windows,mac",gen-audio-subjective,,2026-02-05T05:38:30Z,,high
stable-audio-open,Stable Audio Open,v1,Local,Audio generation,Text-to-music or text-to-audio generation (varies),prompts in many languages; training bias varies,no (typically),,various,PyTorch,varies,GPU strongly recommended; memory depends on model size and duration,generation length limits depend on model and compute,subjective; rely on eval suites where available,see repo/model cards,,https://github.com/Stability-AI/stable-audio-tools,high,high,"Artifacts, repetition, poor structure, prompt sensitivity","Game music, sound effects, prototyping",Need predictable commercial-grade compositions without artifacts,Include content and copyright policy gates in products.,P3,research,batch,free/open,"linux,windows,mac",gen-audio-subjective,,2026-02-05T05:38:30Z,,high
audioldm,AudioLDM,various,Local,Audio generation,Text-to-music or text-to-audio generation (varies),prompts in many languages; training bias varies,no (typically),,various,PyTorch,varies,GPU strongly recommended; memory depends on model size and duration,generation length limits depend on model and compute,subjective; rely on eval suites where available,see repo/model cards,,https://github.com/haoheliu/AudioLDM,high,high,"Artifacts, repetition, poor structure, prompt sensitivity","Game music, sound effects, prototyping",Need predictable commercial-grade compositions without artifacts,Include content and copyright policy gates in products.,P3,research,batch,free/open,"linux,windows,mac",gen-audio-subjective,,2026-02-05T05:38:30Z,,high
jukebox,Jukebox,1,Local,Music generation,Text-to-music or text-to-audio generation (varies),prompts in many languages; training bias varies,no (typically),,various,PyTorch,varies,GPU strongly recommended; memory depends on model size and duration,generation length limits depend on model and compute,subjective; rely on eval suites where available,see repo/model cards,,https://github.com/openai/jukebox,high,high,"Artifacts, repetition, poor structure, prompt sensitivity","Game music, sound effects, prototyping",Need predictable commercial-grade compositions without artifacts,Include content and copyright policy gates in products.,P3,research,batch,free/open,"linux,windows,mac",gen-audio-subjective,,2026-02-05T05:38:30Z,,high
riffusion,Riffusion,various,Local,Audio generation,Text-to-music or text-to-audio generation (varies),prompts in many languages; training bias varies,no (typically),,various,PyTorch,varies,GPU strongly recommended; memory depends on model size and duration,generation length limits depend on model and compute,subjective; rely on eval suites where available,see repo/model cards,,https://github.com/riffusion/riffusion,high,high,"Artifacts, repetition, poor structure, prompt sensitivity","Game music, sound effects, prototyping",Need predictable commercial-grade compositions without artifacts,Include content and copyright policy gates in products.,P3,research,batch,free/open,"linux,windows,mac",gen-audio-subjective,,2026-02-05T05:38:30Z,,high
basicpitch,BasicPitch,latest,Local,Music transcription (MIDI),"Transcription to MIDI/notes, pitch tracking",,chunked possible; often batch,,various,TF/PyTorch,varies,CPU ok for smaller; GPU helps,windowed/chunked,task-specific; see papers and repos,see repo/model cards,,https://github.com/spotify/basic-pitch,medium,medium,"Polyphony errors, timing jitter, instrument confusion","MIDI extraction, practice tools, music analysis",Need perfectly accurate scores on complex recordings,Store confidence scores and post-process with heuristics.,P3,production,batch,free/open,"mac,linux,windows",music-transcription,,2026-02-05T05:38:30Z,,medium
mt3,MT3,latest,Local,Multi-instrument transcription,"Transcription to MIDI/notes, pitch tracking",,chunked possible; often batch,,various,TF/PyTorch,varies,CPU ok for smaller; GPU helps,windowed/chunked,task-specific; see papers and repos,see repo/model cards,,https://github.com/magenta/magenta,medium,medium,"Polyphony errors, timing jitter, instrument confusion","MIDI extraction, practice tools, music analysis",Need perfectly accurate scores on complex recordings,Store confidence scores and post-process with heuristics.,,,,,,,,2026-02-05T05:38:30Z,,
onsets-and-frames,Onsets and Frames,various,Local,Piano transcription,"Transcription to MIDI/notes, pitch tracking",,chunked possible; often batch,,various,TF/PyTorch,varies,CPU ok for smaller; GPU helps,windowed/chunked,task-specific; see papers and repos,see repo/model cards,,https://github.com/magenta/magenta,medium,medium,"Polyphony errors, timing jitter, instrument confusion","MIDI extraction, practice tools, music analysis",Need perfectly accurate scores on complex recordings,Store confidence scores and post-process with heuristics.,,,,,,,,2026-02-05T05:38:30Z,,
crepe,CREPE,latest,Local,Pitch tracking,"Transcription to MIDI/notes, pitch tracking",,chunked possible; often batch,,various,TF/PyTorch,varies,CPU ok for smaller; GPU helps,windowed/chunked,task-specific; see papers and repos,see repo/model cards,,https://github.com/marl/crepe,medium,medium,"Polyphony errors, timing jitter, instrument confusion","MIDI extraction, practice tools, music analysis",Need perfectly accurate scores on complex recordings,Store confidence scores and post-process with heuristics.,P3,production,batch,free/open,"mac,linux,windows",music-transcription,,2026-02-05T05:38:30Z,,medium
encodec,EnCodec,various,Local,Neural audio codec,Encode and decode audio to learned tokens or compressed streams,,some support streaming framing,,various,PyTorch,varies,CPU or GPU; depends on bitrate and throughput needs,streaming frames,bitrate vs quality trade-offs; see papers,see repo/paper,,https://github.com/facebookresearch/audiocraft,medium,high,Codec artifacts at low bitrates; out-of-domain audio issues,"Generative audio tokenization, compression experiments",Simple compression where Opus/AAC is enough,Useful for building audio token pipelines.,P2,research,near-realtime,free/open,"mac,linux,windows",codec-quality,,2026-02-05T05:38:30Z,,high
descript-dac,Descript Audio Codec (DAC),latest,Local,Neural audio codec,Encode and decode audio to learned tokens or compressed streams,,some support streaming framing,,various,PyTorch,varies,CPU or GPU; depends on bitrate and throughput needs,streaming frames,bitrate vs quality trade-offs; see papers,see repo/paper,,https://github.com/descriptinc/descript-audio-codec,medium,high,Codec artifacts at low bitrates; out-of-domain audio issues,"Generative audio tokenization, compression experiments",Simple compression where Opus/AAC is enough,Useful for building audio token pipelines.,P2,research,near-realtime,free/open,"mac,linux,windows",codec-quality,,2026-02-05T05:38:30Z,,high
soundstream,SoundStream,various,Local,Neural audio codec,Encode and decode audio to learned tokens or compressed streams,,some support streaming framing,,various,PyTorch,varies,CPU or GPU; depends on bitrate and throughput needs,streaming frames,bitrate vs quality trade-offs; see papers,see repo/paper,,https://arxiv.org/abs/2107.03312,medium,high,Codec artifacts at low bitrates; out-of-domain audio issues,"Generative audio tokenization, compression experiments",Simple compression where Opus/AAC is enough,Useful for building audio token pipelines.,P2,research,near-realtime,free/open,"mac,linux,windows",codec-quality,,2026-02-05T05:38:30Z,,high
aasist-family,AASIST,various,Local,Deepfake / spoof detection,"Deepfake detection, spoof detection, watermark detection",,windowed,,various,PyTorch,varies,CPU ok; GPU for throughput,windowed,ASVspoof leaderboards and task metrics; see datasets,see repo/site,,https://github.com/clovaai/aasist,medium,high,Adversarial adaptation; dataset bias; false positives on noisy audio,Risk scoring for synthetic audio; forensic triage,Automated final decisions without human review,"Use as signals, not as sole gatekeepers.",P2,research,batch,free/open,"mac,linux,windows",forensics-asvspoof,,2026-02-05T05:38:30Z,,high
asvspoof-baselines,ASVspoof baselines,various,Local,Deepfake / spoof detection,"Deepfake detection, spoof detection, watermark detection",,windowed,,various,PyTorch,varies,CPU ok; GPU for throughput,windowed,ASVspoof leaderboards and task metrics; see datasets,see repo/site,,https://www.asvspoof.org/,medium,high,Adversarial adaptation; dataset bias; false positives on noisy audio,Risk scoring for synthetic audio; forensic triage,Automated final decisions without human review,"Use as signals, not as sole gatekeepers.",P2,research,batch,free/open,"mac,linux,windows",forensics-asvspoof,,2026-02-05T05:38:30Z,,high
audioseal,AudioSeal,latest,Local,Audio watermarking / detection,"Deepfake detection, spoof detection, watermark detection",,windowed,,various,PyTorch,varies,CPU ok; GPU for throughput,windowed,ASVspoof leaderboards and task metrics; see datasets,see repo/site,,https://github.com/facebookresearch/audiocraft,medium,high,Adversarial adaptation; dataset bias; false positives on noisy audio,Risk scoring for synthetic audio; forensic triage,Automated final decisions without human review,"Use as signals, not as sole gatekeepers.",P2,research,batch,free/open,"mac,linux,windows",forensics-asvspoof,,2026-02-05T05:38:30Z,,high
gemini-audio-understanding,Gemini,API audio understanding,API,Audio-language model,"Audio QA, summarization, transcription, classification (varies)",varies,no for some; Gemini audio is not real-time per docs,,various,Hosted API or PyTorch,varies,GPU recommended for local,provider limits or tokenizer limits,research benchmarks vary; test on your tasks,see docs/model cards,see provider pricing,https://ai.google.dev/gemini-api/docs/audio,growing,high,Hallucinated interpretations; sensitivity to prompt and audio quality,Higher-level audio understanding beyond plain ASR,Need deterministic transcription with low error tolerance,Separate these from pure ASR; treat outputs as probabilistic.,,production,near-realtime,payg,,"asr-api,tts-api",,2026-02-05T05:38:30Z,,medium
gemma-3n-audio,Gemma 3n,audio,Local,Audio-language model / ASR / AST,"Audio QA, summarization, transcription, classification (varies)",varies,no for some; Gemini audio is not real-time per docs,,various,Hosted API or PyTorch,varies,GPU recommended for local,provider limits or tokenizer limits,research benchmarks vary; test on your tasks,see docs/model cards,,https://ai.google.dev/gemma/docs/capabilities/audio,growing,high,Hallucinated interpretations; sensitivity to prompt and audio quality,Higher-level audio understanding beyond plain ASR,Need deterministic transcription with low error tolerance,Separate these from pure ASR; treat outputs as probabilistic.,,,,,,,,2026-02-05T05:38:30Z,,
qwen2-audio,Qwen2-Audio,various,Local,Audio-language model,"Audio QA, summarization, transcription, classification (varies)",varies,no for some; Gemini audio is not real-time per docs,,various,Hosted API or PyTorch,varies,GPU recommended for local,provider limits or tokenizer limits,research benchmarks vary; test on your tasks,see docs/model cards,,https://huggingface.co/models?search=qwen%20audio,growing,high,Hallucinated interpretations; sensitivity to prompt and audio quality,Higher-level audio understanding beyond plain ASR,Need deterministic transcription with low error tolerance,Separate these from pure ASR; treat outputs as probabilistic.,,,,,,,,2026-02-05T05:38:30Z,,
salmomn,SALMONN,various,Local,Audio-language model,"Audio QA, summarization, transcription, classification (varies)",varies,no for some; Gemini audio is not real-time per docs,,various,Hosted API or PyTorch,varies,GPU recommended for local,provider limits or tokenizer limits,research benchmarks vary; test on your tasks,see docs/model cards,,https://github.com/bytedance/SALMONN,growing,high,Hallucinated interpretations; sensitivity to prompt and audio quality,Higher-level audio understanding beyond plain ASR,Need deterministic transcription with low error tolerance,Separate these from pure ASR; treat outputs as probabilistic.,,,,,,,,2026-02-05T05:38:30Z,,
birdnet,BirdNET,various,Local,Bioacoustics,Species identification from environmental audio,,windowed,,BirdNET checkpoints,TF/PyTorch depending on version,varies,CPU ok,windowed,eco/bioacoustics evaluations; see project docs,see repo,,https://github.com/birdnet-team/BirdNET-Analyzer,high,medium,Regional species mismatch; background noise confounds,"Wildlife monitoring, biodiversity projects",Need general environmental tagging across all sound types,Add regional calibration and location priors if used in products.,,,,,,,,2026-02-05T05:38:30Z,,
whisper-tiny-whisper-cpp-gguf-q8-0,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-whisper-cpp-gguf-q6-k,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-whisper-cpp-gguf-q5-0,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-whisper-cpp-gguf-q4-0,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-whisper-cpp-gguf-q4-k-m,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-whisper-cpp-gguf-q3-k-l,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-whisper-cpp-gguf-q2-k,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-gguf-q8-0,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-gguf-q6-k,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-gguf-q5-0,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-gguf-q4-0,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-gguf-q4-k-m,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-gguf-q3-k-l,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-whisper-cpp-gguf-q2-k,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-gguf-q8-0,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-gguf-q6-k,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-gguf-q5-0,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-gguf-q4-0,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-gguf-q4-k-m,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-gguf-q3-k-l,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-whisper-cpp-gguf-q2-k,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-gguf-q8-0,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-gguf-q6-k,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-gguf-q5-0,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-gguf-q4-0,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-gguf-q4-k-m,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-gguf-q3-k-l,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-whisper-cpp-gguf-q2-k,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-gguf-q8-0,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v2,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-gguf-q6-k,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v2,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-gguf-q5-0,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v2,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-gguf-q4-0,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v2,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-gguf-q4-k-m,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v2,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-gguf-q3-k-l,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v2,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-whisper-cpp-gguf-q2-k,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v2,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-gguf-q8-0,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-gguf-q6-k,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-gguf-q5-0,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-gguf-q4-0,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-gguf-q4-k-m,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-gguf-q3-k-l,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-whisper-cpp-gguf-q2-k,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-gguf-q8-0,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3-turbo,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-gguf-q6-k,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3-turbo,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-gguf-q5-0,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3-turbo,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-gguf-q4-0,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3-turbo,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-gguf-q4-k-m,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3-turbo,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-gguf-q3-k-l,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3-turbo,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-whisper-cpp-gguf-q2-k,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-large-v3-turbo,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-gguf-q8-0,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny.en,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-gguf-q6-k,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny.en,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-gguf-q5-0,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny.en,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-gguf-q4-0,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny.en,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-gguf-q4-k-m,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny.en,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-gguf-q3-k-l,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny.en,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-whisper-cpp-gguf-q2-k,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-tiny.en,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-gguf-q8-0,Whisper,base.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base.en,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-gguf-q6-k,Whisper,base.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base.en,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-gguf-q5-0,Whisper,base.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base.en,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-gguf-q4-0,Whisper,base.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base.en,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-gguf-q4-k-m,Whisper,base.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base.en,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-gguf-q3-k-l,Whisper,base.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base.en,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-whisper-cpp-gguf-q2-k,Whisper,base.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-base.en,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-gguf-q8-0,Whisper,small.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small.en,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-gguf-q6-k,Whisper,small.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small.en,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-gguf-q5-0,Whisper,small.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small.en,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-gguf-q4-0,Whisper,small.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small.en,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-gguf-q4-k-m,Whisper,small.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small.en,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-gguf-q3-k-l,Whisper,small.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small.en,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-whisper-cpp-gguf-q2-k,Whisper,small.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-small.en,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-gguf-q8-0,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium.en,whisper.cpp,gguf:q8_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-gguf-q6-k,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium.en,whisper.cpp,gguf:q6_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-gguf-q5-0,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium.en,whisper.cpp,gguf:q5_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-gguf-q4-0,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium.en,whisper.cpp,gguf:q4_0,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-gguf-q4-k-m,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium.en,whisper.cpp,gguf:q4_k_m,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-gguf-q3-k-l,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium.en,whisper.cpp,gguf:q3_k_l,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-whisper-cpp-gguf-q2-k,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Streaming via chunking; whisper.cpp supports realtime examples,,whisper-medium.en,whisper.cpp,gguf:q2_k,CPU; Apple Silicon Metal acceleration available; GGUF/GGML quant improves speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,"See whisper.cpp docs; quant affects speed, memory, and accuracy",See repos; quant files have their own distribution terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",Offline transcription on Apple Silicon; low-memory devices with quantized weights,If you need maximum accuracy and have ample GPU compute,More granular quant variants for packaging and performance tuning.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-faster-whisper-ctranslate2-float32,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-tiny,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-faster-whisper-ctranslate2-float16,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-tiny,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-faster-whisper-ctranslate2-int8-float16,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-tiny,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-faster-whisper-ctranslate2-float32,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-base,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-faster-whisper-ctranslate2-float16,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-base,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-faster-whisper-ctranslate2-int8-float16,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-base,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-faster-whisper-ctranslate2-float32,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-small,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-faster-whisper-ctranslate2-float16,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-small,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-faster-whisper-ctranslate2-int8-float16,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-small,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-faster-whisper-ctranslate2-float32,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-medium,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-faster-whisper-ctranslate2-float16,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-medium,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-faster-whisper-ctranslate2-int8-float16,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-medium,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-faster-whisper-ctranslate2-float32,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-large-v2,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-faster-whisper-ctranslate2-float16,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-large-v2,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-faster-whisper-ctranslate2-int8-float16,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-large-v2,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-faster-whisper-ctranslate2-float32,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-large-v3,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-faster-whisper-ctranslate2-float16,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-large-v3,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-faster-whisper-ctranslate2-int8-float16,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-large-v3,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-faster-whisper-ctranslate2-float32,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-large-v3-turbo,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-faster-whisper-ctranslate2-float16,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-large-v3-turbo,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-faster-whisper-ctranslate2-int8-float16,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible; best for batch throughput,,whisper-large-v3-turbo,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-faster-whisper-ctranslate2-float32,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-tiny.en,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-faster-whisper-ctranslate2-float16,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-tiny.en,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-faster-whisper-ctranslate2-int8-float16,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-tiny.en,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-faster-whisper-ctranslate2-float32,Whisper,base.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-base.en,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-faster-whisper-ctranslate2-float16,Whisper,base.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-base.en,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-faster-whisper-ctranslate2-int8-float16,Whisper,base.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-base.en,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-faster-whisper-ctranslate2-float32,Whisper,small.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-small.en,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-faster-whisper-ctranslate2-float16,Whisper,small.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-small.en,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-faster-whisper-ctranslate2-int8-float16,Whisper,small.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-small.en,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-faster-whisper-ctranslate2-float32,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-medium.en,faster-whisper (CTranslate2),float32,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-faster-whisper-ctranslate2-float16,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-medium.en,faster-whisper (CTranslate2),float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-faster-whisper-ctranslate2-int8-float16,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible; best for batch throughput,,whisper-medium.en,faster-whisper (CTranslate2),int8_float16,CPU ok; GPU recommended for throughput; compute_type controls speed and memory,Chunked processing; 30s windows typical; can run longer with segmentation.,See faster-whisper docs; compute_type affects latency and memory,See repos and model weights terms,,https://github.com/openai/whisper,very high,medium,"Overlapping speakers, heavy noise, strong accents, or domain jargon can reduce accuracy; timestamps drift without alignment.",High throughput batch transcription; server-side pipelines,If you need tiny binary size or ultra-simple deployment,Explicit CTranslate2 compute_type variants.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-onnxruntime-fp16,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-tiny,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-onnxruntime-int8,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-tiny,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-coreml-fp16,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-tiny,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny-coreml-int8,Whisper,tiny,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-tiny,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-onnxruntime-fp16,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-base,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-onnxruntime-int8,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-base,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-coreml-fp16,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-base,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base-coreml-int8,Whisper,base,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-base,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-onnxruntime-fp16,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-small,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-onnxruntime-int8,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-small,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-coreml-fp16,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-small,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small-coreml-int8,Whisper,small,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-small,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-onnxruntime-fp16,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-medium,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-onnxruntime-int8,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-medium,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-coreml-fp16,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-medium,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium-coreml-int8,Whisper,medium,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-medium,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-onnxruntime-fp16,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v2,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-onnxruntime-int8,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v2,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-coreml-fp16,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v2,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v2-coreml-int8,Whisper,large-v2,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v2,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-onnxruntime-fp16,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v3,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-onnxruntime-int8,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v3,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-coreml-fp16,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v3,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-coreml-int8,Whisper,large-v3,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v3,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-onnxruntime-fp16,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v3-turbo,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-onnxruntime-int8,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v3-turbo,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-coreml-fp16,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v3-turbo,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-large-v3-turbo-coreml-int8,Whisper,large-v3-turbo,Local,ASR,"ASR, transcription",multilingual,Chunked streaming possible,,whisper-large-v3-turbo,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-onnxruntime-fp16,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-tiny.en,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-onnxruntime-int8,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-tiny.en,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-coreml-fp16,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-tiny.en,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-tiny.en-coreml-int8,Whisper,tiny.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-tiny.en,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-onnxruntime-fp16,Whisper,base.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-base.en,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-onnxruntime-int8,Whisper,base.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-base.en,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-coreml-fp16,Whisper,base.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-base.en,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-base.en-coreml-int8,Whisper,base.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-base.en,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-onnxruntime-fp16,Whisper,small.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-small.en,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-onnxruntime-int8,Whisper,small.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-small.en,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-coreml-fp16,Whisper,small.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-small.en,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-small.en-coreml-int8,Whisper,small.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-small.en,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-onnxruntime-fp16,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-medium.en,onnxruntime,fp16,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-onnxruntime-int8,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-medium.en,onnxruntime,int8,CPU or GPU; best for cross-platform deployment; requires exported ONNX graph,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-coreml-fp16,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-medium.en,coreml,fp16,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
whisper-medium.en-coreml-int8,Whisper,medium.en,Local,ASR,"ASR, transcription",English,Chunked streaming possible,,whisper-medium.en,coreml,int8,Apple devices; best for on-device iOS/macOS deployment; requires CoreML conversion,Chunked processing; 30s windows typical; can run longer with segmentation.,Export and runtime dependent; benchmark on target device,See base model and export tooling licenses,,https://github.com/openai/whisper,high,high,"Conversion mismatches, operator support gaps, accuracy drift after quantization",Mobile and edge deployment; standardized runtime packaging,If you do not want to maintain conversion pipelines,Treat exports as separate artifacts with reproducible build steps.,P0,production,near-realtime,free/open,"mac,linux,windows","asr-general,asr-multispeaker,asr-noisy",,2026-02-05T05:38:30Z,,low
silero-vad,Silero VAD,latest,Local,VAD,Voice activity detection; segmentation; stream chunking,language-agnostic,yes,,silero-vad,PyTorch / ONNX,varies,CPU,windowed,Measure false-positive/false-negative rates on your data,see repo,,https://github.com/snakers4/silero-vad,high,low,False triggers on music or breath; misses low-volume speech,"Live ASR chunking, removing silence, saving cost on API calls",Need precise phoneme-level boundaries,Key glue primitive for nearly every audio pipeline.,P0,production,realtime,free/open,"mac,linux,windows,ios,android",vad,https://github.com/snakers4/silero-vad,2026-02-05T05:38:30Z,low,low
webrtcvad,WebRTC VAD,various,Local,VAD,Voice activity detection (classical),language-agnostic,yes,,webrtcvad,C / Python bindings,,CPU,,Classical baseline; tune aggressiveness,see WebRTC terms,,https://github.com/wiseman/py-webrtcvad,high,low,Sensitive to noise; needs tuning,Ultra-lightweight real-time VAD baseline,Very noisy environments without pre-denoise,Use as a baseline and compare to Silero VAD.,P0,production,realtime,free/open,"mac,linux,windows,ios,android",vad,https://github.com/wiseman/py-webrtcvad,2026-02-05T05:38:30Z,low,low
pyannote-audio-diarization,pyannote.audio,latest,Local,Diarization,Speaker diarization; segmentation; sometimes overlap handling (pipeline dependent),language-agnostic,typically batch (some chunking possible),,pyannote pipelines,PyTorch,,GPU recommended for speed; CPU works slower,,AMi/VoxConverse style benchmarks; evaluate DER,see repo/model cards (some require HF auth/terms),,https://github.com/pyannote/pyannote-audio,high,high,Overlapping speech errors; short turns; domain mismatch,"Meeting diarization, speaker-attributed transcripts",Strictly real-time without lag; heavy overlap without specialized model,Often paired with WhisperX.,P0,production,batch,mixed,"mac,linux,windows",diarization,https://github.com/pyannote/pyannote-audio,2026-02-05T05:38:30Z,medium,medium
resemblyzer,Resemblyzer,latest,Local,Speaker Embeddings,Speaker embeddings; similarity; clustering support,language-agnostic,windowed,,d-vector style,PyTorch,,CPU ok; GPU for throughput,,Evaluate EER on speaker verification sets,see repo,,https://github.com/resemble-ai/Resemblyzer,high,medium,Domain mismatch; noisy audio degrades embeddings,"Speaker clustering, duplicate speaker detection, personalization experiments",High-stakes biometric identification,Useful glue for diarization alternatives.,P1,production,batch,free/open,"mac,linux,windows",speaker-embeddings,https://github.com/resemble-ai/Resemblyzer,2026-02-05T05:38:30Z,high,medium
speechbrain-ecapa-tdnn,SpeechBrain,ECAPA-TDNN (various),Local,Speaker Embeddings,Speaker verification embeddings; clustering,language-agnostic,windowed,,ECAPA-TDNN checkpoints,PyTorch,,CPU ok; GPU for throughput,,Commonly reported on VoxCeleb; evaluate EER,see model card,,https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb,high,medium,Noisy audio; short segments; channel mismatch,"Speaker verification, clustering, diarization embeddings backbone",Regulated biometric use without compliance plan,Strong default for speaker embeddings.,P1,production,batch,free/open,"mac,linux,windows",speaker-embeddings,https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb,2026-02-05T05:38:30Z,high,medium
