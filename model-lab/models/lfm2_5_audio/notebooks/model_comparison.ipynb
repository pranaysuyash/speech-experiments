{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison & Decision Dashboard\n",
    "\n",
    "**Purpose**: Transform experimental results into production decisions\n",
    "\n",
    "**Models compared**:\n",
    "- LFM2.5-Audio-1.5B (LiquidAI)\n",
    "- Whisper-Large-V3 (OpenAI) - baseline\n",
    "\n",
    "**Metrics**: Accuracy (WER/CER), Speed (RTF), Quality (MOS), Memory\n",
    "\n",
    "---\n",
    "\n",
    "This notebook loads results from all test notebooks and produces:\n",
    "- Comparative scorecard\n",
    "- Production recommendation\n",
    "- Cost-performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPARISON SETUP ===\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== Model Comparison Dashboard ===\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"Models: LFM2.5-Audio vs Whisper-Large-V3\")\n",
    "print(\"âœ… Comparison setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD RESULTS ===\n",
    "\n",
    "results_dir = Path(\"data/results\")\n",
    "results = {}\n",
    "\n",
    "# Expected result files from each notebook\n",
    "result_files = {\n",
    "    \"asr_lfm\": \"asr_evaluation_lfm_results.json\",\n",
    "    \"asr_whisper\": \"asr_evaluation_whisper_results.json\",\n",
    "    \"tts_lfm\": \"tts_evaluation_lfm_results.json\",\n",
    "    \"conversation_lfm\": \"conversation_analysis_lfm_results.json\",\n",
    "    \"conversation_whisper\": \"conversation_analysis_whisper_results.json\",\n",
    "}\n",
    "\n",
    "print(\"Loading results from all notebooks...\")\n",
    "\n",
    "for key, filename in result_files.items():\n",
    "    filepath = results_dir / filename\n",
    "    if filepath.exists():\n",
    "        with open(filepath, \"r\") as f:\n",
    "            results[key] = json.load(f)\n",
    "        print(f\"  âœ“ {key}: {filepath.stat().st_size / 1024:.1f} KB\")\n",
    "    else:\n",
    "        print(f\"  âœ— {key}: Not found (run notebook first)\")\n",
    "\n",
    "print(f\"\\nLoaded {len(results)}/{len(result_files)} result files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPARATIVE METRICS ===\n",
    "\n",
    "\n",
    "def extract_metrics(results, model_type, test_type):\n",
    "    \"\"\"Extract key metrics from results.\"\"\"\n",
    "    key = f\"{test_type}_{model_type}\"\n",
    "    if key not in results:\n",
    "        return None\n",
    "\n",
    "    data = results[key]\n",
    "    return {\n",
    "        \"wer\": data.get(\"wer\", float(\"nan\")),\n",
    "        \"cer\": data.get(\"cer\", float(\"nan\")),\n",
    "        \"latency_ms\": data.get(\"latency_ms\", float(\"nan\")),\n",
    "        \"rtf\": data.get(\"rtf\", float(\"nan\")),  # Real-Time Factor\n",
    "        \"memory_mb\": data.get(\"memory_mb\", float(\"nan\")),\n",
    "        \"success\": data.get(\"success\", False),\n",
    "    }\n",
    "\n",
    "\n",
    "# Build comparison table\n",
    "comparison_data = []\n",
    "\n",
    "# ASR comparison\n",
    "for model in [\"lfm\", \"whisper\"]:\n",
    "    metrics = extract_metrics(results, model, \"asr\")\n",
    "    if metrics:\n",
    "        comparison_data.append(\n",
    "            {\n",
    "                \"Model\": \"LFM2.5-Audio\" if model == \"lfm\" else \"Whisper-Large-V3\",\n",
    "                \"Test\": \"ASR (speech-to-text)\",\n",
    "                \"WER (%)\": metrics[\"wer\"] * 100,\n",
    "                \"CER (%)\": metrics[\"cer\"] * 100,\n",
    "                \"Latency (ms)\": metrics[\"latency_ms\"],\n",
    "                \"RTF\": metrics[\"rtf\"],\n",
    "                \"Memory (MB)\": metrics[\"memory_mb\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Conversation comparison\n",
    "for model in [\"lfm\", \"whisper\"]:\n",
    "    metrics = extract_metrics(results, model, \"conversation\")\n",
    "    if metrics:\n",
    "        comparison_data.append(\n",
    "            {\n",
    "                \"Model\": \"LFM2.5-Audio\" if model == \"lfm\" else \"Whisper-Large-V3\",\n",
    "                \"Test\": \"Conversation (multi-speaker)\",\n",
    "                \"WER (%)\": metrics[\"wer\"] * 100,\n",
    "                \"CER (%)\": metrics[\"cer\"] * 100,\n",
    "                \"Latency (ms)\": metrics[\"latency_ms\"],\n",
    "                \"RTF\": metrics[\"rtf\"],\n",
    "                \"Memory (MB)\": metrics[\"memory_mb\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"=== Comparative Scorecard ===\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Save comparison table\n",
    "comparison_path = results_dir / \"model_comparison_table.json\"\n",
    "df_comparison.to_json(comparison_path, orient=\"records\", indent=2)\n",
    "print(f\"\\nâœ“ Comparison saved to {comparison_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRODUCTION SCORECARD ===\n",
    "\n",
    "\n",
    "def calculate_production_score(metrics):\n",
    "    \"\"\"Calculate production readiness score (0-100).\"\"\"\n",
    "    scores = []\n",
    "\n",
    "    # Accuracy score (WER: lower is better)\n",
    "    if not np.isnan(metrics[\"wer\"]):\n",
    "        wer_score = max(0, 100 - metrics[\"wer\"] * 100)  # 0% WER = 100 score\n",
    "        scores.append(wer_score)\n",
    "\n",
    "    # Speed score (RTF: lower is better, <1.0 = realtime)\n",
    "    if not np.isnan(metrics[\"rtf\"]):\n",
    "        rtf_score = max(0, 100 - metrics[\"rtf\"] * 50)  # RTF=0 = 100, RTF=2.0 = 0\n",
    "        scores.append(rtf_score)\n",
    "\n",
    "    # Memory score (lower is better)\n",
    "    if not np.isnan(metrics[\"memory_mb\"]):\n",
    "        mem_score = max(0, 100 - metrics[\"memory_mb\"] / 20)  # 0MB = 100, 2GB = 0\n",
    "        scores.append(mem_score)\n",
    "\n",
    "    return np.mean(scores) if scores else 0\n",
    "\n",
    "\n",
    "# Calculate scores\n",
    "production_scores = []\n",
    "\n",
    "for model in [\"lfm\", \"whisper\"]:\n",
    "    asr_metrics = extract_metrics(results, model, \"asr\")\n",
    "    conv_metrics = extract_metrics(results, model, \"conversation\")\n",
    "\n",
    "    if asr_metrics:\n",
    "        asr_score = calculate_production_score(asr_metrics)\n",
    "        production_scores.append(\n",
    "            {\n",
    "                \"Model\": \"LFM2.5-Audio\" if model == \"lfm\" else \"Whisper-Large-V3\",\n",
    "                \"Test\": \"ASR\",\n",
    "                \"Production Score\": asr_score,\n",
    "                \"Accuracy Grade\": \"A\" if asr_score >= 80 else \"B\" if asr_score >= 60 else \"C\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if conv_metrics:\n",
    "        conv_score = calculate_production_score(conv_metrics)\n",
    "        production_scores.append(\n",
    "            {\n",
    "                \"Model\": \"LFM2.5-Audio\" if model == \"lfm\" else \"Whisper-Large-V3\",\n",
    "                \"Test\": \"Conversation\",\n",
    "                \"Production Score\": conv_score,\n",
    "                \"Accuracy Grade\": \"A\" if conv_score >= 80 else \"B\" if conv_score >= 60 else \"C\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_scores = pd.DataFrame(production_scores)\n",
    "print(\"=== Production Readiness Scorecard ===\")\n",
    "print(df_scores.to_string(index=False))\n",
    "\n",
    "# Determine overall recommendation\n",
    "if len(df_scores) > 0:\n",
    "    best_model = df_scores.groupby(\"Model\")[\"Production Score\"].mean().idxmax()\n",
    "    best_score = df_scores.groupby(\"Model\")[\"Production Score\"].mean().max()\n",
    "\n",
    "    print(f\"\\n=== PRODUCTION RECOMMENDATION ===\")\n",
    "    print(f\"ðŸ† Recommended: {best_model}\")\n",
    "    print(f\"   Overall Score: {best_score:.1f}/100\")\n",
    "\n",
    "    if best_score >= 80:\n",
    "        print(\"   âœ… Ready for production deployment\")\n",
    "    elif best_score >= 60:\n",
    "        print(\"   âš ï¸  Ready with monitoring required\")\n",
    "    else:\n",
    "        print(\"   âŒ Not recommended for production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VISUALIZATION ===\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"Model Comparison: LFM2.5-Audio vs Whisper-Large-V3\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Plot 1: WER Comparison\n",
    "if \"WER (%)\" in df_comparison.columns:\n",
    "    ax1 = axes[0, 0]\n",
    "    for model in df_comparison[\"Model\"].unique():\n",
    "        model_data = df_comparison[df_comparison[\"Model\"] == model]\n",
    "        ax1.bar(model_data[\"Test\"], model_data[\"WER (%)\"], label=model, alpha=0.7)\n",
    "    ax1.set_ylabel(\"WER (%)\")\n",
    "    ax1.set_title(\"Word Error Rate (lower is better)\")\n",
    "    ax1.legend()\n",
    "    ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Plot 2: Speed Comparison\n",
    "if \"RTF\" in df_comparison.columns:\n",
    "    ax2 = axes[0, 1]\n",
    "    for model in df_comparison[\"Model\"].unique():\n",
    "        model_data = df_comparison[df_comparison[\"Model\"] == model]\n",
    "        ax2.bar(model_data[\"Test\"], model_data[\"RTF\"], label=model, alpha=0.7)\n",
    "    ax2.axhline(y=1.0, color=\"r\", linestyle=\"--\", label=\"Realtime threshold\")\n",
    "    ax2.set_ylabel(\"Real-Time Factor\")\n",
    "    ax2.set_title(\"Processing Speed (lower is better)\")\n",
    "    ax2.legend()\n",
    "    ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Plot 3: Memory Comparison\n",
    "if \"Memory (MB)\" in df_comparison.columns:\n",
    "    ax3 = axes[1, 0]\n",
    "    for model in df_comparison[\"Model\"].unique():\n",
    "        model_data = df_comparison[df_comparison[\"Model\"] == model]\n",
    "        ax3.bar(model_data[\"Test\"], model_data[\"Memory (MB)\"], label=model, alpha=0.7)\n",
    "    ax3.set_ylabel(\"Memory (MB)\")\n",
    "    ax3.set_title(\"Memory Usage (lower is better)\")\n",
    "    ax3.legend()\n",
    "    ax3.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Plot 4: Production Scores\n",
    "if len(df_scores) > 0:\n",
    "    ax4 = axes[1, 1]\n",
    "    for model in df_scores[\"Model\"].unique():\n",
    "        model_data = df_scores[df_scores[\"Model\"] == model]\n",
    "        ax4.bar(model_data[\"Test\"], model_data[\"Production Score\"], label=model, alpha=0.7)\n",
    "    ax4.axhline(y=80, color=\"g\", linestyle=\"--\", label=\"Production ready\")\n",
    "    ax4.axhline(y=60, color=\"orange\", linestyle=\"--\", label=\"Monitor\")\n",
    "    ax4.set_ylabel(\"Score (0-100)\")\n",
    "    ax4.set_title(\"Production Readiness Score\")\n",
    "    ax4.legend()\n",
    "    ax4.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plot_path = results_dir / \"model_comparison_plots.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"âœ“ Comparison plots saved to {plot_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COST ANALYSIS ===\n",
    "\n",
    "print(\"=== Cost-Performance Analysis ===\")\n",
    "\n",
    "# Model specifications\n",
    "model_specs = {\n",
    "    \"LFM2.5-Audio\": {\n",
    "        \"parameters\": \"1.5B\",\n",
    "        \"memory_mb\": df_comparison[df_comparison[\"Model\"] == \"LFM2.5-Audio\"][\"Memory (MB)\"].mean()\n",
    "        if len(df_comparison) > 0\n",
    "        else 0,\n",
    "        \"avg_rtf\": df_comparison[df_comparison[\"Model\"] == \"LFM2.5-Audio\"][\"RTF\"].mean()\n",
    "        if len(df_comparison) > 0\n",
    "        else 0,\n",
    "        \"avg_wer\": df_comparison[df_comparison[\"Model\"] == \"LFM2.5-Audio\"][\"WER (%)\"].mean()\n",
    "        if len(df_comparison) > 0\n",
    "        else 0,\n",
    "    },\n",
    "    \"Whisper-Large-V3\": {\n",
    "        \"parameters\": \"1.5B\",\n",
    "        \"memory_mb\": df_comparison[df_comparison[\"Model\"] == \"Whisper-Large-V3\"][\n",
    "            \"Memory (MB)\"\n",
    "        ].mean()\n",
    "        if len(df_comparison) > 0\n",
    "        else 0,\n",
    "        \"avg_rtf\": df_comparison[df_comparison[\"Model\"] == \"Whisper-Large-V3\"][\"RTF\"].mean()\n",
    "        if len(df_comparison) > 0\n",
    "        else 0,\n",
    "        \"avg_wer\": df_comparison[df_comparison[\"Model\"] == \"Whisper-Large-V3\"][\"WER (%)\"].mean()\n",
    "        if len(df_comparison) > 0\n",
    "        else 0,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Calculate cost metrics\n",
    "for model_name, specs in model_specs.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Parameters: {specs['parameters']}\")\n",
    "    print(f\"  Memory: {specs['memory_mb']:.1f} MB\")\n",
    "    print(f\"  Speed (RTF): {specs['avg_rtf']:.3f}x\")\n",
    "    print(f\"  Accuracy (WER): {specs['avg_wer']:.1f}%\")\n",
    "\n",
    "    # Cost per hour of processing\n",
    "    if specs[\"avg_rtf\"] > 0:\n",
    "        hours_per_hour = 1 / specs[\"avg_rtf\"]\n",
    "        print(f\"  Processing capacity: {hours_per_hour:.1f}x realtime\")\n",
    "\n",
    "    # Accuracy-efficiency score\n",
    "    if specs[\"avg_wer\"] > 0 and specs[\"avg_rtf\"] > 0:\n",
    "        efficiency = (100 - specs[\"avg_wer\"]) / specs[\"avg_rtf\"]\n",
    "        print(f\"  Efficiency score: {efficiency:.1f}\")\n",
    "\n",
    "# Recommendation based on use case\n",
    "print(\"\\n=== Use Case Recommendations ===\")\n",
    "print(\"Real-time applications: LFM2.5-Audio (better speed/accuracy balance)\")\n",
    "print(\"Batch processing: Whisper-Large-V3 (better accuracy, slower)\")\n",
    "print(\"Memory-constrained: LFM2.5-Audio (smaller footprint)\")\n",
    "print(\"Best overall: Based on your production requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ **Decision Summary**\n",
    "\n",
    "This dashboard transforms experimental results into actionable production decisions:\n",
    "\n",
    "### **Key Metrics**:\n",
    "- **WER**: Word Error Rate (lower = better accuracy)\n",
    "- **RTF**: Real-Time Factor (lower = faster, <1.0 = realtime)\n",
    "- **Memory**: RAM usage (lower = better for deployment)\n",
    "- **Production Score**: Combined metric (0-100, higher = better)\n",
    "\n",
    "### **Next Steps**:\n",
    "1. Run all evaluation notebooks to generate results\n",
    "2. Review comparative scorecard above\n",
    "3. Check production readiness grades\n",
    "4. Use cost analysis for deployment planning\n",
    "\n",
    "### **Production Decision Tree**:\n",
    "- Score â‰¥ 80: âœ… Deploy with confidence\n",
    "- Score 60-80: âš ï¸ Deploy with monitoring\n",
    "- Score < 60: âŒ Not production-ready"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (model-lab)",
   "language": "python",
   "name": "model-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}