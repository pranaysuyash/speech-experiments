{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è ASR Evaluation - LLM Recording vs Wikipedia Text\n",
    "\n",
    "**Complete ASR (Automatic Speech Recognition) Evaluation Pipeline**\n",
    "\n",
    "## Test Data:\n",
    "- **Input**: `llm_recording_pranay.m4a` (2-minute Wikipedia reading)\n",
    "- **Ground Truth**: `llm.txt` (original Wikipedia text)\n",
    "- **Goal**: Transcribe audio and compare accuracy with original text\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **WER**: Word Error Rate\n",
    "- **CER**: Character Error Rate  \n",
    "- **Timing**: Processing speed and real-time factor\n",
    "- **Quality**: Transcription confidence and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üéôÔ∏è ASR Evaluation: LLM Recording Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Device setup\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Device: {device.upper()}\")\n",
    "\n",
    "# Test files\n",
    "AUDIO_FILE = Path(\"data/audio/PRIMARY/llm_recording_pranay.m4a\")\n",
    "TEXT_FILE = Path(\"data/text/PRIMARY/llm.txt\")\n",
    "\n",
    "print(f\"Audio: {AUDIO_FILE.name}\")\n",
    "print(f\"Text: {TEXT_FILE.name}\")\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load LFM Model\n",
    "from liquid_audio import LFM2AudioModel, LFM2AudioProcessor, ChatState\n",
    "\n",
    "print(\"üì¶ Loading LFM components...\")\n",
    "\n",
    "HF_REPO = \"LiquidAI/LFM2.5-Audio-1.5B\"\n",
    "load_start = time.time()\n",
    "\n",
    "processor = LFM2AudioProcessor.from_pretrained(HF_REPO).eval()\n",
    "model = LFM2AudioModel.from_pretrained(HF_REPO).eval()\n",
    "\n",
    "if device != \"cpu\":\n",
    "    model = model.to(device)\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "print(f\"‚úÖ Model loaded: {load_time:.2f}s\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and Prepare Test Files\n",
    "def load_audio_for_lfm(audio_path):\n",
    "    \"\"\"Load and convert audio to LFM format.\"\"\"\n",
    "    # Load audio (handles m4a format)\n",
    "    waveform, sr = torchaudio.load(str(audio_path))\n",
    "\n",
    "    # Convert to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample to 24kHz for optimal performance\n",
    "    if sr != 24000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 24000)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = 24000\n",
    "\n",
    "    return waveform, sr\n",
    "\n",
    "\n",
    "# Load audio\n",
    "print(f\"üéµ Loading audio: {AUDIO_FILE.name}\")\n",
    "waveform, sr = load_audio_for_lfm(AUDIO_FILE)\n",
    "print(f\"‚úÖ Audio loaded: {waveform.shape}\")\n",
    "print(f\"   Duration: {waveform.shape[1] / sr:.1f}s ({waveform.shape[1] / sr / 60:.1f} minutes)\")\n",
    "\n",
    "# Load ground truth text\n",
    "print(f\"üìù Loading text: {TEXT_FILE.name}\")\n",
    "with open(TEXT_FILE, \"r\") as f:\n",
    "    ground_truth = f.read().strip()\n",
    "print(f\"‚úÖ Text loaded: {len(ground_truth)} characters\")\n",
    "print(f\"   Words: {len(ground_truth.split())}\")\n",
    "\n",
    "# Show first few lines of text\n",
    "preview = ground_truth[:200] + \"...\" if len(ground_truth) > 200 else ground_truth\n",
    "print(f'   Preview: \"{preview}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: ASR Transcription\n",
    "print(\"üéôÔ∏è  Starting ASR Transcription...\")\n",
    "print(\"This may take several minutes for a 2-minute recording...\")\n",
    "print()\n",
    "\n",
    "# Create ChatState for ASR\n",
    "chat = ChatState(processor)\n",
    "\n",
    "# System prompt for ASR\n",
    "chat.new_turn(\"system\")\n",
    "chat.add_text(\"Perform ASR.\")\n",
    "chat.end_turn()\n",
    "\n",
    "# Add audio input\n",
    "chat.new_turn(\"user\")\n",
    "chat.add_audio(waveform, sr)\n",
    "chat.end_turn()\n",
    "\n",
    "# Generate transcription\n",
    "chat.new_turn(\"assistant\")\n",
    "\n",
    "start_time = time.time()\n",
    "transcribed_text = \"\"\n",
    "token_count = 0\n",
    "\n",
    "print(\"üîç Processing audio...\")\n",
    "for t in model.generate_sequential(**chat, max_new_tokens=2048):\n",
    "    if t.numel() == 1:  # Text token\n",
    "        token_text = processor.text.decode(t)\n",
    "        transcribed_text += token_text\n",
    "        token_count += 1\n",
    "\n",
    "        # Progress indicator\n",
    "        if token_count % 50 == 0:\n",
    "            print(f\"   Tokens: {token_count}, Text length: {len(transcribed_text)}\")\n",
    "\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Transcription complete!\")\n",
    "print(f\"   Processing time: {latency:.1f}s ({latency / 60:.1f} minutes)\")\n",
    "print(f\"   Tokens generated: {token_count}\")\n",
    "print(f\"   Characters: {len(transcribed_text)}\")\n",
    "print(f\"   Words: {len(transcribed_text.split())}\")\n",
    "print(f\"   Real-time factor: {latency / (waveform.shape[1] / sr):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Display Transcription Results\n",
    "print(\"üìù TRANSCRIPTION RESULT:\")\n",
    "print(\"=\" * 60)\n",
    "print(transcribed_text)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare text lengths\n",
    "print(\"\\nüìä Basic Comparison:\")\n",
    "print(f\"Ground truth: {len(ground_truth)} chars, {len(ground_truth.split())} words\")\n",
    "print(f\"Transcription: {len(transcribed_text)} chars, {len(transcribed_text.split())} words\")\n",
    "\n",
    "# Length comparison\n",
    "char_diff = len(transcribed_text) - len(ground_truth)\n",
    "word_diff = len(transcribed_text.split()) - len(ground_truth.split())\n",
    "\n",
    "print(f\"Character difference: {char_diff:+d} ({char_diff / len(ground_truth) * 100:+.1f}%)\")\n",
    "print(f\"Word difference: {word_diff:+d} ({word_diff / len(ground_truth.split()) * 100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Calculate Word Error Rate (WER)\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    \"\"\"Calculate Word Error Rate using Levenshtein distance.\"\"\"\n",
    "    ref_words = reference.lower().split()\n",
    "    hyp_words = hypothesis.lower().split()\n",
    "\n",
    "    # Levenshtein distance for word sequences\n",
    "    m, n = len(ref_words), len(hyp_words)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_words[i - 1] == hyp_words[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
    "\n",
    "    return dp[m][n] / max(1, len(ref_words))\n",
    "\n",
    "\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    \"\"\"Calculate Character Error Rate.\"\"\"\n",
    "    ref_chars = list(reference.lower())\n",
    "    hyp_chars = list(hypothesis.lower())\n",
    "\n",
    "    m, n = len(ref_chars), len(hyp_chars)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_chars[i - 1] == hyp_chars[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
    "\n",
    "    return dp[m][n] / max(1, len(ref_chars))\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "wer = calculate_wer(ground_truth, transcribed_text)\n",
    "cer = calculate_cer(ground_truth, transcribed_text)\n",
    "\n",
    "print(\"üìä ACCURACY METRICS:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Word Error Rate (WER): {wer:.4f} ({wer * 100:.2f}%)\")\n",
    "print(f\"Character Error Rate (CER): {cer:.4f} ({cer * 100:.2f}%)\")\n",
    "print(f\"Accuracy (1-WER): {(1 - wer) * 100:.1f}%\")\n",
    "\n",
    "# Quality assessment\n",
    "print(\"\\nüéØ Quality Assessment:\")\n",
    "if wer < 0.05:\n",
    "    print(\"   ‚úÖ EXCELLENT - Near-perfect transcription\")\n",
    "elif wer < 0.10:\n",
    "    print(\"   ‚úÖ VERY GOOD - Professional quality\")\n",
    "elif wer < 0.15:\n",
    "    print(\"   ‚úÖ GOOD - Usable for most applications\")\n",
    "elif wer < 0.25:\n",
    "    print(\"   ‚ö†Ô∏è  FAIR - May require post-processing\")\n",
    "else:\n",
    "    print(\"   ‚ùå POOR - Significant errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Detailed Error Analysis\n",
    "def analyze_errors(reference, hypothesis):\n",
    "    \"\"\"Analyze specific types of errors.\"\"\"\n",
    "    ref_words = reference.lower().split()\n",
    "    hyp_words = hypothesis.lower().split()\n",
    "\n",
    "    # Simple error analysis\n",
    "    errors = {\"substitutions\": 0, \"insertions\": 0, \"deletions\": 0, \"total\": 0}\n",
    "\n",
    "    # Calculate error types (simplified)\n",
    "    errors[\"total\"] = abs(len(ref_words) - len(hyp_words)) + sum(\n",
    "        1 for r, h in zip(ref_words, hyp_words) if r != h\n",
    "    )\n",
    "    errors[\"substitutions\"] = sum(1 for r, h in zip(ref_words, hyp_words) if r != h)\n",
    "    errors[\"insertions\"] = max(0, len(hyp_words) - len(ref_words))\n",
    "    errors[\"deletions\"] = max(0, len(ref_words) - len(hyp_words))\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# Analyze errors\n",
    "error_analysis = analyze_errors(ground_truth, transcribed_text)\n",
    "\n",
    "print(\"üîç DETAILED ERROR ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Substitutions: {error_analysis['substitutions']}\")\n",
    "print(f\"Insertions: {error_analysis['insertions']}\")\n",
    "print(f\"Deletions: {error_analysis['deletions']}\")\n",
    "print(f\"Total errors: {error_analysis['total']}\")\n",
    "\n",
    "if error_analysis[\"total\"] > 0:\n",
    "    print(f\"\\nError breakdown:\")\n",
    "    print(\n",
    "        f\"   Substitutions: {error_analysis['substitutions'] / error_analysis['total'] * 100:.1f}%\"\n",
    "    )\n",
    "    print(f\"   Insertions: {error_analysis['insertions'] / error_analysis['total'] * 100:.1f}%\")\n",
    "    print(f\"   Deletions: {error_analysis['deletions'] / error_analysis['total'] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Performance Analysis\n",
    "print(\"‚è±Ô∏è  PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate performance metrics\n",
    "audio_duration = waveform.shape[1] / sr\n",
    "processing_speed = audio_duration / latency\n",
    "\n",
    "print(f\"Audio duration: {audio_duration:.1f}s ({audio_duration / 60:.1f} minutes)\")\n",
    "print(f\"Processing time: {latency:.1f}s ({latency / 60:.1f} minutes)\")\n",
    "print(f\"Real-time factor: {latency / audio_duration:.2f}x\")\n",
    "print(f\"Processing speed: {processing_speed:.2f}x real-time\")\n",
    "\n",
    "# Performance assessment\n",
    "print(\"\\nüéØ Performance Assessment:\")\n",
    "if processing_speed > 1.0:\n",
    "    print(\"   ‚úÖ REAL-TIME - Processes faster than real-time\")\n",
    "elif processing_speed > 0.5:\n",
    "    print(\"   ‚úÖ NEAR REAL-TIME - Suitable for live applications\")\n",
    "elif processing_speed > 0.1:\n",
    "    print(\"   ‚ö†Ô∏è  OFFLINE PROCESSING - Suitable for batch processing\")\n",
    "else:\n",
    "    print(\"   ‚ùå SLOW - May not be suitable for production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results\n",
    "def save_asr_results(results_dict, filename=\"asr_evaluation_results.json\"):\n",
    "    \"\"\"Save ASR evaluation results.\"\"\"\n",
    "    results_path = Path(\"results\")\n",
    "    results_path.mkdir(exist_ok=True)\n",
    "\n",
    "    output_file = results_path / filename\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Results saved: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# Compile comprehensive results\n",
    "results = {\n",
    "    \"test_info\": {\n",
    "        \"model\": HF_REPO,\n",
    "        \"device\": device,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"test_type\": \"ASR_Evaluation\",\n",
    "    },\n",
    "    \"audio_info\": {\n",
    "        \"file\": str(AUDIO_FILE),\n",
    "        \"duration_minutes\": audio_duration / 60,\n",
    "        \"sample_rate\": sr,\n",
    "    },\n",
    "    \"text_info\": {\n",
    "        \"ground_truth_file\": str(TEXT_FILE),\n",
    "        \"ground_truth_chars\": len(ground_truth),\n",
    "        \"ground_truth_words\": len(ground_truth.split()),\n",
    "        \"transcribed_chars\": len(transcribed_text),\n",
    "        \"transcribed_words\": len(transcribed_text.split()),\n",
    "    },\n",
    "    \"accuracy_metrics\": {\n",
    "        \"wer\": wer,\n",
    "        \"cer\": cer,\n",
    "        \"accuracy_percentage\": (1 - wer) * 100,\n",
    "        \"error_analysis\": error_analysis,\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"processing_time_seconds\": latency,\n",
    "        \"real_time_factor\": latency / audio_duration,\n",
    "        \"processing_speed_x\": processing_speed,\n",
    "    },\n",
    "    \"quality_assessment\": {\n",
    "        \"overall_quality\": \"EXCELLENT\" if wer < 0.1 else \"GOOD\" if wer < 0.15 else \"FAIR\",\n",
    "        \"performance_rating\": \"REAL_TIME\" if processing_speed > 1.0 else \"NEAR_REAL_TIME\",\n",
    "    },\n",
    "    \"transcription_text\": transcribed_text,\n",
    "    \"ground_truth_text\": ground_truth,\n",
    "}\n",
    "\n",
    "# Save results\n",
    "save_asr_results(results)\n",
    "\n",
    "# Also save transcription separately\n",
    "transcription_path = Path(\"results/asr_transcription.txt\")\n",
    "with open(transcription_path, \"w\") as f:\n",
    "    f.write(transcribed_text)\n",
    "print(f\"‚úÖ Transcription saved: {transcription_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary and Recommendations\n",
    "print(\"üéØ ASR EVALUATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"‚úÖ COMPLETED ASR EVALUATION:\")\n",
    "print(f\"   Model: {HF_REPO}\")\n",
    "print(f\"   Device: {device.upper()}\")\n",
    "print(f\"   Test: 2-minute LLM Wikipedia reading\")\n",
    "print()\n",
    "print(\"üìä KEY RESULTS:\")\n",
    "print(f\"   Word Error Rate: {wer * 100:.1f}%\")\n",
    "print(f\"   Character Error Rate: {cer * 100:.1f}%\")\n",
    "print(f\"   Processing Speed: {processing_speed:.2f}x real-time\")\n",
    "print()\n",
    "print(\"üéØ QUALITY ASSESSMENT:\")\n",
    "quality = \"EXCELLENT\" if wer < 0.1 else \"GOOD\" if wer < 0.15 else \"FAIR\"\n",
    "performance = \"REAL-TIME\" if processing_speed > 1.0 else \"NEAR REAL-TIME\"\n",
    "print(f\"   Transcription Quality: {quality}\")\n",
    "print(f\"   Performance: {performance}\")\n",
    "print()\n",
    "print(\"üìã NEXT STEPS:\")\n",
    "print(\"   1. Run TTS evaluation: synthesize llm.txt\")\n",
    "print(\"   2. Compare synthesized audio with original recording\")\n",
    "print(\"   3. Test with NotebookLM conversation analysis\")\n",
    "print(\"   4. Compare with other models (Whisper, etc.)\")\n",
    "print()\n",
    "print(\"‚úÖ ASR evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (model-lab)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}