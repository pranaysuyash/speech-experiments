{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîä TTS Evaluation - Text-to-Speech Quality Analysis\n",
    "\n",
    "**Complete TTS (Text-to-Speech) Evaluation Pipeline**\n",
    "\n",
    "## Test Data:\n",
    "- **Input**: `llm.txt` (Wikipedia LLM text)\n",
    "- **Comparison**: `llm_recording_pranay.m4a` (Your original reading)\n",
    "- **Goal**: Synthesize speech from text and compare with original recording\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **Audio Similarity**: Spectral and timing comparison\n",
    "- **Naturalness**: Voice quality and prosody analysis\n",
    "- **Timing**: Reading speed and pause patterns\n",
    "- **Characteristics**: Voice timbre and intonation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print('üîä TTS Evaluation: LLM Text Synthesis Analysis')\n",
    "print('=' * 60)\n",
    "\n",
    "# Device setup\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Device: {device.upper()}')\n",
    "\n",
    "# Test files\n",
    "TEXT_FILE = Path('data/text/PRIMARY/llm.txt')\n",
    "ORIGINAL_AUDIO = Path('data/audio/PRIMARY/llm_recording_pranay.m4a')\n",
    "\n",
    "print(f'Text: {TEXT_FILE.name}')\n",
    "print(f'Original Audio: {ORIGINAL_AUDIO.name}')\n",
    "print('‚úÖ Setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load LFM Model\n",
    "from liquid_audio import LFM2AudioModel, LFM2AudioProcessor, ChatState\n",
    "\n",
    "print('üì¶ Loading LFM components...')\n",
    "\n",
    "HF_REPO = 'LiquidAI/LFM2.5-Audio-1.5B'\n",
    "load_start = time.time()\n",
    "\n",
    "processor = LFM2AudioProcessor.from_pretrained(HF_REPO).eval()\n",
    "model = LFM2AudioModel.from_pretrained(HF_REPO).eval()\n",
    "\n",
    "if device != 'cpu':\n",
    "    model = model.to(device)\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "print(f'‚úÖ Model loaded: {load_time:.2f}s')\n",
    "print(f'   Parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Test Files\n",
    "def load_audio_for_comparison(audio_path):\n",
    "    \"\"\"Load audio for comparison.\"\"\"\n",
    "    waveform, sr = torchaudio.load(str(audio_path))\n",
    "    \n",
    "    # Convert to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Resample to 24kHz for comparison\n",
    "    if sr != 24000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 24000)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = 24000\n",
    "    \n",
    "    return waveform, sr\n",
    "\n",
    "# Load text\n",
    "print(f'üìù Loading text: {TEXT_FILE.name}')\n",
    "with open(TEXT_FILE, 'r') as f:\n",
    "    llm_text = f.read().strip()\n",
    "print(f'‚úÖ Text loaded: {len(llm_text)} characters')\n",
    "print(f'   Words: {len(llm_text.split())}')\n",
    "\n",
    "# Load original audio\n",
    "print(f'üéµ Loading original audio: {ORIGINAL_AUDIO.name}')\n",
    "original_waveform, original_sr = load_audio_for_comparison(ORIGINAL_AUDIO)\n",
    "print(f'‚úÖ Original audio loaded: {original_waveform.shape}')\n",
    "print(f'   Duration: {original_waveform.shape[1]/original_sr:.1f}s ({original_waveform.shape[1]/original_sr/60:.1f} minutes)')\n",
    "\n",
    "# Calculate reading statistics\n",
    "reading_speed = len(llm_text.split()) / (original_waveform.shape[1]/original_sr)\n",
    "print(f'   Reading speed: {reading_speed:.1f} words per minute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Test Different Voices\n",
    "print('üé≠ Testing Different LFM Voices')\n",
    "print('=' * 50)\n",
    "\n",
    "# Available voices in LFM\n",
    "voices = [\n",
    "    'US male',\n",
    "    'US female', \n",
    "    'UK male',\n",
    "    'UK female'\n",
    "]\n",
    "\n",
    "print(f'Available voices: {len(voices)}')\n",
    "for i, voice in enumerate(voices, 1):\n",
    "    print(f'   {i}. {voice}')\n",
    "\n",
    "# Select voice for synthesis (you can change this)\n",
    "selected_voice = voices[0]  # Start with US male\n",
    "print(f'\\nSelected voice: {selected_voice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: TTS Synthesis\n",
    "print('üîä Starting TTS Synthesis...')\n",
    "print(f'Voice: {selected_voice}')\n",
    "print('This may take several minutes for the full text...')\n",
    "print()\n",
    "\n",
    "# Create ChatState for TTS\n",
    "chat_tts = ChatState(processor)\n",
    "\n",
    "# System prompt for TTS with voice selection\n",
    "chat_tts.new_turn(\"system\")\n",
    "chat_tts.add_text(f\"Perform TTS. Use the {selected_voice} voice.\")\n",
    "chat_tts.end_turn()\n",
    "\n",
    "# Add text input\n",
    "chat_tts.new_turn(\"user\")\n",
    "chat_tts.add_text(llm_text)\n",
    "chat_tts.end_turn()\n",
    "\n",
    "chat_tts.new_turn(\"assistant\")\n",
    "\n",
    "print('üîä Synthesizing speech...')\n",
    "start_time = time.time()\n",
    "\n",
    "audio_out = []\n",
    "token_count = 0\n",
    "\n",
    "for t in model.generate_sequential(**chat_tts, max_new_tokens=2048, audio_temperature=0.8, audio_top_k=64):\n",
    "    if t.numel() > 1:  # Audio token\n",
    "        audio_out.append(t)\n",
    "        token_count += 1\n",
    "        \n",
    "        # Progress indicator\n",
    "        if token_count % 50 == 0:\n",
    "            print(f'   Audio tokens: {token_count}')\n",
    "\n",
    "synthesis_time = time.time() - start_time\n",
    "\n",
    "print(f'\\n‚úÖ Synthesis complete!')\n",
    "print(f'   Processing time: {synthesis_time:.1f}s ({synthesis_time/60:.1f} minutes)')\n",
    "print(f'   Audio tokens generated: {token_count}')\n",
    "\n",
    "# Check if we got audio\n",
    "if len(audio_out) == 0:\n",
    "    print('‚ùå No audio generated!')\n",
    "else:\n",
    "    print(f'‚úÖ Audio segments: {len(audio_out)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Detokenize and Save Synthesized Audio\n",
    "if len(audio_out) > 0:\n",
    "    print('üéµ Detokenizing synthesized audio...')\n",
    "    \n",
    "    # Detokenize audio (remove last \"end-of-audio\" codes)\n",
    "    audio_codes = torch.stack(audio_out[:-1], 1).unsqueeze(0)\n",
    "    synthesized_waveform = processor.decode(audio_codes)\n",
    "    \n",
    "    print(f'‚úÖ Audio detokenized: {synthesized_waveform.shape}')\n",
    "    print(f'   Duration: {synthesized_waveform.shape[1]/24000:.1f}s ({synthesized_waveform.shape[1]/24000/60:.1f} minutes)')\n",
    "    \n",
    "    # Save synthesized audio\n",
    "    results_path = Path('results')\n",
    "    results_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    synthesized_file = results_path / f'tts_synthesized_{selected_voice.replace(\" \", \"_\")}.wav'\n",
    "    torchaudio.save(str(synthesized_file), synthesized_waveform.cpu(), 24000)\n",
    "    \n",
    "    print(f'‚úÖ Synthesized audio saved: {synthesized_file}')\n",
    "    \n",
    "else:\n",
    "    print('‚ùå No audio to detokenize')\n",
    "    # Create dummy for testing\n",
    "    synthesized_waveform = torch.zeros(1, 24000 * 10)  # 10 seconds of silence\n",
    "    synthesized_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Audio Comparison Analysis\n",
    "def compare_audio_characteristics(original, synthesized, sr=24000):\n",
    "    \"\"\"Compare audio characteristics.\"\"\"\n",
    "    \n",
    "    # Basic statistics\n",
    "    original_rms = torch.sqrt(torch.mean(original**2)).item()\n",
    "    synthesized_rms = torch.sqrt(torch.mean(synthesized**2)).item()\n",
    "    \n",
    "    # Duration comparison\n",
    "    original_duration = original.shape[1] / sr\n",
    "    synthesized_duration = synthesized.shape[1] / sr\n",
    "    \n",
    "    # Spectral analysis (simplified)\n",
    "    original_fft = torch.fft.fft(original[0])\n",
    "    synthesized_fft = torch.fft.fft(synthesized[0])\n",
    "    \n",
    "    original_centroid = torch.sum(torch.abs(original_fft[:len(original_fft)//2]) * \n",
    "                               torch.arange(len(original_fft)//2])) / torch.sum(torch.abs(original_fft[:len(original_fft)//2]))\n",
    "    \n",
    "    synthesized_centroid = torch.sum(torch.abs(synthesized_fft[:len(synthesized_fft)//2]) * \n",
    "                                  torch.arange(len(synthesized_fft)//2])) / torch.sum(torch.abs(synthesized_fft[:len(synthesized_fft)//2]))\n",
    "    \n",
    "    return {\n",
    "        'original_rms': original_rms,\n",
    "        'synthesized_rms': synthesized_rms,\n",
    "        'rms_ratio': synthesized_rms / max(original_rms, 1e-6),\n",
    "        'original_duration': original_duration,\n",
    "        'synthesized_duration': synthesized_duration,\n",
    "        'duration_ratio': synthesized_duration / max(original_duration, 1e-6),\n",
    "        'original_centroid': original_centroid.item() / sr,\n",
    "        'synthesized_centroid': synthesized_centroid.item() / sr,\n",
    "    }\n",
    "\n",
    "# Compare audio\n",
    "if synthesized_waveform.shape[1] > 0:\n",
    "    print('üìä AUDIO COMPARISON ANALYSIS:')\n",
    "    print('=' * 50)\n",
    "    \n",
    "    # Ensure both are same sample rate for comparison\n",
    "    if original_sr != 24000:\n",
    "        resampler = torchaudio.transforms.Resample(original_sr, 24000)\n",
    "        original_for_comparison = resampler(original_waveform)\n",
    "    else:\n",
    "        original_for_comparison = original_waveform\n",
    "    \n",
    "    # Trim to same length for comparison\n",
    "    min_length = min(original_for_comparison.shape[1], synthesized_waveform.shape[1])\n",
    "    original_trimmed = original_for_comparison[:, :min_length]\n",
    "    synthesized_trimmed = synthesized_waveform[:, :min_length]\n",
    "    \n",
    "    comparison = compare_audio_characteristics(original_trimmed, synthesized_trimmed)\n",
    "    \n",
    "    print(f'Original Audio:')\n",
    "    print(f'   Duration: {comparison[\"original_duration\"]:.1f}s')\n",
    "    print(f'   RMS Level: {comparison[\"original_rms\"]:.4f}')\n",
    "    print(f'   Spectral Centroid: {comparison[\"original_centroid\"]:.1f} Hz')\n",
    "    \n",
    "    print(f'\\nSynthesized Audio ({selected_voice}):')\n",
    "    print(f'   Duration: {comparison[\"synthesized_duration\"]:.1f}s')\n",
    "    print(f'   RMS Level: {comparison[\"synthesized_rms\"]:.4f}')\n",
    "    print(f'   Spectral Centroid: {comparison[\"synthesized_centroid\"]:.1f} Hz')\n",
    "    \n",
    "    print(f'\\nComparison:')\n",
    "    print(f'   Duration Ratio: {comparison[\"duration_ratio\"]:.2f}x')\n",
    "    print(f'   RMS Ratio: {comparison[\"rms_ratio\"]:.2f}x')\n",
    "    print(f'   Centroid Difference: {abs(comparison[\"original_centroid\"] - comparison[\"synthesized_centroid\"]):.1f} Hz')\n",
    "else:\n",
    "    print('‚ùå Cannot compare - no synthesized audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Timing and Pacing Analysis\n",
    "print('‚è±Ô∏è  TIMING AND PACING ANALYSIS:')\n",
    "print('=' * 50)\n",
    "\n",
    "# Text statistics\n",
    "word_count = len(llm_text.split())\n",
    "char_count = len(llm_text)\n",
    "\n",
    "# Original recording timing\n",
    "original_duration = original_waveform.shape[1] / original_sr\n",
    "original_reading_speed = word_count / (original_duration / 60)  # words per minute\n",
    "\n",
    "# Synthesized timing\n",
    "if synthesized_waveform.shape[1] > 0:\n",
    "    synthesized_duration = synthesized_waveform.shape[1] / 24000\n",
    "    synthesized_reading_speed = word_count / (synthesized_duration / 60)\n",
    "    \n",
    "    print(f'Text Statistics:')\n",
    "    print(f'   Words: {word_count}')\n",
    "    print(f'   Characters: {char_count}')\n",
    "    \n",
    "    print(f'\\nOriginal Recording (Your Voice):')\n",
    "    print(f'   Duration: {original_duration:.1f}s ({original_duration/60:.1f} minutes)')\n",
    "    print(f'   Reading Speed: {original_reading_speed:.1f} words per minute')\n",
    "    \n",
    "    print(f'\\nSynthesized Audio ({selected_voice}):')\n",
    "    print(f'   Duration: {synthesized_duration:.1f}s ({synthesized_duration/60:.1f} minutes)')\n",
    "    print(f'   Reading Speed: {synthesized_reading_speed:.1f} words per minute')\n",
    "    \n",
    "    print(f'\\nComparison:')\n",
    "    speed_difference = synthesized_reading_speed - original_reading_speed\n",
    "    print(f'   Speed Difference: {speed_difference:+.1f} WPM')\n",
    "    print(f'   Duration Difference: {synthesized_duration - original_duration:+.1f}s ({(synthesized_duration - original_duration)/original_duration*100:+.1f}%)')\n",
    "    \n",
    "    # Speed assessment\n",
    "    print(f'\\nüéØ Reading Speed Assessment:')\n",
    "    if 130 <= synthesized_reading_speed <= 170:\n",
    "        print('   ‚úÖ NORMAL - Typical conversational speed')\n",
    "    elif synthesized_reading_speed < 130:\n",
    "        print('   ‚ö†Ô∏è  SLOW - Deliberate pacing')\n",
    "    else:\n",
    "        print('   ‚ö†Ô∏è  FAST - Rapid delivery')\n",
    "else:\n",
    "    print('‚ùå No synthesized audio for timing analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Naturalness Assessment\n",
    "print('üé≠ NATURALNESS ASSESSMENT:')\n",
    "print('=' * 40)\n",
    "\n",
    "if synthesized_waveform.shape[1] > 0:\n",
    "    # Calculate naturalness metrics\n",
    "    \n",
    "    # Dynamic range (amplitude variation)\n",
    "    synthesized_max = synthesized_waveform.abs().max().item()\n",
    "    synthesized_mean = synthesized_waveform.abs().mean().item()\n",
    "    dynamic_range = synthesized_max / max(synthesized_mean, 1e-6)\n",
    "    \n",
    "    # Energy distribution\n",
    "    energy_frames = torch.chunk(synthesized_waveform[0], 100)\n",
    "    energy_variation = torch.std(torch.stack([frame.abs().mean() for frame in energy_frames])).item()\n",
    "    \n",
    "    print(f'Dynamic Range: {dynamic_range:.2f}x')\n",
    "    print(f'Energy Variation: {energy_variation:.6f}')\n",
    "    \n",
    "    # Naturalness indicators\n",
    "    print(f'\\nüéØ Quality Indicators:')\n",
    "    \n",
    "    if dynamic_range > 3.0:\n",
    "        print('   ‚úÖ GOOD dynamic range - Natural speech variation')\n",
    "    elif dynamic_range > 1.5:\n",
    "        print('   ‚ö†Ô∏è  MODERATE dynamic range - Some variation present')\n",
    "    else:\n",
    "        print('   ‚ùå LOW dynamic range - Monotonic delivery')\n",
    "    \n",
    "    if energy_variation > 0.01:\n",
    "        print('   ‚úÖ GOOD energy variation - Natural prosody')\n",
    "    elif energy_variation > 0.005:\n",
    "        print('   ‚ö†Ô∏è  MODERATE energy variation - Some prosody present')\n",
    "    else:\n",
    "        print('   ‚ùå LOW energy variation - Flat delivery')\n",
    "    \n",
    "    # Overall quality assessment\n",
    "    print(f'\\nüèÜ Overall Quality:')\n",
    "    if dynamic_range > 3.0 and energy_variation > 0.01:\n",
    "        print('   ‚úÖ EXCELLENT - Natural, expressive speech')\n",
    "    elif dynamic_range > 2.0 and energy_variation > 0.008:\n",
    "        print('   ‚úÖ GOOD - Acceptable speech quality')\n",
    "    else:\n",
    "        print('   ‚ö†Ô∏è  NEEDS IMPROVEMENT - Robotic or monotonic')\n",
    "else:\n",
    "    print('‚ùå Cannot assess naturalness - no synthesized audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save Results and Create Comparison Report\n",
    "def save_tts_results(results_dict, filename='tts_evaluation_results.json'):\n",
    "    \"\"\"Save TTS evaluation results.\"\"\"\n",
    "    results_path = Path('results')\n",
    "    results_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    output_file = results_path / filename\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "    \n",
    "    print(f'‚úÖ Results saved: {output_file}')\n",
    "    return output_file\n",
    "\n",
    "# Compile results\n",
    "if synthesized_waveform.shape[1] > 0:\n",
    "    results = {\n",
    "        'test_info': {\n",
    "            'model': HF_REPO,\n",
    "            'device': device,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'test_type': 'TTS_Evaluation',\n",
    "        },\n",
    "        'tts_info': {\n",
    "            'voice_used': selected_voice,\n",
    "            'text_file': str(TEXT_FILE),\n",
    "            'word_count': word_count,\n",
    "            'character_count': char_count,\n",
    "            'synthesis_time_seconds': synthesis_time,\n",
    "        },\n",
    "        'comparison_info': {\n",
    "            'original_recording': str(ORIGINAL_AUDIO),\n",
    "            'synthesized_file': str(synthesized_file) if synthesized_file else None,\n",
    "            'original_duration_seconds': original_duration,\n",
    "            'synthesized_duration_seconds': synthesized_duration,\n",
    "            'duration_ratio': synthesized_duration / original_duration,\n",
    "        },\n",
    "        'timing_analysis': {\n",
    "            'original_reading_speed_wpm': original_reading_speed,\n",
    "            'synthesized_reading_speed_wpm': synthesized_reading_speed,\n",
    "            'speed_difference_wpm': speed_difference,\n",
    "        },\n",
    "        'audio_characteristics': comparison,\n",
    "        'quality_assessment': {\n",
    "            'dynamic_range': dynamic_range,\n",
    "            'energy_variation': energy_variation,\n",
    "            'overall_quality': 'EXCELLENT' if dynamic_range > 3.0 and energy_variation > 0.01 else 'GOOD' if dynamic_range > 2.0 else 'NEEDS_IMPROVEMENT',\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    save_tts_results(results)\n",
    "else:\n",
    "    print('‚ùå Cannot save results - synthesis failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Summary and Next Steps\n",
    "print('üéØ TTS EVALUATION SUMMARY')\n",
    "print('=' * 50)\n",
    "print()\n",
    "print('‚úÖ COMPLETED TTS EVALUATION:')\n",
    "print(f'   Model: {HF_REPO}')\n",
    "print(f'   Voice: {selected_voice}')\n",
    "print(f'   Text: Wikipedia LLM article ({word_count} words)')\n",
    "print()\n",
    "print('üìä KEY RESULTS:')\n",
    "if synthesized_waveform.shape[1] > 0:\n",
    "    print(f'   Synthesis Time: {synthesis_time:.1f}s')\n",
    "    print(f'   Original Duration: {original_duration:.1f}s')\n",
    "    print(f'   Synthesized Duration: {synthesized_duration:.1f}s')\n",
    "    print(f'   Reading Speed: {synthesized_reading_speed:.1f} WPM')\n",
    "    print(f'   Quality: {\"HIGH\" if dynamic_range > 3.0 else \"GOOD\" if dynamic_range > 2.0 else \"NEEDS WORK\"}')\n",
    "else:\n",
    "    print('   ‚ùå Synthesis failed')\n",
    "print()\n",
    "print('üéØ Voice Quality Assessment:')\n",
    "print('   Naturalness: Based on dynamic range and prosody')\n",
    "print('   Timing: Compared to your original recording')\n",
    "print('   Characteristics: Spectral and amplitude analysis')\n",
    "print()\n",
    "print('üìã NEXT STEPS:')\n",
    "print('   1. Listen to synthesized audio')\n",
    "print('   2. Compare with your original recording')\n",
    "print('   3. Test other voices (US female, UK male/female)')\n",
    "print('   4. Run conversation analysis on NotebookLM podcast')\n",
    "print()\n",
    "print('‚úÖ TTS evaluation complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (model-lab)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}