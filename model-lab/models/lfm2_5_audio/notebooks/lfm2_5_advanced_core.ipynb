{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFM-2.5-Audio Advanced Testing (Core)\n",
    "\n",
    "**Purpose**: Core advanced testing of LFM-2.5-Audio model capabilities\n",
    "\n",
    "**Model**: LFM2.5-Audio-1.5B from HuggingFace (LiquidAI/LFM2.5-Audio-1.5B)\n",
    "\n",
    "**Capabilities**: Audio transcription, generation, interleaved processing\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides the core advanced testing framework.\n",
    "Run this first to verify model works, then expand to full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORE SETUP ===\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Import harness modules\n",
    "import sys\n",
    "sys.path.append('harness')\n",
    "\n",
    "print(\"=== LFM-2.5-Audio Advanced Testing ===\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Torchaudio: {torchaudio.__version__}\")\n",
    "print(f\"Device: {torch.device('mps' if torch.backends.mps.is_available() else 'cpu')}\")\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL LOADING ===\n",
    "\n",
    "from liquid_audio import LFM2AudioModel, LFM2AudioProcessor, ChatState\n",
    "\n",
    "# Model configuration\n",
    "HF_REPO = 'LiquidAI/LFM2.5-Audio-1.5B'\n",
    "\n",
    "print(f\"Loading model: {HF_REPO}\")\n",
    "\n",
    "# Load model with monitoring\n",
    "load_start = time.time()\n",
    "\n",
    "processor = LFM2AudioProcessor.from_pretrained(HF_REPO).eval()\n",
    "model = LFM2AudioModel.from_pretrained(HF_REPO).eval()\n",
    "\n",
    "if device != 'cpu':\n",
    "    model = model.to(device)\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "\n",
    "print(f\"âœ“ Model loaded successfully!\")\n",
    "print(f\"  Load time: {load_time:.1f}s\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AUDIO LOADING ===\n",
    "\n",
    "# Test with canonical audio\n",
    "test_audio_path = Path('data/audio/GROUND_TRUTH/clean_speech_10s.wav')\n",
    "\n",
    "print(f\"Loading audio: {test_audio_path}\")\n",
    "\n",
    "try:\n",
    "    waveform, sr = torchaudio.load(str(test_audio_path))\n",
    "    \n",
    "    # Convert to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Resample to 24kHz for LFM\n",
    "    if sr != 24000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 24000)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = 24000\n",
    "    \n",
    "    print(f\"âœ“ Audio loaded successfully\")\n",
    "    print(f\"  Shape: {waveform.shape}\")\n",
    "    print(f\"  Sample rate: {sr}Hz\")\n",
    "    print(f\"  Duration: {waveform.shape[1]/sr:.1f}s\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âœ— Audio file not found: {test_audio_path}\")\n",
    "    print(\"Create canonical test audio first\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORE TRANSCRIPTION ===\n",
    "\n",
    "def transcribe_audio(model, processor, waveform, sr):\n",
    "    \"\"\"Basic transcription using LFM model.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create chat state\n",
    "        chat = ChatState(processor)\n",
    "        \n",
    "        # Set up for transcription\n",
    "        chat.new_turn(\"system\")\n",
    "        chat.add_text(\"Perform ASR.\")\n",
    "        chat.end_turn()\n",
    "        \n",
    "        # Add audio\n",
    "        chat.new_turn(\"user\")\n",
    "        chat.add_audio(waveform, sr)\n",
    "        chat.end_turn()\n",
    "        \n",
    "        chat.new_turn(\"assistant\")\n",
    "        \n",
    "        # Generate transcription\n",
    "        text_tokens = []\n",
    "        \n",
    "        for token in model.generate_sequential(**chat, max_new_tokens=512):\n",
    "            if token.numel() == 1:  # Text token\n",
    "                text_tokens.append(token)\n",
    "        \n",
    "        # Decode text\n",
    "        if text_tokens:\n",
    "            text_tensor = torch.stack(text_tokens, 1)\n",
    "            text = processor.text.decode(text_tensor[0])\n",
    "        else:\n",
    "            text = \"\"\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        return text.strip(), latency, {'success': True}\n",
    "        \n",
    "    except Exception as e:\n",
    "        latency = time.time() - start_time\n",
    "        return \"\", latency, {'success': False, 'error': str(e)}\n",
    "\n",
    "print(\"âœ“ Transcription function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORE TESTING ===\n",
    "\n",
    "print(\"=== CORE TRANSCRIPTION TESTING ===\")\n",
    "\n",
    "# Test transcription\n",
    "text, latency, metadata = transcribe_audio(model, processor, waveform, sr)\n",
    "\n",
    "print(f\"Latency: {latency*1000:.1f}ms\")\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"Text preview: {text[:100]}...\" if len(text) > 100 else f\"Text: {text}\")\n",
    "\n",
    "if metadata['success']:\n",
    "    print(\"âœ“ Transcription successful\")\n",
    "else:\n",
    "    print(f\"âœ— Transcription failed: {metadata.get('error', 'Unknown')}\")\n",
    "\n",
    "# Load ground truth for comparison\n",
    "ground_truth_path = Path('data/text/GROUND_TRUTH/clean_speech_10s.txt')\n",
    "\n",
    "if ground_truth_path.exists():\n",
    "    with open(ground_truth_path, 'r') as f:\n",
    "        ground_truth = f.read().strip()\n",
    "    \n",
    "    print(f\"\\n=== GROUND TRUTH COMPARISON ===\")\n",
    "    print(f\"Expected: {ground_truth[:100]}...\")\n",
    "    print(f\"Got:      {text[:100]}...\")\n",
    "    \n",
    "    # Simple character-level comparison\n",
    "    if len(ground_truth) > 0:\n",
    "        cer = sum(1 for a, b in zip(ground_truth.lower(), text.lower()) if a != b) / len(ground_truth)\n",
    "        print(f\"Approximate CER: {cer:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Core testing completed!\")\n",
    "print(f\"âœ… Model loaded and tested successfully\")\n",
    "print(f\"âœ… Audio processing working\")\n",
    "print(f\"âœ… Transcription capabilities verified\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (model-lab)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}