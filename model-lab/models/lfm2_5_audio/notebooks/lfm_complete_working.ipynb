{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ LFM-2.5-Audio Complete Working Implementation\n",
    "\n",
    "**Based on Official Documentation**: https://github.com/Liquid4All/liquid-audio\n",
    "\n",
    "This notebook implements **real, working ASR (speech-to-text)** using the official LFM API.\n",
    "\n",
    "## Key Features:\n",
    "- ‚úÖ **Real ASR transcription** using `generate_sequential()`\n",
    "- ‚úÖ **Official API usage** from liquid-audio documentation\n",
    "- ‚úÖ **Performance metrics** and timing\n",
    "- ‚úÖ **Quality evaluation** with WER calculation\n",
    "- ‚úÖ **Multi-turn conversation** examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Environment Check\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(\"üîß LFM-2.5-Audio Complete Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Environment validation\n",
    "import sys\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Executable: {sys.executable}\")\n",
    "\n",
    "# Device setup\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Device: {device.upper()}\")\n",
    "\n",
    "# Model config\n",
    "HF_REPO = \"LiquidAI/LFM2.5-Audio-1.5B\"\n",
    "print(f\"Model: {HF_REPO}\")\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import LFM Components (Official API)\n",
    "from liquid_audio import ChatState, LFM2AudioModel, LFM2AudioProcessor, LFMModality\n",
    "\n",
    "print(\"üì¶ Loading LFM components (Official API)...\")\n",
    "\n",
    "load_start = time.time()\n",
    "\n",
    "# Load processor and model using official API\n",
    "processor = LFM2AudioProcessor.from_pretrained(HF_REPO).eval()\n",
    "model = LFM2AudioModel.from_pretrained(HF_REPO).eval()\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "\n",
    "# Move to device if needed\n",
    "if device != \"cpu\":\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Components loaded: {load_time:.2f}s\")\n",
    "print(f\"   Processor: {processor.__class__.__name__}\")\n",
    "print(f\"   Model: {model.__class__.__name__}\")\n",
    "print(f\"   Device: {device.upper()}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Test Audio\n",
    "def load_audio_for_lfm(audio_path):\n",
    "    \"\"\"Load audio file for LFM processing.\"\"\"\n",
    "    # LFM expects audio at any sample rate, but 24kHz is optimal\n",
    "    waveform, sr = torchaudio.load(str(audio_path))\n",
    "\n",
    "    # Convert to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample to 24kHz for optimal performance\n",
    "    if sr != 24000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 24000)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = 24000\n",
    "\n",
    "    return waveform, sr\n",
    "\n",
    "\n",
    "# Load test audio\n",
    "audio_path = Path(\"data/audio/clean_speech_10s.wav\")\n",
    "\n",
    "if audio_path.exists():\n",
    "    waveform, sr = load_audio_for_lfm(audio_path)\n",
    "    print(f\"‚úÖ Audio loaded: {audio_path.name}\")\n",
    "    print(f\"   Shape: {waveform.shape}\")\n",
    "    print(f\"   Sample rate: {sr} Hz\")\n",
    "    print(f\"   Duration: {waveform.shape[1] / sr:.1f}s\")\n",
    "else:\n",
    "    print(f\"‚ùå Audio file not found: {audio_path}\")\n",
    "    # Create dummy audio for testing\n",
    "    print(\"Creating dummy audio for testing...\")\n",
    "    waveform = torch.randn(1, 24000 * 5)  # 5 seconds at 24kHz\n",
    "    sr = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: ASR Transcription (Official API)\n",
    "print(\"üéôÔ∏è  ASR Transcription (Official API)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create ChatState\n",
    "chat = ChatState(processor)\n",
    "\n",
    "# System prompt for ASR (from official docs)\n",
    "chat.new_turn(\"system\")\n",
    "chat.add_text(\"Perform ASR.\")\n",
    "chat.end_turn()\n",
    "\n",
    "# Add audio input\n",
    "chat.new_turn(\"user\")\n",
    "chat.add_audio(waveform, sr)\n",
    "chat.end_turn()\n",
    "\n",
    "# Generate transcription\n",
    "chat.new_turn(\"assistant\")\n",
    "\n",
    "print(\"üîç Transcribing audio...\")\n",
    "start_time = time.time()\n",
    "\n",
    "transcribed_text = \"\"\n",
    "for t in model.generate_sequential(**chat, max_new_tokens=512):\n",
    "    if t.numel() == 1:  # Text token\n",
    "        token_text = processor.text.decode(t)\n",
    "        print(token_text, end=\"\", flush=True)\n",
    "        transcribed_text += token_text\n",
    "\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(\"\\n\\n‚úÖ Transcription complete!\")\n",
    "print(f\"   Latency: {latency:.2f}s\")\n",
    "print(f\"   Text length: {len(transcribed_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Quality Evaluation\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    \"\"\"Calculate Word Error Rate.\"\"\"\n",
    "    # Simple WER calculation\n",
    "    ref_words = reference.lower().split()\n",
    "    hyp_words = hypothesis.lower().split()\n",
    "\n",
    "    # Levenshtein distance for word sequences\n",
    "    m, n = len(ref_words), len(hyp_words)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_words[i - 1] == hyp_words[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
    "\n",
    "    return dp[m][n] / max(1, len(ref_words))\n",
    "\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth_path = Path(\"data/text/clean_speech_10s.txt\")\n",
    "\n",
    "if ground_truth_path.exists():\n",
    "    with open(ground_truth_path) as f:\n",
    "        ground_truth = f.read().strip()\n",
    "\n",
    "    print(\"üìä Quality Evaluation\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Transcription: {transcribed_text}\")\n",
    "\n",
    "    # Calculate WER\n",
    "    wer = calculate_wer(ground_truth, transcribed_text)\n",
    "\n",
    "    print(\"\\nüìà Metrics:\")\n",
    "    print(f\"   WER: {wer:.3f} ({wer * 100:.1f}%)\")\n",
    "    print(f\"   Latency: {latency:.2f}s\")\n",
    "    print(f\"   Real-time factor: {latency / (waveform.shape[1] / sr):.2f}x\")\n",
    "\n",
    "    if wer < 0.1:\n",
    "        print(\"   ‚úÖ EXCELLENT quality (WER < 10%)\")\n",
    "    elif wer < 0.2:\n",
    "        print(\"   ‚úÖ Good quality (WER < 20%)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Needs improvement\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ground truth file not found for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test TTS (Text-to-Speech)\n",
    "print(\"üîä TTS Test (Text-to-Speech)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create new chat for TTS\n",
    "chat_tts = ChatState(processor)\n",
    "\n",
    "# System prompt for TTS with voice selection\n",
    "chat_tts.new_turn(\"system\")\n",
    "chat_tts.add_text(\"Perform TTS. Use the US male voice.\")\n",
    "chat_tts.end_turn()\n",
    "\n",
    "# Input text\n",
    "test_text = \"Hello, this is a test of the LFM text to speech system.\"\n",
    "\n",
    "chat_tts.new_turn(\"user\")\n",
    "chat_tts.add_text(test_text)\n",
    "chat_tts.end_turn()\n",
    "\n",
    "chat_tts.new_turn(\"assistant\")\n",
    "\n",
    "print(f'Input: \"{test_text}\"')\n",
    "print(\"üîä Generating speech...\")\n",
    "\n",
    "start_tts = time.time()\n",
    "\n",
    "audio_out = []\n",
    "for t in model.generate_sequential(\n",
    "    **chat_tts, max_new_tokens=512, audio_temperature=0.8, audio_top_k=64\n",
    "):\n",
    "    if t.numel() > 1:  # Audio token\n",
    "        audio_out.append(t)\n",
    "\n",
    "tts_latency = time.time() - start_tts\n",
    "\n",
    "# Detokenize audio\n",
    "if audio_out:\n",
    "    audio_codes = torch.stack(audio_out[:-1], 1).unsqueeze(0)\n",
    "    waveform_tts = processor.decode(audio_codes)\n",
    "\n",
    "    # Save generated audio\n",
    "    output_path = Path(\"results/tts_output.wav\")\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    torchaudio.save(str(output_path), waveform_tts.cpu(), 24000)\n",
    "\n",
    "    print(\"‚úÖ TTS complete!\")\n",
    "    print(f\"   Latency: {tts_latency:.2f}s\")\n",
    "    print(f\"   Duration: {waveform_tts.shape[1] / 24000:.1f}s\")\n",
    "    print(f\"   Saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No audio generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Multi-turn Conversation Test\n",
    "print(\"üí¨ Multi-turn Conversation Test\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create chat with interleaved generation\n",
    "chat_conv = ChatState(processor)\n",
    "\n",
    "# System prompt for interleaved generation\n",
    "chat_conv.new_turn(\"system\")\n",
    "chat_conv.add_text(\"Respond with interleaved text and audio.\")\n",
    "chat_conv.end_turn()\n",
    "\n",
    "# First turn: Audio input\n",
    "chat_conv.new_turn(\"user\")\n",
    "chat_conv.add_audio(waveform, sr)  # Use the audio we loaded earlier\n",
    "chat_conv.end_turn()\n",
    "\n",
    "chat_conv.new_turn(\"assistant\")\n",
    "\n",
    "print(\"üé§ Generating response with interleaved text and audio...\")\n",
    "\n",
    "text_out = []\n",
    "audio_out = []\n",
    "modality_out = []\n",
    "\n",
    "for i, t in enumerate(\n",
    "    model.generate_interleaved(\n",
    "        **chat_conv, max_new_tokens=256, audio_temperature=1.0, audio_top_k=4\n",
    "    )\n",
    "):\n",
    "    if t.numel() == 1:  # Text token\n",
    "        token_text = processor.text.decode(t)\n",
    "        print(token_text, end=\"\", flush=True)\n",
    "        text_out.append(t)\n",
    "        modality_out.append(LFMModality.TEXT)\n",
    "    else:  # Audio token\n",
    "        audio_out.append(t)\n",
    "        modality_out.append(LFMModality.AUDIO_OUT)\n",
    "\n",
    "    # Safety limit\n",
    "    if i > 200:\n",
    "        break\n",
    "\n",
    "print(\"\\n‚úÖ Conversation turn complete!\")\n",
    "print(f\"   Text tokens: {len(text_out)}\")\n",
    "print(f\"   Audio tokens: {len(audio_out)}\")\n",
    "\n",
    "# Save audio response if generated\n",
    "if audio_out:\n",
    "    audio_codes = torch.stack(audio_out[:-1], 1).unsqueeze(0)\n",
    "    waveform_response = processor.decode(audio_codes)\n",
    "\n",
    "    response_path = Path(\"results/conversation_response.wav\")\n",
    "    response_path.parent.mkdir(exist_ok=True)\n",
    "    torchaudio.save(str(response_path), waveform_response.cpu(), 24000)\n",
    "    print(f\"   Audio saved to: {response_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Performance Metrics\n",
    "import os\n",
    "\n",
    "import psutil\n",
    "\n",
    "print(\"üìä System Performance Metrics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get process info\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_info = process.memory_info()\n",
    "\n",
    "print(\"Memory Usage:\")\n",
    "print(f\"   RSS: {memory_info.rss / 1e6:.1f} MB\")\n",
    "print(f\"   VMS: {memory_info.vms / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  Timing Summary:\")\n",
    "print(f\"   Model load: {load_time:.2f}s\")\n",
    "print(f\"   ASR latency: {latency:.2f}s\")\n",
    "if \"tts_latency\" in locals():\n",
    "    print(f\"   TTS latency: {tts_latency:.2f}s\")\n",
    "\n",
    "print(\"\\nüéØ Performance Assessment:\")\n",
    "real_time_factor = latency / (waveform.shape[1] / sr)\n",
    "if real_time_factor < 1.0:\n",
    "    print(\"   ‚úÖ REAL-TIME capable (<1.0x)\")\n",
    "elif real_time_factor < 2.0:\n",
    "    print(\"   ‚úÖ Near real-time (<2.0x)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Not real-time (>2.0x)\")\n",
    "\n",
    "if \"wer\" in locals():\n",
    "    print(\"\\nüìù Quality Assessment:\")\n",
    "    if wer < 0.1:\n",
    "        print(\"   ‚úÖ EXCELLENT accuracy (WER < 10%)\")\n",
    "    elif wer < 0.2:\n",
    "        print(\"   ‚úÖ GOOD accuracy (WER < 20%)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  FAIR accuracy (WER > 20%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results\n",
    "def save_results(results_dict, filename=\"lfm_complete_results.json\"):\n",
    "    \"\"\"Save test results to JSON file.\"\"\"\n",
    "    results_path = Path(\"results\")\n",
    "    results_path.mkdir(exist_ok=True)\n",
    "\n",
    "    output_file = results_path / filename\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Results saved: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# Compile comprehensive results\n",
    "results = {\n",
    "    \"test_info\": {\n",
    "        \"model\": HF_REPO,\n",
    "        \"device\": device,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"model_load_time\": load_time,\n",
    "        \"asr_latency\": latency,\n",
    "        \"real_time_factor\": real_time_factor,\n",
    "        \"memory_mb\": memory_info.rss / 1e6,\n",
    "    },\n",
    "    \"quality\": {\n",
    "        \"transcription\": transcribed_text if \"transcribed_text\" in locals() else \"\",\n",
    "        \"ground_truth\": ground_truth if \"ground_truth\" in locals() else \"\",\n",
    "        \"wer\": wer if \"wer\" in locals() else None,\n",
    "    },\n",
    "    \"capabilities_tested\": {\n",
    "        \"asr\": True,\n",
    "        \"tts\": \"tts_latency\" in locals(),\n",
    "        \"conversation\": len(audio_out) > 0 if \"audio_out\" in locals() else False,\n",
    "    },\n",
    "    \"audio_info\": {\n",
    "        \"file_tested\": str(audio_path) if audio_path.exists() else \"dummy\",\n",
    "        \"duration_seconds\": waveform.shape[1] / sr,\n",
    "        \"sample_rate\": sr,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save results\n",
    "save_results(results)\n",
    "\n",
    "print(\"\\nüéâ COMPLETE LFM TEST SUCCESSFUL!\")\n",
    "print(\"üìã All capabilities tested and documented\")\n",
    "print(\"üöÄ Ready for systematic model comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (model-lab)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}