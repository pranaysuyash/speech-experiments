{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’¬ Conversation Analysis - NotebookLM Podcast\n",
    "**Complete Multi-Speaker Conversation Analysis Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "CONVERSATION_FILE = Path(\"data/audio/PRIMARY/UX_Psychology_From_Miller_s_Law_to_AI.m4a\")\n",
    "\n",
    "print(f\"ðŸ’¬ Conversation Analysis: {CONVERSATION_FILE.name}\")\n",
    "print(f\"Device: {device.upper()}\")\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load LFM Model\n",
    "from liquid_audio import ChatState, LFM2AudioModel, LFM2AudioProcessor\n",
    "\n",
    "HF_REPO = \"LiquidAI/LFM2.5-Audio-1.5B\"\n",
    "\n",
    "processor = LFM2AudioProcessor.from_pretrained(HF_REPO).eval()\n",
    "model = LFM2AudioModel.from_pretrained(HF_REPO).eval()\n",
    "\n",
    "if device != \"cpu\":\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"âœ… Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Conversation Audio\n",
    "def load_conversation_audio(audio_path, max_duration=300):\n",
    "    waveform, sr = torchaudio.load(str(audio_path))\n",
    "\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    max_samples = int(max_duration * sr)\n",
    "    if waveform.shape[1] > max_samples:\n",
    "        waveform = waveform[:, :max_samples]\n",
    "\n",
    "    if sr != 24000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 24000)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = 24000\n",
    "\n",
    "    return waveform, sr\n",
    "\n",
    "\n",
    "waveform, sr = load_conversation_audio(CONVERSATION_FILE)\n",
    "duration = waveform.shape[1] / sr\n",
    "\n",
    "print(f\"âœ… Conversation loaded: {duration:.1f}s ({duration / 60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Conversation Transcription\n",
    "chat = ChatState(processor)\n",
    "\n",
    "chat.new_turn(\"system\")\n",
    "chat.add_text(\"Transcribe a conversation between multiple speakers.\")\n",
    "chat.end_turn()\n",
    "\n",
    "chat.new_turn(\"user\")\n",
    "chat.add_audio(waveform, sr)\n",
    "chat.end_turn()\n",
    "\n",
    "chat.new_turn(\"assistant\")\n",
    "\n",
    "print(\"ðŸŽ™ï¸ Transcribing conversation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "transcribed_text = \"\"\n",
    "for t in model.generate_sequential(**chat, max_new_tokens=2048):\n",
    "    if t.numel() == 1:\n",
    "        transcribed_text += processor.text.decode(t)\n",
    "\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Transcription complete: {latency:.1f}s\")\n",
    "print(f\"Words: {len(transcribed_text.split())}\")\n",
    "print(f\"Processing speed: {duration / latency:.2f}x real-time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Display Results\n",
    "print(\"ðŸ“ TRANSCRIPTION:\")\n",
    "print(\"=\" * 50)\n",
    "print(transcribed_text[:500] + \"...\" if len(transcribed_text) > 500 else transcribed_text)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Analysis\n",
    "words = transcribed_text.split()\n",
    "sentences = transcribed_text.split(\".\")\n",
    "\n",
    "print(\"ðŸ“Š ANALYSIS:\")\n",
    "print(f\"Words: {len(words)}\")\n",
    "print(f\"Sentences: {len(sentences)}\")\n",
    "print(f\"Speaking rate: {len(words) / (duration / 60):.1f} WPM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save Results\n",
    "results = {\n",
    "    \"test_info\": {\n",
    "        \"model\": HF_REPO,\n",
    "        \"device\": device,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    },\n",
    "    \"conversation_info\": {\n",
    "        \"file\": str(CONVERSATION_FILE),\n",
    "        \"duration_minutes\": duration / 60,\n",
    "    },\n",
    "    \"transcription_info\": {\n",
    "        \"words\": len(words),\n",
    "        \"processing_speed_realtime\": duration / latency,\n",
    "    },\n",
    "}\n",
    "\n",
    "results_path = Path(\"results\")\n",
    "results_path.mkdir(exist_ok=True)\n",
    "\n",
    "with open(results_path / \"conversation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"âœ… Results saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (model-lab)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}