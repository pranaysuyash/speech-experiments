{
  "provider_id": "whisper",
  "capability": "asr",
  "input": {
    "audio_file": "llm_recording_pranay.wav",
    "audio_sha256": "4c6a2507ee15cdc0",
    "duration_s": 163.1786875,
    "sr": 16000
  },
  "output": {
    "text": "A large language model LLM is a language model trained with self-supervised machine learning on a vast amount of text designed for natural language processing tasks especially language generation the largest and most capable LLMs are generated pre-trained transformers GPTs and provide the core capabilities of modern chatbots LLMs can be fine-tuned for specific tasks or guided by prompt engineering these models require sorry acquire predictive power regarding syntax semantics and ontologies inherent in human language corpora but also inherit in accuracies and biases present in the data they are trained on they consist of billions to trillions of parameters and operate as general-purpose sequence models and rating summarizing translating and reasoning over text LLMs represent a significant new technology in their ability to generalize across tasks with minimal task specific supervision enabling capabilities like conversational agents code generation knowledge retrieval and automated reasoning that previously required bespoke systems LLMs evolved from earlier statistical and recurrent neural network approaches to language modeling the transformer architecture introduced in 2017 replaced recurrence with self-attention allowing efficient parallelization longer context handling and scalable training on precedented data volumes this innovation enabled models like GPT BERT and the successors which demonstrated emergent behaviors at scale such as few short learning and compositional reasoning reinforcement learning particularly policy gradient algorithms has been adapted to fine-tune LLMs for desired behaviors beyond raw text raw next token prediction reinforcement learning from human feedback RLHF applies these models to optimize the policy the LLMs output distribution against reward signals derived from human or automated preference judgments this has been critical for aligning model outputs with user expectations improving factuality reducing harmful responses and enhancing task performance benchmark evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive multitask evaluations measuring reasoning factual accuracy alignment and safety hill climbing and iteratively optimizing models against benchmarks has emerged as dominant strategy reducing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements",
    "text_length": 2473,
    "normalized_text": "a large language model llm is a language model trained with self-supervised machine learning on a vast amount of text designed for natural language processing tasks especially language generation the largest and most capable llms are generated pre-trained transformers gpts and provide the core capabilities of modern chatbots llms can be fine-tuned for specific tasks or guided by prompt engineering these models require sorry acquire predictive power regarding syntax semantics and ontologies inherent in human language corpora but also inherit in accuracies and biases present in the data they are trained on they consist of billions to trillions of parameters and operate as general-purpose sequence models and rating summarizing translating and reasoning over text llms represent a significant new technology in their ability to generalize across tasks with minimal task specific supervision enabling capabilities like conversational agents code generation knowledge retrieval and automated reasoning that previously required bespoke systems llms evolved from earlier statistical and recurrent neural network approaches to language modeling the transformer architecture introduced in 2017 replaced recurrence with self-attention allowing efficient parallelization longer context handling and scalable training on precedented data volumes this innovation enabled models like gpt bert and the successors which demonstrated emergent behaviors at scale such as few short learning and compositional reasoning reinforcement learning particularly policy gradient algorithms has been adapted to fine-tune llms for desired behaviors beyond raw text raw next token prediction reinforcement learning from human feedback rlhf applies these models to optimize the policy the llms output distribution against reward signals derived from human or automated preference judgments this has been critical for aligning model outputs with user expectations improving factuality reducing harmful responses and enhancing task performance benchmark evaluations for llms have evolved from narrow linguistic assessments toward comprehensive multitask evaluations measuring reasoning factual accuracy alignment and safety hill climbing and iteratively optimizing models against benchmarks has emerged as dominant strategy reducing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements",
    "segments": [
      {
        "id": 0,
        "seek": 0,
        "start": 0.0,
        "end": 5.64,
        "text": " A large language model LLM is a language model trained with self-supervised",
        "tokens": [
          50365,
          316,
          2416,
          2856,
          2316,
          441,
          43,
          44,
          307,
          257,
          2856,
          2316,
          8895,
          365,
          2698,
          12,
          48172,
          24420,
          50647
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16928582239632656,
        "compression_ratio": 1.7153846153846153,
        "no_speech_prob": NaN
      },
      {
        "id": 1,
        "seek": 0,
        "start": 5.64,
        "end": 9.24,
        "text": " machine learning on a vast amount of text designed for natural language",
        "tokens": [
          50647,
          3479,
          2539,
          322,
          257,
          8369,
          2372,
          295,
          2487,
          4761,
          337,
          3303,
          2856,
          50827
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16928582239632656,
        "compression_ratio": 1.7153846153846153,
        "no_speech_prob": NaN
      },
      {
        "id": 2,
        "seek": 0,
        "start": 9.24,
        "end": 13.6,
        "text": " processing tasks especially language generation the largest and most capable",
        "tokens": [
          50827,
          9007,
          9608,
          2318,
          2856,
          5125,
          264,
          6443,
          293,
          881,
          8189,
          51045
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16928582239632656,
        "compression_ratio": 1.7153846153846153,
        "no_speech_prob": NaN
      },
      {
        "id": 3,
        "seek": 0,
        "start": 13.6,
        "end": 17.46,
        "text": " LLMs are generated pre-trained transformers GPTs and provide the core",
        "tokens": [
          51045,
          441,
          43,
          26386,
          366,
          10833,
          659,
          12,
          17227,
          2001,
          4088,
          433,
          26039,
          33424,
          293,
          2893,
          264,
          4965,
          51238
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16928582239632656,
        "compression_ratio": 1.7153846153846153,
        "no_speech_prob": NaN
      },
      {
        "id": 4,
        "seek": 0,
        "start": 17.46,
        "end": 22.62,
        "text": " capabilities of modern chatbots LLMs can be fine-tuned for specific tasks or",
        "tokens": [
          51238,
          10862,
          295,
          4363,
          5081,
          65,
          1971,
          441,
          43,
          26386,
          393,
          312,
          2489,
          12,
          83,
          43703,
          337,
          2685,
          9608,
          420,
          51496
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16928582239632656,
        "compression_ratio": 1.7153846153846153,
        "no_speech_prob": NaN
      },
      {
        "id": 5,
        "seek": 0,
        "start": 22.62,
        "end": 28.2,
        "text": " guided by prompt engineering these models require sorry acquire predictive",
        "tokens": [
          51496,
          19663,
          538,
          12391,
          7043,
          613,
          5245,
          3651,
          2597,
          20001,
          35521,
          51775
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16928582239632656,
        "compression_ratio": 1.7153846153846153,
        "no_speech_prob": NaN
      },
      {
        "id": 6,
        "seek": 2820,
        "start": 28.2,
        "end": 34.64,
        "text": " power regarding syntax semantics and ontologies inherent in human language",
        "tokens": [
          50365,
          1347,
          8595,
          28431,
          4361,
          45298,
          293,
          6592,
          6204,
          26387,
          294,
          1952,
          2856,
          50687
        ],
        "temperature": 0.4,
        "avg_logprob": -0.21626172746930802,
        "compression_ratio": 1.7403100775193798,
        "no_speech_prob": 0.000873803102876991
      },
      {
        "id": 7,
        "seek": 2820,
        "start": 34.64,
        "end": 39.42,
        "text": " corpora but also inherit in accuracies and biases present in the data they are",
        "tokens": [
          50687,
          6804,
          64,
          457,
          611,
          21389,
          294,
          5771,
          20330,
          293,
          32152,
          1974,
          294,
          264,
          1412,
          436,
          366,
          50926
        ],
        "temperature": 0.4,
        "avg_logprob": -0.21626172746930802,
        "compression_ratio": 1.7403100775193798,
        "no_speech_prob": 0.000873803102876991
      },
      {
        "id": 8,
        "seek": 2820,
        "start": 39.42,
        "end": 44.2,
        "text": " trained on they consist of billions to trillions of parameters and operate as",
        "tokens": [
          50926,
          8895,
          322,
          436,
          4603,
          295,
          17375,
          281,
          504,
          46279,
          295,
          9834,
          293,
          9651,
          382,
          51165
        ],
        "temperature": 0.4,
        "avg_logprob": -0.21626172746930802,
        "compression_ratio": 1.7403100775193798,
        "no_speech_prob": 0.000873803102876991
      },
      {
        "id": 9,
        "seek": 2820,
        "start": 44.2,
        "end": 48.78,
        "text": " general-purpose sequence models and rating summarizing translating and",
        "tokens": [
          51165,
          2674,
          12,
          42601,
          8310,
          5245,
          293,
          10990,
          14611,
          3319,
          35030,
          293,
          51394
        ],
        "temperature": 0.4,
        "avg_logprob": -0.21626172746930802,
        "compression_ratio": 1.7403100775193798,
        "no_speech_prob": 0.000873803102876991
      }
    ]
  },
  "metrics": {
    "latency_ms_p50": 68456.56400000007,
    "rtf": 0.4195190257306125,
    "wer": null,
    "cer": null
  },
  "system": {
    "device": "mps",
    "model": "large-v3",
    "inference_type": "local",
    "capabilities": [
      "asr",
      "alignment"
    ]
  },
  "evidence": {
    "grade": "smoke",
    "dataset_id": "asr_smoke_v1",
    "truth_sha256": "d3f67ac6db0e10be",
    "sanity_gates": {
      "wer_valid": "\u2705 Pass",
      "length_chk": "\u2139\ufe0f 1.03"
    },
    "wer_valid": true
  },
  "protocol": {
    "normalization_version": "1.0",
    "bundle_contract": "v1"
  },
  "manifest": {
    "provider_id": "whisper",
    "git_hash": "861fa73ab11b1c79565d46937447527ba211d478",
    "timestamp": "2026-01-12T14:57:48.337832",
    "provider_versions": {
      "openai-whisper": "20250625",
      "faster-whisper": "1.2.1",
      "liquid-audio": "1.1.0",
      "torch": "2.9.1",
      "torchaudio": "2.9.1",
      "transformers": "4.57.3"
    },
    "config_hash": "fc204d66a7e82a6b",
    "config_path": "models/whisper/config.yaml",
    "dataset_hash": "016a69898d9c6cc9",
    "audio_file": "llm_recording_pranay.wav",
    "text_file": "llm.txt"
  },
  "timestamps": {
    "started_at": "2026-01-12T14:58:57.471882",
    "finished_at": "2026-01-12T14:58:57.471923"
  },
  "errors": [],
  "provenance": {
    "schema_version": "1.0",
    "created_at": "2026-01-12T14:57:48.228635",
    "dataset_id": "asr_smoke_v1",
    "has_ground_truth": true,
    "metrics_valid": true,
    "dataset_hash": null,
    "audio_hash": "4c6a2507ee15cdc0",
    "ground_truth_hash": "6f34729aabc63d30"
  },
  "run_context": {
    "device": "mps",
    "audio_duration_s": 163.1786875,
    "model_version": null,
    "runner_git_hash": "861fa73"
  }
}