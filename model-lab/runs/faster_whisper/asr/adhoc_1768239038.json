{
  "run_context": {
    "task": "asr",
    "model_id": "faster_whisper",
    "grade": "adhoc",
    "timestamp": "2026-01-12T23:00:38.261420",
    "git_hash": null,
    "command": [
      "scripts/run_asr.py",
      "--model",
      "faster_whisper",
      "--audio",
      "data/audio/PRIMARY/llm_recording_pranay.wav",
      "--pre",
      "trim_silence"
    ],
    "device": "mps",
    "model_version": null
  },
  "inputs": {
    "audio_path": "/Users/pranay/Projects/speech_experiments/model-lab/data/audio/PRIMARY/llm_recording_pranay.wav",
    "audio_hash": "7d0a5379b32c8361",
    "source_media_path": null,
    "source_media_hash": null,
    "dataset_id": "adhoc_7d0a5379b32c",
    "dataset_hash": null,
    "audio_duration_s": 163.1786875,
    "sample_rate": 16000
  },
  "metrics_quality": {
    "wer": null,
    "cer": null,
    "mer": null,
    "wil": null,
    "der": null,
    "der_proxy": null,
    "speaker_accuracy": null,
    "jaccard_error": null,
    "bleu": null
  },
  "metrics_structural": {
    "latency_ms": 18264.35089111328,
    "duration_s": 163.1786875,
    "rtf": 0.1119285316663261,
    "word_count": 338,
    "segment_count": 28
  },
  "output": {
    "text": " A large language model, LLM is a language model trained with self supervised machine learning  on a vast amount of text designed for natural language processing tasks, especially language  generation. The largest and most capable LLM's are generated pre-trained transformers,  GPDs and provide the core capabilities of modern chatbots. LLMs can be fine tuned for specific tasks  or guided by prompt engineering. These models require, sorry, acquire predictive power regarding  syntax semantics and ontologies inherent in human language corpora, but they also inherit  in accuracy and biases present in the data they are trained on. They consist of billions  to trillions of parameters and operate as general purpose sequence models and rating, summarizing,  translating and reasoning overtakes. LLMs represent a significant new technology in their ability  to generalize across tasks with minimal tasks specific supervision, enabling capabilities  like conversational agents, code generation, knowledge retrieval and automated reasoning.  That previously required bespoke systems. LLMs evolved from earlier statistical and recurrent  neural network approaches to language modeling. The transformer architecture introduced in 2017  replaced recurrence with self-attention allowing efficient parallelization,  longer contact handling and scalable training on unprecedented data volumes. This innovation  enabled models like GPD, BERT and the successors which demonstrated emergent behaviors at scale,  such as few short learning and compositional reasoning. Enforcement learning particularly  policy gradient algorithms has been adapted to fine tuned LLMs for desired behaviors  beyond raw text, raw next token prediction. Reinforcement learning from human feedback  are led to apply these models to optimize the policy, the LLMs, output distribution against  reward signals, drive from human or automated preference judgments. This has been critical  for aligning model outputs with user expectations, improving factuality, reducing harmful responses  and enhancing task performance. Benchmark evaluations for LLMs have evolved from  narrow linguistic assessments toward comprehensive multi-task evaluations, measuring reasoning,  factual accuracy, alignment and safety. Hill climbing iteratively optimizing models against  Benchmarks has emerged as dominant strategy, reducing rapid incremental performance gains,  but raising concerns of overfeiting to Benchmarks rather than achieving genuine  generalization or robust capability improvement.",
    "segments": [
      {
        "start": 0.0,
        "end": 5.5200000000000005,
        "text": " A large language model, LLM is a language model trained with self supervised machine learning"
      },
      {
        "start": 5.5200000000000005,
        "end": 10.16,
        "text": " on a vast amount of text designed for natural language processing tasks, especially language"
      },
      {
        "start": 10.16,
        "end": 15.040000000000001,
        "text": " generation. The largest and most capable LLM's are generated pre-trained transformers,"
      },
      {
        "start": 15.040000000000001,
        "end": 21.36,
        "text": " GPDs and provide the core capabilities of modern chatbots. LLMs can be fine tuned for specific tasks"
      },
      {
        "start": 21.36,
        "end": 28.32,
        "text": " or guided by prompt engineering. These models require, sorry, acquire predictive power regarding"
      },
      {
        "start": 29.12,
        "end": 35.36,
        "text": " syntax semantics and ontologies inherent in human language corpora, but they also inherit"
      },
      {
        "start": 35.36,
        "end": 41.120000000000005,
        "text": " in accuracy and biases present in the data they are trained on. They consist of billions"
      },
      {
        "start": 41.120000000000005,
        "end": 46.16,
        "text": " to trillions of parameters and operate as general purpose sequence models and rating, summarizing,"
      },
      {
        "start": 46.16,
        "end": 53.519999999999996,
        "text": " translating and reasoning overtakes. LLMs represent a significant new technology in their ability"
      },
      {
        "start": 53.68,
        "end": 59.2,
        "text": " to generalize across tasks with minimal tasks specific supervision, enabling capabilities"
      }
    ]
  },
  "artifacts": {},
  "provenance": {
    "has_ground_truth": false,
    "metrics_valid": true,
    "ingest_tool": "native",
    "ingest_version": "soundfile",
    "is_extracted": false,
    "original_format": ".wav"
  },
  "gates": {},
  "errors": [],
  "meta": {
    "task": "asr",
    "model_id": "faster_whisper",
    "timestamp": "2026-01-12T23:00:38.261420"
  },
  "evidence": {
    "grade": "adhoc",
    "dataset_id": "adhoc_7d0a5379b32c"
  },
  "system": {
    "device": "mps"
  },
  "metrics": {
    "latency_ms": 18264.35089111328,
    "duration_s": 163.1786875,
    "rtf": 0.1119285316663261,
    "word_count": 338,
    "segment_count": 28,
    "wer": null,
    "cer": null,
    "mer": null,
    "wil": null,
    "der": null,
    "der_proxy": null,
    "speaker_accuracy": null,
    "jaccard_error": null,
    "bleu": null
  },
  "preprocessing": [
    {
      "name": "trim_silence",
      "version": "1.0.0",
      "params": {},
      "in_audio_hash": "7d0a5379b32c8361",
      "out_audio_hash": "63492904e96243d8",
      "metrics": {
        "trimmed_start_ms": 1057.7,
        "trimmed_end_ms": 506.2,
        "total_trimmed_ms": 1563.9,
        "threshold_db": -40.0
      },
      "duration_in_s": 163.1786875,
      "duration_out_s": 161.6148125
    }
  ]
}