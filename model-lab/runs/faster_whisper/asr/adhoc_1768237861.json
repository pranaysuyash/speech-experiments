{
  "run_context": {
    "task": "asr",
    "model_id": "faster_whisper",
    "grade": "adhoc",
    "timestamp": "2026-01-12T22:41:01.373376",
    "git_hash": null,
    "command": [
      "scripts/run_asr.py",
      "--model",
      "faster_whisper",
      "--audio",
      "data/audio/PRIMARY/llm_recording_pranay.wav"
    ],
    "device": "mps",
    "model_version": null
  },
  "inputs": {
    "audio_path": "/Users/pranay/Projects/speech_experiments/model-lab/data/audio/PRIMARY/llm_recording_pranay.wav",
    "audio_hash": "7d0a5379b32c8361",
    "source_media_path": null,
    "source_media_hash": null,
    "dataset_id": "adhoc_7d0a5379b32c",
    "dataset_hash": null,
    "audio_duration_s": 163.1786875,
    "sample_rate": 16000
  },
  "metrics_quality": {
    "wer": null,
    "cer": null,
    "mer": null,
    "wil": null,
    "der": null,
    "der_proxy": null,
    "speaker_accuracy": null,
    "jaccard_error": null,
    "bleu": null
  },
  "metrics_structural": {
    "latency_ms": 20089.125871658325,
    "duration_s": 163.1786875,
    "rtf": 0.12311121127051794,
    "word_count": 336,
    "segment_count": 33
  },
  "output": {
    "text": " A large language model LLM is a language model trained with self supervised machine learning  on a vast amount of text designed for natural language processing tasks, especially language  generation.  The largest and most capable LLMs are generated pre-trained transformers, GPDs and provide  the core capabilities of modern chatbots.  LLMs can be fine tuned for specific tasks or guided by prompt engineering.  These models require, sorry, acquire predictive power regarding syntax semantics and ontologies  inherent in human language corpora, but they also inherent in accuracy and biases present  in the data they are trained on.  They consist of billions, tutorials or parameters and operate as general purpose sequence models  and rating, summarizing, translating and reasoning over text.  LLMs represent a significant new technology in their ability to generalize across tasks  with minimal task specific supervision, enabling capabilities like conversational agents, code  generation, knowledge retrieval and automated reasoning that previously required bespoke  systems.  LLMs evolved from earlier statistical and recurrent neural network approaches to language modeling.  The transformer architecture introduced in 2017 replaced recurrence with self-attention  allowing efficient parallelization, longer contact handling and scalable training on unprecedented  data volumes.  This innovation enabled models like GPD, BERT and the successes which demonstrated emergent  behaviors at scale, such as few short learning and compositional reasoning.  Inforcement learning particularly policy gradient algorithms has been adapted to fine-tune  LLMs for desired behaviors beyond raw text, raw next token prediction.  Inforcement learning from human feedback, Aralacheph applies these models to optimize the  policy, the LLMs output distribution against rewards, signals, drive from human or automated  preference judgments.  This has been critical for aligning model outputs with user expectations.  LLMs have been reduced to reducing harmful responses and enhancing task performance.  Benchmark evaluations for AralMs have evolved from narrow linguistic assessments toward comprehensive  multi-task evaluations, measuring reasoning, factual accuracy, alignment and safety, hill  climbing, iteratively optimizing models against benchmarks, azimages, dominant strategy, reducing  rapid incremental performance gains, but raising concerns of overfeiting to benchmarks rather  than achieving genuine generalization or robust capability improvements.",
    "segments": [
      {
        "start": 0.0,
        "end": 6.640000000000001,
        "text": " A large language model LLM is a language model trained with self supervised machine learning"
      },
      {
        "start": 6.640000000000001,
        "end": 11.28,
        "text": " on a vast amount of text designed for natural language processing tasks, especially language"
      },
      {
        "start": 11.28,
        "end": 12.280000000000001,
        "text": " generation."
      },
      {
        "start": 12.280000000000001,
        "end": 17.12,
        "text": " The largest and most capable LLMs are generated pre-trained transformers, GPDs and provide"
      },
      {
        "start": 17.12,
        "end": 19.96,
        "text": " the core capabilities of modern chatbots."
      },
      {
        "start": 19.96,
        "end": 25.080000000000002,
        "text": " LLMs can be fine tuned for specific tasks or guided by prompt engineering."
      },
      {
        "start": 25.080000000000002,
        "end": 32.96,
        "text": " These models require, sorry, acquire predictive power regarding syntax semantics and ontologies"
      },
      {
        "start": 32.96,
        "end": 38.2,
        "text": " inherent in human language corpora, but they also inherent in accuracy and biases present"
      },
      {
        "start": 38.2,
        "end": 41.24,
        "text": " in the data they are trained on."
      },
      {
        "start": 41.24,
        "end": 45.8,
        "text": " They consist of billions, tutorials or parameters and operate as general purpose sequence models"
      }
    ]
  },
  "artifacts": {},
  "provenance": {
    "has_ground_truth": false,
    "metrics_valid": true,
    "ingest_tool": "native",
    "ingest_version": "soundfile",
    "is_extracted": false,
    "original_format": ".wav"
  },
  "gates": {},
  "errors": [],
  "meta": {
    "task": "asr",
    "model_id": "faster_whisper",
    "timestamp": "2026-01-12T22:41:01.373376"
  },
  "evidence": {
    "grade": "adhoc",
    "dataset_id": "adhoc_7d0a5379b32c"
  },
  "system": {
    "device": "mps"
  },
  "metrics": {
    "latency_ms": 20089.125871658325,
    "duration_s": 163.1786875,
    "rtf": 0.12311121127051794,
    "word_count": 336,
    "segment_count": 33,
    "wer": null,
    "cer": null,
    "mer": null,
    "wil": null,
    "der": null,
    "der_proxy": null,
    "speaker_accuracy": null,
    "jaccard_error": null,
    "bleu": null
  }
}